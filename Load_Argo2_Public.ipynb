{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43041, 50, 2) (43041, 60, 2)\n",
      "(6325, 50, 2) ()\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_data(city, split, type):\n",
    "    f = ROOT_PATH + split + \"/\" + city + \"_\" + type\n",
    "    data = pickle.load(open(f, \"rb\"))\n",
    "    data = np.asarray(data)\n",
    "    return data\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    if city != \"all\":\n",
    "        inputs = get_data(city, split, \"inputs\")\n",
    "    else:\n",
    "        inputs = []\n",
    "        for place in cities:\n",
    "            inputs.append(get_data(place, split, \"inputs\"))\n",
    "        inputs = np.concatenate(inputs, axis=0)\n",
    "    \n",
    "    outputs = None\n",
    "    mean, std = None, None\n",
    "\n",
    "    if split==\"train\":\n",
    "        if city != \"all\":\n",
    "            outputs = get_data(city, split, \"outputs\")\n",
    "        else:\n",
    "            outputs = []\n",
    "            for place in cities:\n",
    "                outputs.append(get_data(place, split, \"outputs\"))\n",
    "            outputs = np.concatenate(outputs, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        trajectories = np.concatenate((inputs, outputs), axis=1)\n",
    "        trajectories = trajectories.reshape(-1, trajectories.shape[1], trajectories.shape[2])\n",
    "        trajectories = trajectories.astype(np.float32)\n",
    "\n",
    "        if normalized:\n",
    "            mean = np.mean(trajectories, axis=0)\n",
    "            std = np.std(trajectories, axis=0)\n",
    "            trajectories = (trajectories - mean) / std\n",
    "            mean = mean[0]\n",
    "            std = std[0]\n",
    "\n",
    "        inputs = [trajectory[:50] for trajectory in trajectories]\n",
    "        outputs = [trajectory[50:] for trajectory in trajectories]\n",
    "\n",
    "    inputs, outputs = np.asarray(inputs), np.asarray(outputs)\n",
    "    print(inputs.shape, outputs.shape)\n",
    "    return inputs, outputs, mean, std\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.city, self.split = city, split\n",
    "\n",
    "        self.inputs, self.outputs, self.mean, self.std = get_city_trajectories(city=city, split=split, normalized=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            data = (self.inputs[idx], self.outputs[idx])\n",
    "        else:\n",
    "            data = (self.inputs[idx])\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def get_mean_std(self):\n",
    "        return self.mean, self.std\n",
    "\n",
    "# intialize each dataset\n",
    "# train_dataset = ArgoverseDataset(city=\"all\", split=\"train\")\n",
    "train_austin = ArgoverseDataset(city = \"austin\", split = \"train\")\n",
    "# train_miami = ArgoverseDataset(city = \"miami\", split = \"train\")\n",
    "# train_palo_alto = ArgoverseDataset(city = \"palo-alto\", split = \"train\")\n",
    "# train_pittsburgh = ArgoverseDataset(city = \"pittsburgh\", split = \"train\")\n",
    "# train_dearborn = ArgoverseDataset(city = \"dearborn\", split = \"train\")\n",
    "# train_washington_dc = ArgoverseDataset(city = \"washington-dc\", split = \"train\")\n",
    "\n",
    "# data_mean, data_std = train_dataset.get_mean_std()\n",
    "austin_mean, austin_std = train_austin.get_mean_std()\n",
    "# miami_mean, miami_std = train_miami.get_mean_std()\n",
    "# palo_alto_mean, palo_alto_std = train_palo_alto.get_mean_std()\n",
    "# pittsburgh_mean, pittsburgh_std = train_pittsburgh.get_mean_std()\n",
    "# dearborn_mean, dearborn_std = train_dearborn.get_mean_std()\n",
    "# washington_dc_mean, washington_dc_std = train_washington_dc.get_mean_std()\n",
    "\n",
    "# test_dataset = ArgoverseDataset(city=\"all\", split=\"test\")\n",
    "test_austin = ArgoverseDataset(city = \"austin\", split = \"test\")\n",
    "# test_miami = ArgoverseDataset(city = \"miami\", split = \"test\")\n",
    "# test_palo_alto = ArgoverseDataset(city = \"palo-alto\", split = \"test\")\n",
    "# test_pittsburgh = ArgoverseDataset(city = \"pittsburgh\", split = \"test\")\n",
    "# test_dearborn = ArgoverseDataset(city = \"dearborn\", split = \"test\")\n",
    "# test_washington_dc = ArgoverseDataset(city = \"washington-dc\", split = \"test\")\n",
    "\n",
    "# Train Validation Split\n",
    "train_size = int(0.8 * len(train_austin))\n",
    "valid_size = len(train_austin) - train_size\n",
    "# train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
    "train_austin, val_austin = torch.utils.data.random_split(train_austin, [train_size, valid_size])\n",
    "# train_miami, val_miami = torch.utils.data.random_split(train_miami, [train_size, valid_size])\n",
    "# train_palo_alto, val_palo_alto = torch.utils.data.random_split(train_palo_alto, [train_size, valid_size])\n",
    "# train_pittsburgh, val_pittsburgh = torch.utils.data.random_split(train_pittsburgh, [train_size, valid_size])\n",
    "# train_dearborn, val_dearborn = torch.utils.data.random_split(train_dearborn, [train_size, valid_size])\n",
    "# train_washington_dc, val_washington_dc = torch.utils.data.random_split(train_washington_dc, [train_size, valid_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 10  # batch size \n",
    "train_loader = DataLoader(train_austin,batch_size=batch_sz,shuffle=True)\n",
    "val_loader = DataLoader(val_austin,batch_size=batch_sz,shuffle=True)\n",
    "test_loader = DataLoader(test_austin,batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1YAAAC0CAYAAACXOL1/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA8ZklEQVR4nO2de5AU133vv/tiF7Q4pELbV0kQ41vOY13lxyIprjKJKUpIThnHwo9b8qYUiJBSLUsIxHhXRggkgwhFBOYl4bBlPYnkLTtRLDlSqowVYRIpZUcCLPuWubl2VQbJTmw3qYvEot3Znd29fxzazM6c0zPdfU7P6e7vp4padHrZPeo5ffr8Xt9f28zMzAwIIYQQQgghhESmvdUTIIQQQgghhJC0Q8OKEEIIIYQQQmJCw4oQQgghhBBCYkLDihBCCCGEEEJiQsOKEEIIIYQQQmLSGeabFy5ciEKhYGgq+aJUKvFeaoT3Uy+8n/oolUoAwPupCa5NvfB+6oX3Ux+8l3rh/dRLqVTC2bNn68ZDGVaFQgGvvvqqtknlmauuuor3UiO8n3rh/dTHVVddBQC8n5rg2tQL76deeD/1wXupF95Pvfjv9lqYCkgIIYQQQgghMaFhRQghhBBCCCExoWFFCCGEEEIIITEJVWM1C88DSiWgUAAcR9uE8sjLLwNHjwLXXQcsXdrq2aQfLk1ikpdfBh5+GPjFL4B3vQu45ZYQz63nAadOib/393OBxsS/na+/DoyNAStWAH19rZ4VsQ3/ndDbC7zxhhhbtAgYHRVjP/wh8JOfAO98J3DFFZoeTb6I9ML7qZ0HHgBGRoCBAeCuu1o9m+wQzbAaGQFuvhmYMweYmAAeeUR8MiQ0110HfPvb4u/btwMf/rA4uJFo+Euzs1MszQMHANdt9axIVqh+Xn0efxz4jd8Q6+zGGwMO9iMjwJ//uViYANDVBTzxBPfOiIyMAGvWAJOTs8dvvlkYvoQAwPAwsGGD+Hu5PPtaV1f9+vHHqx/N0Gd6voj0wjOndi67DHj7bfH3738fuO8+4Zwi8QmfCuh5YoGPjQFvvim+3nyzGCehePnl+kPav/4r8JnPtGY+aad6aZ4/L16it94K7NnT6pmRLCB7Xn3++7+BnTuB974X+PjHJd9QqYjF6RtVgDjRrV7NvTMCnic3qgBx5vriFxOfErEMzwPuvlu8A8rleqMKkK8ff/zP/gz4+teFs+Tyy4GPfEREs0ZGGvxi/1mvfREND8f+f8olPHNq54EHLhlVPuPjwO/+bmvmkzXCG1alkvAaVDM2xk0jAkePyseffho4fTrZuWSBUkk4CGsZGqJxReKjel5ref55yQuqXAba2uq/uVIB9u6NPbe8ceqU+lAMANu28ZnPI54HvPKKOI5ccQWwa1f0nzU1BdxwA/DUU+Lv4+Piz5o1Dc705bL8RbRuHY2BKMjOnF1dYpxE4tAh+fiPf8yMKR2EN6wKhdleV5+//EtuGiG57jr1tRdeSG4eWUG1NAFhXNH2J3EIel5r+fGPgeeeqxro7hanMxkPPMC90wB0qOSLkRFg8WLgmmtEgGh83MzvmZy8VCYppbtb/iKqVBr8QyJF9mKfnBTjJBKXXaa+9rnPJTePrBLesHIcYPPm+vHxcZ5cQ7J0qboe43vfS3YuWcBxRCq7ivXreX4l0Vm6NJxx9dhjVf/R2alenNPTjFqFpL9fOK0bQYdKPqhNA2+WZtZQaDo7gY0b5deOHTPwCzOO44j83rlzgXe8Q3zdt09ErPhCj8Q996iv/eQnyc0jq0STW3ddoKenfpxRq9Ds3y8ff+opHgii4LrA7t3yaxMTPL+SeHzrW8BLLwkNiiCvHyAELWbhusK6l7F3L/fOEDiOEBfo6QHmzQv+XjpUso8sW6yW7m7xbvjWt8SfH/1IRJXXr5dn7qno7BSGfSDFovyH7t/PxRiFgQHgzBmRyrNvnzBcr71WhCgbFr2RWoIchL/zO8nNI6tEM6wcR27yzpnDvNeQ9PcDHR3ya7ffzj04CoODauNq1y4arCQeS5eKaNToKPAP/yAcqDK+8Q3J4JYtQLtk252Y4MIMycCAkFn/zneAX/4y2KHCW5ttVGng8+cL4/v++4XM+uCgOFRed51QQlu1Cjh4UGTpNcuRI00oAzqOkFmrhZk90XEc8UFv3Eghi5gEZaR++cvJzSOrRG8QLItaMe81NI4j0lVkTE0xJTsqg4PApk3yaxs2cB8mevjQh9S1HGfPShQCHUf95tq2jQszJI4DXH21+BrkUGEyRbaRZYsdPgz80z8J43vLltnG0OnTwE03NScv3d4unJ+33CIM+KZVvpnZox8KWRhlyRL2UtVBdMPKcYBHH529yCsVqi5EoFiUO7EBoTBGolEsivSPWjo6aLASPZRK6mcXEM9vneGlSgmkQmBsVA4VBgqyT3W22Jkz4jHzjW4fzwN27AA++EG5/LpPV5fohPDSS8B3vwv8138BX/lKyL60qsweLsboUMhCC4sWyceffDLZeWSV6IYVINrcV58qJicZlo2A46i7Xj/0EG9nVFRiFm+/LVJAmJpN4lIoBBtWAHDhgmRw5Ur5N7PWKjbFIgMFeaU6glmNb1BdcQWwdataPdZPG/zZz0QN39Kl8p/XNIxa6UUWmnzkkRgfUD55441w4yQc8QyrUqk+JNDeznBABFRRKwqGxcN1RUqIrPXa2rV8t5F4+CIKQUhFLlTFlVRYiU1QoIC3Nn/4Muxbt6rTdru7hUElSxuMhWoxtrUxfS0q1aHJEyeA97yHL3JiFfEMK1lY9sIF4PrrGQ4ISVDUim1u4uG6QmSg9oDLjAyig4EBtdjfJz4hd1jDcdRdGqmwEhtVoIC3Nj94nmjqvXZtcC1Vd7fwBWs1qKr59Kfrx8bGgN5eA78sJziO0AW/8kqqA0ZAlQqoGifhiGdYVYdlqxkfZ0pgBBi1Mkd/v7w/644dopCZkKh4HvDXf10/3tkJPPxwwD90XbXCCiVBY6EKFACUX88DIyMi7e/664MbBff0CIVPVT9JLYyOyqVDn37a4C/NONWNy6gOGBrZ0ps7VyxVEp94hhUg3LXPPFMfDqBSS2gYtTKH6qBVLguji84uEpVjx0R5aS2Dg014wItFeQOeqSl6U2LiunLxGsqvZxc/SrV6tTCoVEaVX0v1+ushVP6iohJWYJ1VdKgOGBnPA3burB+fmaEGiC7iG1aAOJlOT88eK5cZ6o4Ao1bmcF2547BcZr0VicbIiDjEyVi+vIkf4DjioZdBIYtYqMRrAJ5ps8jwsEhlWrVK3pdq3jyx/xuppQrCcYDNm+vHaQhEh+qAkVEtuXvuoQaILvQYVtUpgf7Jtb1d5L8yFBCKoKjVvn08DMTBX6YyLzbrrUhYPE8Y5DLZ5jlzhL+pKVTeFApZxEaVbcnnPVvs2QPceqt4FlX1VLt2Cc2DxAyqamRFf+fPAydPJjyRjEB1wMj09sqfEVkpIImGHsMKEPH0EycuRa7Gxpj3GpFiUS4YNjNDB1dcBgZEobIs+4r1ViQMw8PyVKPubuDxx0O844OaBlNtITYq+fWdO/lqygLDw8DQUPD3zJkDfPazLTx3Ow6wf3/9+MaNXIRRqW1ctmIF8MorvJ8NGB0VwdJqurpYX6UTfYYVID6Z2jcYw92hcRxxnqplYgI4fjz5+WSNvj4hvVsL661Is3ieSCerZc4cYbiHrtsIErJYt46HhRio6is7OvhqSjueJ3ReVPjpf6EcHaZYsgSYP3/22NgYHSdx8BuXvfCCUAakQmBDenvra4InJ1m5oxO9hhXzXrWxbJk8Ze0LX+AZSwestyJxUEWrtm6NoTCmErKoVNgbMCay5310lJlYaefUKbnaKwDs3g185zsimGFcoKIZCgV58RcL/uJBhcBQyEQqqQioF72GlZ/3Wn04qFSEN4GEolAQqX+1UMRCD6y3IlFRRat6esQBPjJBQhbPPx/jBxPHETWqtTAYmG7OnZOP33KLUOW8+moLIlU+KhELvnDiQYXAUMjs+0qF8Q+d6DWsAJHnWl2IPTlJ70EEgs5YFLHQQ1C9FZ2IRIUqWqVFValYFA2wajl4UFTok8i8+931Y5UKHVVZZN68Vs9AgapzNV840WGmVGja2oL/m8RDv2FVKtWHAeg9iARFLMyjqreiE5HIUPUAiR2t8nEc4L775NeGhrgoDUBV+/SyYIF8fHjY0s9UVfDHF050qBAYilKp/lzZ3s4zpU70G1b0HmgjSMSCmUH6UDkRqRpGaimV5Cm6WnuAuK48jAoAGzZwUUakv18eDKSqfXpZtEg+3tlp8UGRUSv91CoEWlFUZycyufXxcYpX6ES/YSXzHsjyiklTqEQstm2jg0sXKidiezs1A8hsjh+XpwFq7QHiOCL1T4W1J0a7cRzgoYfk16hqn05kQsSAeEat9eUyamUGXyGQkapAKLduHv2GFXDJezA0JNy7e/ZQAjMiQS8HOq/1IVMNu3AB+MQn+K4jAs8D9t/j4Sq8goW49OD19Bh4KbmuPCWwXGa4OgZBqvbr13M/TRuq96MsqmwVjFqZxfPY00oB5dbNY8aw8tm5U3hhKIEZGccBDhyQX2OtlT78QGvtu65cBm69lcYVAUb3DuMnE4vwAq7BGSzGDRCOopkZQ97xlSvlKYEMV8eiWJRnAUxM8LamDccB7ryzfnx62vJsA1XUqq2NL/W4jIywp1UAjFiZx5xhRQlMbaic16y10svAAPDss3JFKXqzc87wMAq7bkUPyvg1nMc8jOFR3IyF8HDggKHsk0JBLdfEcHVkgpxVrKtMH/398nGVFLs1yPKHx8YYOogDe1o1hBEr85gzrGQiFuUyP72IrFzJWqsk6O+XN5ykNzvHeB6wfj1qTZwKOjC8qaRHDVBGkAVQLnNBxkCVEtjRQd9f2lApA1odsQLknVoB4Omnk59LVqBDvyGMWJnHnGFVLWLhbx7t7cCVVzI0G4GgVKPbb6dDRhdBZ1mmv+eU4eF6JxGA+d2T+FSxYPZ3uy6we7f82rZtXJAxKBbrz7Wjo8DJk62ZD4nGf/yHfPzAAcsfD9VLnS+a6FCVuiGMWJnHbI3VwABw4oRIeAZEWJah2UgEHfinplLgnUsRKm/2+LjoecWlmyNUjasAtBnLAaxhcFDkotZSqQA7dpj//RnFcUSz9VruvJPPeFrwPGDjRvk16wMVjiNXTKY6YHTY06ohjFiZx6xhBcj1UK3f8ezEdeXnK4C1VropFuWiTcPDrIfNFarGVZs2aeoI3CQrV8rHDx7kISwGS5YA8+fPHuO5Nj0EHSNSEaigOqB+2NMqEEaszGPesGKtlVa2bJE3ueT5Si8q0SZABF3XruV7LxeoGletXp3sPPr7RQGQDApZRKZQqD9kADzXpgVZs1NA1COnIlDBnlZmqO1pRfn1X8GIlXnMG1astdKK48gVAgHWWulG1tvKh++9HOB58kOPkcZVDXAc4NAh+TUKWUSG59p0Mzpa72js6AC++c0UBSoYtTIL5ddnwYiVecwbVgBrrTTjuvL2Nqy10ouqt5UP33sZRyFaYa5xVQMoZGEEnmvTy/HjotSwmqkpYNGi1swnErTuzZET+fUwATmKUZonGcMKYK2VRhxH1ADJYK2VXgYGgNdfl5fT8L2XYTwP2L5dfi0p0QoZFLLQDs+16eTsaQ/f2PwKFmL2abIVAeXY0Lo3Qw7k10dGgCuuAJYvF18bBeQKBXnZMHv46SM5w4q1VlopFllrlRSOA9x/v/y9t2MHcPp08nMihjl1Sl58s359sqIVMihkoR3VuZaHDUsZGcGv9y/GP05eizNYjBtw6TTZqoByLGjdmyHj8uueB6xZI5bJhQvi6+rVwXuWaqllzN5sKckZVqy10gprrZJFtRmVy0JXgEs4Y7z4onxcZdQkCYUstMPDRoq4mN7VUR7Dr+FNzMMYHsXNv4pctTKgHAtGrfSTcfl1mf+vUgH27g3+d65bL2AxNpYZe7PlJGdYAay10gxrrZJFJWZRLlMlMFN4nrxpXFeXMGpaDYUsjMDDRkoolerSNSbRhd/tKuHw4dYHlCOjsu7b2mjdxyGH8uu7dzc+j7S1Bf83iU6yhhXAWiuNsNYqWXznV3d3/TVmbGSI4WG5xPq999rj6aSQhRF4uEgBJ08C58/PGurtnsSzrxXSa1T5fPrT9WNjYyyZiEu1/HqGpNdVyQuNnOulUv2/a2/nMVwXyRtWrLXSCmutkmVgQGxYskgh660ygEq0oqfHPld4kJBFo1wQUkepVB+RnpzkPmoVngds3Fg33HFgHxb2WeL0iAMl28ySMel1xxGlHzLOnVP/O1n/t/FxHsN1kbxhxVorrQTVWq1fnwmnjHX09QFbt9aPs94qA6hEK+68055oVTWqmq+9e/nwh6RQEM9wLSxxsQTPA/7xH+s9ifPnA0uWtGZOulHlnXIRxiej0utLl4b/NzL7PZVqmpaSvGEFsNZKM64rT0+bmKC31RRB9VY33cTIVWpRiVYsX57sPJpFlQsyMcGoVUgozGYxfqThjjvq0gBRqWSnEM5xgM2b68e5COOTA+n1ZpE9LuPjIsuWxKc1hhXAWiuNOI681h4QWU20VfUTVG/FyFVKsV20QkaQkMWuXTyMhYSy6xZSHWmoNqp6ezOn8gaA6oCmyKj0+oIF4cYB8bjs21c/vnEjl5gOWmdYZXSRtwrXlZdbTE5SIdAUfr2VyriiUmDKSINohQzXBTZtkl9bt46LMASqqNXMDH1+LSEo/e+hh7Kp8sbQqRkyKr2+aFG4cZ8lS8RjVA1jG3ponWHlL/Lq0GylIiQxSSRU5RZUCDRHXx/w2GNUCkw9aRKtkFEsyhVVKhV6VkIiE2ZjYXcLaJT+97GPpf5QrIRRKzNkUHr9jTfCjfsUCvV+RLaX0EPrDCsAWLFCCFf4TE6yzioG/f31fVgAKgSaJkgpkClEKSFtohW1sPeCNijMZgF5S/+rhVErc2RUej0KbC9hhtYaVqVSvaufscjIOI7IWpJBhUCzqJQC29sZMEgFzz0nH7dVtEJGUO+FPXuSn09KKRRE6l8tdJIkgH/QlXmqspz+J4NRK7NkRHo9aiog20uYo7WGFXtaaSdIIZAiYWaRKQVeuACsWpXaPTsfDA8DDz5YP26zaIWMoN4LQ0N8YzaJKljQ0UGfn1GqD7qrVgFvvz37etbT/2pRLcS2Ni7EuGRIej1OKqCsvQQdSPFprWHFnlbaCVIIpEiYWfzlXOtkTPGenX08T676AtgvWiHDdeU5qQCwYQMXYZPInCSjo5QjNobsoNvWJjbTDAkNhEZW8Dc2RudzXCi9Xme3L4SHq/AK/keHl6fbYITWGlYAe1oZIEgkjCmBZhkYAJ59Frjsstnj9HZbyvBwfdQcEGHfNIhW1OI4IvVPBRdhU1COOGFkB92eHrGZZkhoIDSqgr8jR5KfS5bIkCp11FRA4JID6bMYwRksxrdxLf736GL83kkGNuLQesMKkPe0YnFKLIpFNg1uFf39l/wEPvR2W4jnibwHGQcOpNc77rrylMByGTh+PPn5pJQlS+r3UMqua8avqertlR90+/svCQ3kEdVBn+kn8ciQ9HrUVEBA/O8+uc/DI7gZ8zCGBXgT8zCGd2xkYCMOdhhWMu/BhQvA9dczJTAiQSmBzKE1C73dKaFUkqsUbNqUzmhVNStXylMCt2zhImyS3t76GgTKrmukuqbqyitFlkoGDrpacRxg82b5Nab2xiMj0uvnzoUbr+VTS0romZ/vtEjd2GFYVXsPqhkfZ0pgDFQpgdPTfGZMI2u+xyCsZRw/Lm8IvHp18nPRTaEg19Itl6li0ySUXTeIrKbqkUdEWUDKD7raUdVNlsuMWsWlWnodSKX8+oIF4cbrKBTQPl4jFMOGVrGww7ACxCb6zDP1xSm0nGMhOyMyI8g8hYIQsaqGCoEW4Xlyxa2eHnGiTjtUsYmNSnZ9+/ZUnbvs5NSp2T0sAfGuHx3Nd+qfjKC6SUqv6yOl8utxaqx+BRtaacUewwqQF6ektKDQFmTla4A4U3I/NgcVAi1HJVoxM5Od/SZIxYZpRA1xHNEfupbJSUaeYzEyIjxMFy7MHue7Xo3qWab0uh5SLL8ep8YKABtaGcAuw8o/jVaHvSsVkRpAIqHyulLEwjxUCLQUzxOeXhlpFq2QUSwyjSgGqv7Qzz+f7Dwyw+nTwE03iYNrNT09rKlqhCz9hNLrekix/HrcGis2tNKPXYYVAKxYMTtFYHIyNZ4DGwnKCGIWgXmoEGghe/fKa6uyIFpRS1Aa0bZt3AAa0N8vzle1HDwI7NmT/HxSzciIuKG1h7jLLhMeKNZUBcOiP3OkWH49do0VO6Jrxz7DqlSq17hl1X8sVFkE4+OsYzcNFQItY3hY1BjV0tMjojtZxHXlTZArFW4ADXAc0SdaxtAQjaum8Txg7Vq5Z3x6WhhcJBjVIZ8e0vikWH5dVUv1gx+E+CHsiK4V+wwrSq8boViU11qxjt08VAi0BM+TGxiA8Nil4CUamZUr5eN79/JQ1gDXlfcEBGhcNc3wsDxK3N2dmgNsy1FJr9NDqoeUyq+r6ug3bQqxtas8wHfeyfdDBOwzrCi9bgRZtHchPFyFV3D/eo+31SBUCLQElWBFd3f2UgBr6e8XqR21sNiyIUHp1ACNq4aoahrnzBHepZQcYK3AdekhNUm1/HpKpNcLhfpyAwCYmgrpvJV5gMfHua4iYJ9hBail1zs7mfMZg2rP62cxgjNYjG/jWvzficV4a5gnfFNQIdAC8iRYIcNxgEOH5NdYa9UQ1wV271ZfHxri+UOJKlq1dSvQ15f8fNKMqh4GoNKnTlIkve44mrLYCwVRV1YLRSxCY6dhBcir/s+fZ85nDHzP60J4eAQ3Yx7GsABvYh7G8D//ci0fHoNQIbDFqA53WRSsUMFaq1gMDgYbV+vXcwutw/OE4V5LT09+njvdqBoGU3pdDymUXlf1tA/Vy4oiFtqw17Bi1b8RXBcY3lTCJGZvzG0M+RqHCoEtQhWtyrJghQrWWsUiyLiamKB9WseXvlSfBw2I2o2sR4lN4Tjy/Wx8nNLrOkih9HrsXlY+FLHQgr2GFSDP+bR8gaeBTxULeEePRJ2J6kJGYX1oi1DJq2ddsEIGa61iE2RcsdSlCs9T3yhVgzDSHMuWyWutjhxJfi5ZI8XS67FhQEMLdhtWsqr/cplembg4DtpkIV+qCxmH9aEJEySvnsdUJNZaaWFwUN7CAmBK4K+K/jdulFfVd3RQXj0uhYJI/auFln18Uii9rkr5C5UK6CM7pDAdMBR2G1bVC9wPT7a3A1deaXUxYSqgulBLUNWHMlhogDzLqwcRVGu1Y0fy80kpxaJchj13wb9q9TS/6H/pUuCpp+TfPzSU32dPFyrpdYAiFjqolV5fscJqhUBtqYCAPKDBdMBQ2G1YAWKBnzhxyfM1NpaKYkLrCVIXyr3L1Ryq285goQHyLK/eCFWt1cGDObMKohMkw54pR4nnAUePAl//uvhz9Chw+rQ4aA4Pz1ZPW7NGvJ9l3iNAOEbzVtdoCpWIRbnMZ1gHvvT6Cy9YrxB47ly48UBYsxAb+w0rQN4BjR1W46PqfJk7l2uyMFiYAHmXV2+EqtYKANat4wu0SVxXnhKYmfTe4WHgN38T+OhHgRtuEH8++lHgve8F/uiPgFtvna2epjKofNau5bOnC8cRjhAZ27fzGdZBShQCFywIN94Q1izEIh2GlayY8MIF4PrrrfQepIbcuFztgsHCBKC8ejBBtVaVCp1WISgW5Y6S1G+he/YIw0mm6geIyEhY3v/+eHMis1Gl9U5O8hnWQUoUArXWWAHsaRWTdBhW1bVW1YyPW+k9SBWZd7naSVCwkCmBMaG8enOoDmUA8Pzzyc4lxQSl96Z2Cx0eFrVQYenqCr6+YkW0+RA1qrRePsPxSYlCoNYaK0C9qTFTrCnSYVgBotbqmWfqO6x2dlrnPUgdKpcrvRPGCAoW7tolnMUkIqpoVZ4FK1Rs2SL20FoOHuQiDIEqvTeVGVmeJwQQwtDbKxyfTzwBHD4svEa1qnXr1gF9ffrmSQT9/XKDlvWS8UmJQqDWGisfWU+rCxeAVauYKdaA9BhWgLzD6vnzVCuJi8o7MT1No9UgqmAhIJzFfCdGIChaxRTAehwHuO8++TUuwqZxHFHbXUsqM7JKJbmxXUtPjzh4HT4MvPiiUE8bGBDP2RtvAN/7HvDSS8DjjwM/+hHw4IOmZ55PHAe49175NSoExqdWIXBgoNUzqkN7jRVwyais9RiNjYlaSa4rJekyrNi8zByf/nT9WLkMHD+e/FxyhEqyGQDuuEMIcHFph4DRqvCo1MUAClmEQNXz9tixZOcRG5ncsk9nJ/C1rwlD6Z//WRw0XVeop1U/X76i2tKlQimQkSqzUCHQLP56tvQdor3GymdgAHj22fpMsVTnOZsnXYYVIFcrsbCYMHXIlBcBcSDlwcoYQSmBk5PAn/wJ8Nu/Ddx9Nz+Ghpw+Le/DxGhVMEHqYhSyaBpVRtb+/Sl7doM2JX899PVZfdDMHVQITJbq3m0WoL3Gqpr+fmBqqn489eo85kifYSXzppXLIsebRKdQAGZm6scpvW4c1wV275Zfm5gQf3btAi6/nB+FkpER8QKQKZUxWtUYClnERpWRlUrnbtCm5BeB+gfL6kOm/3e/z9Xp08DDDwODg8DLLyf7/5A3qBCYDH4TbIv7Wmklk+o8ZkmfYVVdTOgX1rW3A1demf0FbhJKr7eUwUF1vZXP1JRQP6amQA2eJ3K+ZUbV3LmMVjWLQshihkXwTaMSsUilDlDQpjQ0BFxzDfBbvyVC6tdee+nvy5aJPlcf/rD4+hd/AXzpS8Af/qHogUXMQYVAs1ja18pYKqBPpjY286TPsAJE3ueJE5eELMbGrFngqSZIep0a4MYpFutFeGQMDdG4moWqrqq720oFJ2tRCFm0AaisYxF8M6icuzMzKc1WDyoCPX9eREMmJsQh0//72Ji4LqvTOnqUkSuTUCHQLJb2tTKaCghQfj0k6TSsAHlNEKXX46OSXt+1ixuzYaqDsaqzjA8F2y6iUgGcM0ds+BYqOFmN62JGUgT/dqUL/+9UKfn5pBCZDtD4eEqz1YMyGaJy9Kjen0cuQYVAs6Skr5URKL/eNOk1rGQLnNLr8VF5JgCRv82N2Si+suu//IsocWgPeEL5cUAdrdq6lUpkUXAclIoHUVtt2YkpHHq+0IoZpY7RUXnk+emnk5+LFlz3Um+qWnWwKFx3XfyfQdRQIdAclva1Mp4KCFB+PQTpNawovW4O15WHTChkkQi+suvgIPDzn6s1BSYmcp6hyZ5VRugtuljXeRjj6MZb6MXbmIu1eARbDzpMQW0ClQ5QqssR/N5Ux44JI8s/WHZ1iUP8O97RXO+r664TEuzEHEEKgdu2pXgRWoKFfa1UKX9Hjmj+RZRfb4r0GlaAXHqdOZ/xoZCFNfgfRSOBrlzCnlVGcBzg8vtcLMIbuAYvYjHO4GsQhwemoDamOui/EB6uwitYCM+GUox4+B4f1710sPzZz4Cf/hT4278VXh5ZRKujQ4gqvPQS8K1vJT/vPKJSCKxUcu6N04Tlfa18du82cFyj/HpD0m1YyaTXL1wArr+eOZ9xCRKy4MkqcRoJdOXOuGK0yiiuC7w1x8GruBpnMfvwwFKNxrgusLprBGewGN/GtTiDxbh+bCQ7pRjVB8sXXhB1FvfcI96/1XR3Az/8IfDcc4xUJY1KIXDvXj7AumlxX6v+fuG/qGVqykCcgfLrDUm3YVWd71rN+DgVAnWgErKgZ6IlBAl05c64YrTKKEHZREDKIy8J4MDDI203Yx7GsABvYh7G8CjWwkHG9s1q+enz5y+N9/aK9/Jjj7HWsVWoTtu5zyHXjAV9rRwHuP12+bVz5wz8QpX8+o4donddzkm3YQWInM9nnqlPQaBCYHzombCKRgJduTGuGK1KBNeVqq+jXAaOH09+PqmiVELn3NkCAp2TGdo3fQ/9qVP1Qgnz5wMPPWRN/UlucRzg0CH5Nar86sGivlaqAOWCBQZ+mepsWC4Lgz7nGWPpN6wA8UH6Pa18qBCoB5VnglGrluC66nqrhfDwtaFX8MSejH8ujFYlxsqVcoGxLVv4+AdSKMgbVm/fnv4bV+2hX7UKePvt2dcrFeBjH+OzaAOqlH6AOb06sKivVSLKgNXI5NcBse/lXCUwG4YVFQLNwaiVdQwO1htXn8UIzuAKvIjl+F9DV+Ct4Yx6jBitSpRCAWhrqx+ncnMDHAe488768cnJdIsryTz0bW3i+bNIfppUUSzKvSMAs3riYlFfK+NNgmvxS3Fk9Qk5Px9mw7ACqBBoEkatrKPauFoID49jDeZhHPNxAfMwjt7bVmfzs2G0KlEoEBqD5cvl48eOJTsPncg89D09QoLZIvlpUoXjyJ1R5TLw/PPJzydLWNrXKjEGBuTpwECu662yY1hRIdAcjFpZiW9cfRCnMAeTs661TVfExpYlTp+W/z8xWmUUCoRGpL9fpAXVsmdPei1SlYe+vz8V8tO5Zdky+eF32zY+xHGxpK9V4qmAPn19wNat9eM5rrfKjmFFhUCzMGplJYODwAZJu5I2QMi6ZUXNYmREbNKyuhVGq4yjEgjNQsmQMRwH+Pzn68fT1kuoWko67x76tKLK6QVYa6UDC/paJZ4KWA3rrWaRHcMKUCsEjo3RKxMXRq2s5eNb+oGOTvnFLHR09TyxOcuMqrlzGa1KgKyWDBlHlQ74wAPpOGzIpKQt8dCTEDSSlGWtlV5a0NdKJatuRG69FtZbzSJbhhUgVwgEGFnRAaNWduI4aDv0kPr6bbel+/NR1VV1d9NbniAqG4FlGgGoeglNT9ufqhskJW2Bh56EJKh/Ah9ifbSor5VKVt2I3LoM1lv9iuwZVo4DbN5cP97RQddqXBi1spcgHfY0HOJUqFQA58wRzzO95YmhKhk6eJCPvxLHEVFjGban6lokJU00oeqfwForPbSwr1XLaqyqCaq3+sAHcrPGsmdYAfLICoUs9MColb0MDgLrJQVXQHpPv6po1datYhMnieE4wL33yq+xTCOAYlEetQLs7uptkZQ00QRrrczSQmdES2usqlHVW01OArfems5zSEiyaVg5DvDooxSyMAGjVnazZYv6EPe5z6UrHO95wM6d9eNUAWwZrit3eLe1MZChxHGAQ4fU1200rjxPfKD79lGoIksE1VqxOV18WuiMaGmNVTVB9VYAcMcd6TqHRCCbhhWgFrLo7OQJIC6MWtlL0CFuZgZ43/vSE7UtlcTzWgtVAFuGqiXO+DjQ25v8fFJDUKouYJdxVV0jsnGjMK4oVJEdgtbitm18h8ehhaqZLa+xqsavt5LljvvtGdJyDolAdg0rQC5kcf48cPJka+aTFRi1shvXVacETk0Bq1PSPPjkSfG8VkMVwJazbJncr3LkSPJzSRXVXb1l2KDgKasR2bhReNzpzMgOqrTxtLUCsJEWqWZaUWNVTV8f8OCD8msZl2HPtmHlOMLbVsuGDZkPRRqHUSu7CUoJTMPL8/Rp8ZzWsm8fD3gtRlWmsWtX6+0C62lkXK1f39r9k4IV+WHlSvn43r18h8elBaqZ1tRYVeO6wOHD8shVhh3x2TasAGDJEmD+/NljOe4IrY2gqJXth/Y80Kiuw+Y+OqpmwPPni+eZtBSV8CrA+vemCDKuJiZae9igYEV+ULUCaPUazCIt6GtlDa4LvPZarmTYs29YFQrCQ19LuUwhi7ioolZ0XdtBIwl2Gw3goGbAlQoPeJagErFg/XuTBBlX27e39r20ebPY1ylYkW2CnG+stdJHQn2trEsFrCZnMuzZN6yCFEooZBEPVdQKaH1KCxEESbDbGLViM+BU4DhCwV9Gq+2C1KB6NicnW9Nz0T8A7tkjcj2HhihYkXUU9bgzlQou7LDQ8ZY2Euxr5af8LYSHq/AKFsKbNd5yciTDnn3DCrikUFJrXFHIIj6uKzdamU5gD1u2AO2SR922qJVKXp3NgK1EpZHSKrsglajqXI4dS3YesgOg7Fkk2UOyBtsAdBzci78fpockFgnWLJ47B3wWIziDxfg2rsUZLMYNGElebl1FjmTY82FYASIUKevfsHEj3atxCOqLQSELO3Ac4K675Ndsilqp5NXZDNhabLELUkt/v7yw+8CBZJ9LilbkF0Wt1QTmYN+GkjWvh1SSYM2iAw+P4GbMwxgW4E3Mwxgexc1wYNEHmBMZ9vwYVoBcyGJsjJGVuLgusGlT/XiGVV9SR7Fof9SK8uqpQ2UX7N9vj71uNY4D3Htv/XhHR7JGDUUr8svFWquZmuFOTOH/lAt8hcchwb5WH1xQkozOKMZbSA5k2PNlWKmELO6/PzMhyJZRLFJ+3WaCola7d7f+M6K8eipR2QX0qYRAVnswOppsmnoLG5sSC3BdjO4+jHF04y304m3MxVo8grNwqGMRl4T6Wv36ol7MxdissbkYx68vsrBze8Zl2PNlWKl0gicmMhOCbBlsGmw/qqjV1FRri2Ior55qVOKgFLFoElW/xaTT1FvU2JTYwfxBF3vWv4Fr8CIW4wy+BvH5VypCFZvEIIm+Vm+8gdr2gm0Xx60kwzLs+TKsAPUpgPLr8WHTYLsJilq1Cr9onvLqqcVxgDvvrB+niEUIliypL+qenk6+xslxxDNXKnHPziF/sNLBq7gaZzHbADh4kP5RYoAgGfYUBzvyZ1g5DvDoo5RfNwGjVvazerV8/Ac/SHYePirBCsqrp4rly+XjFLFokt7eeudCuQwcP57sPBLquUPsRNUzGGDzb+3obhpsdSOrAFQy7Cmut8qfYQVQft0kjFrZzeio/PPZtKk1n49MsKK7m/LqKYMiFjFRPZdbtiR3AxPsuUPsJKhnMEC/szZMODBUKX9HjsT/2SYJkmFPqVM+n4YVQPl1UzBqZTeFgkgxqqUVdVYqwYoDByivnjIoYhGTQgGYqdVlQ7LpgJRcJxC+0fvuqx8vl4Hnn09+PpkjaQeGDeJUjfCDHbJ6qxQ65fNrWAGUXzcFo1b24jiiy7mMJN+aFKzIHBSxiIHjyBUCJieBXbuSmQMl18lFVq6Un3G3bQP27El+PpnClANDlcfZanGqZlHVW42Pi/EUvUTybVip5NdpAMSDUSu7UXV1PXQomXVPwYpMQhGLmCxbJj/N/v3fA1/8ovnfT8l1cpFCAWirlZi7yNAQX+OxMOXAcBzx4aQZlXdueDhVNZ/5NqxU8us0AOLDqJW9tNqzRcGKzEIRixio0gGB5EIFlFwnEFuwrFLCZ906vsYjY9KBoRKnsl3AwkfllAdENllKxCzybVgBNABMERS12rs3+fmQS7Tas0XBisxCEYsYOA6wc6f6+tBQMsZVdc8d3cplJDW4rijPkVGpMAodC1MODJWAha29rGSoVAKB1AQ9aFipDAAW7cZHZbTu2pWKhyPTtMqz5XlCIKYWClZkAopYxGRwELjuOvX1JPOwKL2eewYHgfXr5dcoZBETE02Dz50LN24jfkRPdnYEUhH0oGEFyA0ASq/HJyisu3699Q9HpmmVZ+vUKaC9ZtuhYEWmoIhFTJ58Ut1MCEhm76T0OrnIli3yzG02DdaMjujwggXhxm1lYAB4/XXxMqklBV46GlaAMAD2768f37BBSEKT6LiuvD/BxIT1D0emUXmwTLohR0aAVauACxdmj1OwIlNQxCImjZoJTUyYT6em9Dq5iOPI5dcB4PbbaWtrQVd0OK1NgmU4DnD//XIv3Y4dVp/NaVj5yKTXy2VRNMAUiOgEVcGmIKSbWVQeLFPKgJ4nCk/HxmaP9/RQsCKDqEQsmD7UJK4LHD5cH931MZ1OTel1UoXrygUrp6bkXQJICHRGh7NQY1WNKuupXAY+8AFrnfM0rHxU0uvlMlMg4uK6wKZN9eMUsmgdSSsDDg+Lz7uayy4Dnn2WghUZRCViwfShELgu8POfA2vWyK+bTAmk9DqpwnGAYlF+jc90THRGh7NQY1WLSsxiclL05LRw8dGw8vFfJLK0NTYNjk+xSCELm0hSGdDz5GpnU1PiBE4yh0rEAhAZ1vRTNYnjCGk2VTq1SccUpddJFcWiOoDKlMAYMDocTNDZHLCyXp+GVTUDA8JbL/sAmbYWDwpZ2EdSyoAywQpArAd6wDOLKn0IYKlOKILSqU07pkwol5FU4jjAl78sv8aUwBjojA5nRbyiFv9sLkuDSKLmNCQ0rGrp6xMyOLVEUCJhC5AaKGRhF0nkY6sEK+bOlSv+kMzgOMIfVUu5DBw/nvx8Uo0qnRpI3jHFF1tucV21/DpTAmOgKzqcJfGKWvr6gAcflF+zLPOJhpUMDU2DR0bEWv7IR8RX6l+AQha2YTofu7ootxoKVuSGZcvkUastW/i4h6ZYbL1jir2tcs+WLUwJNIKO6HDWxCtqCXIw3XGHNUqBNKxkqNLWmoxaeR5w443CMzs+Lr7eeCM3HAAUssgTshRAClbkikIBaGurHy+XrXIwpoNWO6bY24qAKYGJwciwHJWDaXLSGqVAGlYqYkStjh0Dpqdnj01Pi3ECClnkAVUK4PQ0BStyRJAtwIbBEQhyTJneO9nbilyEKYGGiRoZznIqoE/QS8USpUAaVipiRK1eey3ceO4IErKgZFj6YQogqUJ1CGPD4IioHFOmo1ZULyNVMCXQEHEiw1lPBfTx+/zJxCyAlgui0bAKQkOtFVGgErKYmaEHNO3IPNtMAcw1K1fKx9kwOAJBTj+T6dTsbUWqYEqgIeJEhrPYx0qF64pohYVKgTSsgogYtVq4MNx4LnEc+c47MUHJsLTT21vfDJgpgLmGDYM1o3L6mU6nVqmXsR4klzAl0ACMDDdPI6XAPXuSnc9FaFg1IkLUqjYDyue739U4ryywbJk8anXPPXxBJ4GJnhcjI8CVV17KEenpoWebsGGwblqZTl2rXkalwFzDlEDNxIkMZ7WPVRBBSoFDQy2x7mlYNSJC1Oqd75T/qK9/nZvMLFQeGAsbvmUS3YWu1bnhvndhZgY4cYIpgIQNg3VjQzo1lQJzD1MCDRC1r1UexCtkqJQCgZbIsNOwaoaQUasrrlD/KCoDVhGk7kKFQPPoLnSVyat3dwOjo9F+HskUbBismaB06qSK16gUSMCUQCPURoabSbfNi3hFLY2UAhOWYadh1Qwhi4WDSkmoDFhDUBiXeQRm0VnoqpJXZ244qYINgzWjSqfeti2ZgwTrQchFmBJokGbTbfMkXlGL6wK7d8uvJSzDTsOqWUIUCzsO8Md/LP8xtedOAhHGlZ22pqaox2ySl1/W83Mor06ahA2DNRNkwNx2m/nTLJUCyUUapQQyuz8iYdJt81hjVc3goBUy7DSsmiVksfDv/V4Cc8oKjiOMKxnUYzaD5wGHDsmvhd2EZSmAlFcnEtgwWDNBN3R6OpnTbJh6EKoHZpqglMAHHki81CUbhEm3zWuNVTUWyLDTsAqDqvqa7tb4FItAZ2f9OBO0zXDqlHAj1tLREU4WXZUCSHl1ooANgzUTdJrdvTsZI6a2HkQG1QNzgSolcHoaeN/7+LGHJky6bV5rrGppsQw7DaswOI446Mtg0+B4OA5w333ya0mktOQNVc717bc3n8bDFEASETYM1ozqNGtLOjXVA3OD4wB33SW/NjUFrF7Njz0UYdJt81xjVUsjGfa77za2EGlYhUX1YTXZ9f7f/93AnLKCKiKYVEpLnlCl+6lOvDJKpfooI1MASROwYbBmgk6zNlirVA/MFcWiWsiiUqEEe2hk6baytNq811jVEiTDvmuXscg5DasoFItqIYuLIUZVL6ujR+mtURJUa7V3L2+cTnTkYp88CZw/P3uMKYCkCdgw2ABB6dSGU18aQvXAXBEkZAHQgRKJ6nRbVVqt6v39gx8kN0+bCKpBBUTkfO1a7S8cGlZRCBKyuNjp+ZOflF+enrYjM8NaikVR51PLxAR3Yp088YR8vNlcbM8DNm6sH9+3jymApCnYMFgzQenUF99LLaPZdCaKW2SGIPVrgBLskQlKqx0dlTv989zPotFCbDLbLAw0rKISdCq4/Xb0LfTw0Y/KL+cx3bVpHEetVrdtW343B514XnwPtkwJcP58YMmSeD+X5AY2DDaA66pTX1p9km2kHkhxi8wxOKg+01KCPSJBabWFAjAzo/53ecWXYVed2TULWtCwikqQkMXUFLBjBz7zGfnlt94yN61MoFK5qlS4E+sgriKgSgmwUmFqDwkFGwZroDrKE5T6YoOQhUo9kOIWmWVwMFiCnR9xSILSah1HXsBWLgO9vUnMzl5cF/jpT8VXGUND2owrGlZxCJK5PXgQl/+DPPXil780OKesoBJR4E4cnziKgFQCJBphw+CY+FGea64R9RXDw8HvpWPHkp1fs1DcItMESbDfeGPy80k1srTaffvEs+J5wPvfL/93eZNcl+E4wP33q6P6mtQCaVjFRbVjAPjj527HQtR/QCphC1JFf7+81ooKgeZYurTx97AZMNFIUICFmb8NqHZynD8vrNFbbxXG1ZYtciGL/fvtvKlhxS1Yi5UqgkQrjx4FvvjFRKeTfqrTavftE/XOfgqtSgWUNSiCRoIWGtQCaVjFJUD+pn16CneCRkAkHEd4D2QwahWPkyfl440kWdkMmBiAmb8RkbU7AIA77gDOnpULWYyP2xkKDNOrh7VYqSRIgn3bNjuXpdU4jnA8bNw4O4X28OFWz8x+GglaxFQLpGGlA8XJoA3AXXigLmrFVMAmUe3EjFpFx/Pk966rK9g48jyx0TAFkBhAlfnLLgsByKI8gIj09PcLB4gs5cXWZvaNxC0A1mKlGMcB/uqv1Ndbra2SSmQptLJMH1JPkLIKEEstkIaVLhQpgZ2Yxt2YXUw4d25Sk0o5QfkDu3ZxF47CqVPi4FXL5z8fbBwND4uNphqmABJNqDJ/2WUhgKCUlnJZ7JEykRpbo1aAWtzCp9laLKYKWsngoPp1QZXACMicK5WK/Hvz2iQ4iGbUAiM0VqRhpQuFEdAGYCMO4hZcepF997sJzivtFIvy6naA7dujoMq/bhStkuliT00xBZBogV0WIuK64mDQ1SW/rjpk2Rq1akQztVjNpgrS+GoJX/0qcMst8mvM8g9JdQqt77FX5VuqmgfnnUZqgQcPApdfHsoZRcNKJ4rUtTYAh3BJyOLv/i7heaUZxwFWr5ZfO3iQu3AYPE99eg3yZsmiVYBoks0UQKKJoFqrzZuTn09qcF3gtdfUSlcybI5aBdGoFqvZVMFGxheNLqPs3EmVQG0MDAAnToibB4hodS1dXaJ5MJHTSC1wakoIAzWpskLDSicBqWtdmMIHIXqI+OufNMkXvqC+pnJ9kXqi9K9SRat6etQeHkIioqq1evhhPuqB9PUBjz0WLs98+/Z0Gg5BtVjNpAo2Mr5URpfM2KIBFolGKoEae7Xmg9FR8U5WMTnJPlaNaKQWCIj0iT/904Y/ioaVboKkb0g0+vqAT3xCfu2b3wROn052Pmnl9dfl4zfdpI48MVpFEkRVawWIwAQf9QB8g+P++9U1A9VMTra+YXBUVLVYzaQKBhlfKqNreLje2KI6YSyCjkpf+AJt1VCohGyqYR+rxjRSCwTEc97A8qcFoJuL8uszNcMT6ML3wXqUyDz8sPraN76R3DzSjEqO8t3vlo97nsjZqIXRKmKIoForQAQpSACOI4SUvv/95lIDVc6WtNKMbHuQ8aVSWduwYbaxtXYt1QljEqQSOD1tby9rK/HXfdAzzz5WzeELWgQFSO65J/BZp2FlAtfFq7ccxjjmYBTz8DZ6sAZP4Czo4Y+M4wA33NDqWaSb97wn3LjskAEwWkWM4rrqtL93vSvZuaSWZlMDa9snZIFGsu1BxpfK6JIZW7UHL5k6IQlkcBD40Ifk137xi2TnknoGBkQEWiVkQ1XA5nFd4Oc/V7+I5swJfNZpWBmisNPFFfgpluM7WIzX8TVc2twHB1s4sTQja3gJAJ/8ZLLzSCvLl9c3FO3sFOMyZIeMuXMZrSLG2bmzXgy0vV29VImE6tRAWSNhAFixItk5JUUj2XaV8SUzug4cqFdXnJqqL5auTTkkTfHYY/LxrC5No/T1AQ8+WD8+Zw4VfMPiOMBXviLvDzA1Ffis07AyhOMAB77q4FVcPStS9fu/3ziFkyjo6wPWrZs9tm6dGCeNcRzgyBGRyjd3rvh65Ij68NFMWg0hBnAc4KmnRGZLT4/4+uSTXHqh8VMD//M/6+tU8753qoyvWqPLdev3wUcf5d6oCb7WNeO3YOjuBubNE2vz8ce5NqPy1a+KQ/ucOUIApIlnXeHGIjoYGBBel7/5G6GGOTAAfPzjrZ5VynnwQeC224B/+zfgD/6Au29Y/EVZKgmPS6PNNuz3E6IJLj2NOI5o5n36NPfOZnCc2QtOtRi5QLXA17pmXBf41Ke4NnUxOAisWdP0/aRhZRjHEeo3RCN9fdx541B7aND9/YRogktPM9w7oyNbjFyg2uDS1AzXpl5C3E+mAhJCCCGEEEJITGhYEUIIIYQQQkhM2mZmZmpbLilZuHAhClS90cLJkyexZMmSVk8jM/B+6oX3Ux+li7Ks3Dv1wLWpF95PvfB+6oP3Ui+8n3oplUo4e/Zs3Xgow4oQQgghhBBCSD1MBSSEEEIIIYSQmNCwIoQQQgghhJCY0LAihBBCCCGEkJjQsCKEEEIIIYSQmNCwIoQQQgghhJCY0LAihBBCCCGEkJjQsCKEEEIIIYSQmNCwIoQQQgghhJCY0LAihBBCCCGEkJj8fyRmBbBd6RbZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1080x216 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1], c='b', s=20)\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1], c='r', s=20)\n",
    "    \n",
    "\n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    if i_batch != 4: continue\n",
    "    inputs, outputs = sample_batch\n",
    "    show_sample_batch(sample_batch)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a53b5",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56c6a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear = nn.Linear(100, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(256, 120)\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        # flatten from [[x, y], [x, y]] to [x, y, x, y]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf38ed7",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e605e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/8, Batch: 10/3444, Loss: 0.14295850694179535\n",
      "Epoch: 0/8, Batch: 20/3444, Loss: 0.09950261563062668\n",
      "Epoch: 0/8, Batch: 30/3444, Loss: 0.27293631434440613\n",
      "Epoch: 0/8, Batch: 40/3444, Loss: 0.17032520473003387\n",
      "Epoch: 0/8, Batch: 50/3444, Loss: 0.07907920330762863\n",
      "Epoch: 0/8, Batch: 60/3444, Loss: 0.04612410068511963\n",
      "Epoch: 0/8, Batch: 70/3444, Loss: 0.03100290335714817\n",
      "Epoch: 0/8, Batch: 80/3444, Loss: 0.02433432638645172\n",
      "Epoch: 0/8, Batch: 90/3444, Loss: 0.047229181975126266\n",
      "Epoch: 0/8, Batch: 100/3444, Loss: 0.08478765934705734\n",
      "Epoch: 0/8, Batch: 110/3444, Loss: 0.04767847806215286\n",
      "Epoch: 0/8, Batch: 120/3444, Loss: 0.02191113494336605\n",
      "Epoch: 0/8, Batch: 130/3444, Loss: 0.034458331763744354\n",
      "Epoch: 0/8, Batch: 140/3444, Loss: 0.05885963514447212\n",
      "Epoch: 0/8, Batch: 150/3444, Loss: 0.06793691217899323\n",
      "Epoch: 0/8, Batch: 160/3444, Loss: 0.04420091584324837\n",
      "Epoch: 0/8, Batch: 170/3444, Loss: 0.057841528207063675\n",
      "Epoch: 0/8, Batch: 180/3444, Loss: 0.06789841502904892\n",
      "Epoch: 0/8, Batch: 190/3444, Loss: 0.04037722200155258\n",
      "Epoch: 0/8, Batch: 200/3444, Loss: 0.04092992842197418\n",
      "Epoch: 0/8, Batch: 210/3444, Loss: 0.0500008724629879\n",
      "Epoch: 0/8, Batch: 220/3444, Loss: 0.04069695621728897\n",
      "Epoch: 0/8, Batch: 230/3444, Loss: 0.042712487280368805\n",
      "Epoch: 0/8, Batch: 240/3444, Loss: 0.022954337298870087\n",
      "Epoch: 0/8, Batch: 250/3444, Loss: 0.050150081515312195\n",
      "Epoch: 0/8, Batch: 260/3444, Loss: 0.027287425473332405\n",
      "Epoch: 0/8, Batch: 270/3444, Loss: 0.022009409964084625\n",
      "Epoch: 0/8, Batch: 280/3444, Loss: 0.013032018207013607\n",
      "Epoch: 0/8, Batch: 290/3444, Loss: 0.019123733043670654\n",
      "Epoch: 0/8, Batch: 300/3444, Loss: 0.026483746245503426\n",
      "Epoch: 0/8, Batch: 310/3444, Loss: 0.030561603605747223\n",
      "Epoch: 0/8, Batch: 320/3444, Loss: 0.03166709840297699\n",
      "Epoch: 0/8, Batch: 330/3444, Loss: 0.022372456267476082\n",
      "Epoch: 0/8, Batch: 340/3444, Loss: 0.04123853147029877\n",
      "Epoch: 0/8, Batch: 350/3444, Loss: 0.0750364437699318\n",
      "Epoch: 0/8, Batch: 360/3444, Loss: 0.04625542089343071\n",
      "Epoch: 0/8, Batch: 370/3444, Loss: 0.052746206521987915\n",
      "Epoch: 0/8, Batch: 380/3444, Loss: 0.03160024806857109\n",
      "Epoch: 0/8, Batch: 390/3444, Loss: 0.039933424443006516\n",
      "Epoch: 0/8, Batch: 400/3444, Loss: 0.049266792833805084\n",
      "Epoch: 0/8, Batch: 410/3444, Loss: 0.04585910961031914\n",
      "Epoch: 0/8, Batch: 420/3444, Loss: 0.024778544902801514\n",
      "Epoch: 0/8, Batch: 430/3444, Loss: 0.032298747450113297\n",
      "Epoch: 0/8, Batch: 440/3444, Loss: 0.08622366935014725\n",
      "Epoch: 0/8, Batch: 450/3444, Loss: 0.04128004610538483\n",
      "Epoch: 0/8, Batch: 460/3444, Loss: 0.02278422750532627\n",
      "Epoch: 0/8, Batch: 470/3444, Loss: 0.04315488412976265\n",
      "Epoch: 0/8, Batch: 480/3444, Loss: 0.055640578269958496\n",
      "Epoch: 0/8, Batch: 490/3444, Loss: 0.09449940919876099\n",
      "Epoch: 0/8, Batch: 500/3444, Loss: 0.07198695093393326\n",
      "Epoch: 0/8, Batch: 510/3444, Loss: 0.03284258022904396\n",
      "Epoch: 0/8, Batch: 520/3444, Loss: 0.016166269779205322\n",
      "Epoch: 0/8, Batch: 530/3444, Loss: 0.02799680083990097\n",
      "Epoch: 0/8, Batch: 540/3444, Loss: 0.052493538707494736\n",
      "Epoch: 0/8, Batch: 550/3444, Loss: 0.052022967487573624\n",
      "Epoch: 0/8, Batch: 560/3444, Loss: 0.02782251685857773\n",
      "Epoch: 0/8, Batch: 570/3444, Loss: 0.10297342389822006\n",
      "Epoch: 0/8, Batch: 580/3444, Loss: 0.037780944257974625\n",
      "Epoch: 0/8, Batch: 590/3444, Loss: 0.03188883885741234\n",
      "Epoch: 0/8, Batch: 600/3444, Loss: 0.03408162295818329\n",
      "Epoch: 0/8, Batch: 610/3444, Loss: 0.04283014312386513\n",
      "Epoch: 0/8, Batch: 620/3444, Loss: 0.03645115718245506\n",
      "Epoch: 0/8, Batch: 630/3444, Loss: 0.06716863811016083\n",
      "Epoch: 0/8, Batch: 640/3444, Loss: 0.04662438482046127\n",
      "Epoch: 0/8, Batch: 650/3444, Loss: 0.025653667747974396\n",
      "Epoch: 0/8, Batch: 660/3444, Loss: 0.06468115001916885\n",
      "Epoch: 0/8, Batch: 670/3444, Loss: 0.028169289231300354\n",
      "Epoch: 0/8, Batch: 680/3444, Loss: 0.02276303991675377\n",
      "Epoch: 0/8, Batch: 690/3444, Loss: 0.029850076884031296\n",
      "Epoch: 0/8, Batch: 700/3444, Loss: 0.01572435349225998\n",
      "Epoch: 0/8, Batch: 710/3444, Loss: 0.015159184113144875\n",
      "Epoch: 0/8, Batch: 720/3444, Loss: 0.04318974167108536\n",
      "Epoch: 0/8, Batch: 730/3444, Loss: 0.01944863051176071\n",
      "Epoch: 0/8, Batch: 740/3444, Loss: 0.041893795132637024\n",
      "Epoch: 0/8, Batch: 750/3444, Loss: 0.02178720012307167\n",
      "Epoch: 0/8, Batch: 760/3444, Loss: 0.07254116982221603\n",
      "Epoch: 0/8, Batch: 770/3444, Loss: 0.09141573309898376\n",
      "Epoch: 0/8, Batch: 780/3444, Loss: 0.10338025540113449\n",
      "Epoch: 0/8, Batch: 790/3444, Loss: 0.022650105878710747\n",
      "Epoch: 0/8, Batch: 800/3444, Loss: 0.022551393136382103\n",
      "Epoch: 0/8, Batch: 810/3444, Loss: 0.013696814887225628\n",
      "Epoch: 0/8, Batch: 820/3444, Loss: 0.03192274644970894\n",
      "Epoch: 0/8, Batch: 830/3444, Loss: 0.07551544904708862\n",
      "Epoch: 0/8, Batch: 840/3444, Loss: 0.020759621635079384\n",
      "Epoch: 0/8, Batch: 850/3444, Loss: 0.03574884682893753\n",
      "Epoch: 0/8, Batch: 860/3444, Loss: 0.024309270083904266\n",
      "Epoch: 0/8, Batch: 870/3444, Loss: 0.01531884353607893\n",
      "Epoch: 0/8, Batch: 880/3444, Loss: 0.05680347979068756\n",
      "Epoch: 0/8, Batch: 890/3444, Loss: 0.03211696445941925\n",
      "Epoch: 0/8, Batch: 900/3444, Loss: 0.01712050288915634\n",
      "Epoch: 0/8, Batch: 910/3444, Loss: 0.029790261760354042\n",
      "Epoch: 0/8, Batch: 920/3444, Loss: 0.03068329580128193\n",
      "Epoch: 0/8, Batch: 930/3444, Loss: 0.033538710325956345\n",
      "Epoch: 0/8, Batch: 940/3444, Loss: 0.025593357160687447\n",
      "Epoch: 0/8, Batch: 950/3444, Loss: 0.06199558079242706\n",
      "Epoch: 0/8, Batch: 960/3444, Loss: 0.014860792085528374\n",
      "Epoch: 0/8, Batch: 970/3444, Loss: 0.028600746765732765\n",
      "Epoch: 0/8, Batch: 980/3444, Loss: 0.05632979795336723\n",
      "Epoch: 0/8, Batch: 990/3444, Loss: 0.023435575887560844\n",
      "Epoch: 0/8, Batch: 1000/3444, Loss: 0.034366436302661896\n",
      "Epoch: 0/8, Batch: 1010/3444, Loss: 0.06723044812679291\n",
      "Epoch: 0/8, Batch: 1020/3444, Loss: 0.023153038695454597\n",
      "Epoch: 0/8, Batch: 1030/3444, Loss: 0.009686711244285107\n",
      "Epoch: 0/8, Batch: 1040/3444, Loss: 0.033336371183395386\n",
      "Epoch: 0/8, Batch: 1050/3444, Loss: 0.026313358917832375\n",
      "Epoch: 0/8, Batch: 1060/3444, Loss: 0.02958344854414463\n",
      "Epoch: 0/8, Batch: 1070/3444, Loss: 0.043834101408720016\n",
      "Epoch: 0/8, Batch: 1080/3444, Loss: 0.025601744651794434\n",
      "Epoch: 0/8, Batch: 1090/3444, Loss: 0.031657274812459946\n",
      "Epoch: 0/8, Batch: 1100/3444, Loss: 0.0429290346801281\n",
      "Epoch: 0/8, Batch: 1110/3444, Loss: 0.05292928218841553\n",
      "Epoch: 0/8, Batch: 1120/3444, Loss: 0.024609319865703583\n",
      "Epoch: 0/8, Batch: 1130/3444, Loss: 0.028617938980460167\n",
      "Epoch: 0/8, Batch: 1140/3444, Loss: 0.026502905413508415\n",
      "Epoch: 0/8, Batch: 1150/3444, Loss: 0.029588928446173668\n",
      "Epoch: 0/8, Batch: 1160/3444, Loss: 0.028711000457406044\n",
      "Epoch: 0/8, Batch: 1170/3444, Loss: 0.025739295408129692\n",
      "Epoch: 0/8, Batch: 1180/3444, Loss: 0.03952082246541977\n",
      "Epoch: 0/8, Batch: 1190/3444, Loss: 0.04281741380691528\n",
      "Epoch: 0/8, Batch: 1200/3444, Loss: 0.044338926672935486\n",
      "Epoch: 0/8, Batch: 1210/3444, Loss: 0.024311471730470657\n",
      "Epoch: 0/8, Batch: 1220/3444, Loss: 0.0196909848600626\n",
      "Epoch: 0/8, Batch: 1230/3444, Loss: 0.024410756304860115\n",
      "Epoch: 0/8, Batch: 1240/3444, Loss: 0.03426201641559601\n",
      "Epoch: 0/8, Batch: 1250/3444, Loss: 0.02036753110587597\n",
      "Epoch: 0/8, Batch: 1260/3444, Loss: 0.03463549166917801\n",
      "Epoch: 0/8, Batch: 1270/3444, Loss: 0.015203062444925308\n",
      "Epoch: 0/8, Batch: 1280/3444, Loss: 0.018192792311310768\n",
      "Epoch: 0/8, Batch: 1290/3444, Loss: 0.06584057211875916\n",
      "Epoch: 0/8, Batch: 1300/3444, Loss: 0.032008323818445206\n",
      "Epoch: 0/8, Batch: 1310/3444, Loss: 0.023305252194404602\n",
      "Epoch: 0/8, Batch: 1320/3444, Loss: 0.01613602042198181\n",
      "Epoch: 0/8, Batch: 1330/3444, Loss: 0.008701745420694351\n",
      "Epoch: 0/8, Batch: 1340/3444, Loss: 0.018667157739400864\n",
      "Epoch: 0/8, Batch: 1350/3444, Loss: 0.043724723160266876\n",
      "Epoch: 0/8, Batch: 1360/3444, Loss: 0.01174138206988573\n",
      "Epoch: 0/8, Batch: 1370/3444, Loss: 0.01778283156454563\n",
      "Epoch: 0/8, Batch: 1380/3444, Loss: 0.024789197370409966\n",
      "Epoch: 0/8, Batch: 1390/3444, Loss: 0.024037128314375877\n",
      "Epoch: 0/8, Batch: 1400/3444, Loss: 0.02508099190890789\n",
      "Epoch: 0/8, Batch: 1410/3444, Loss: 0.03681303560733795\n",
      "Epoch: 0/8, Batch: 1420/3444, Loss: 0.04266702011227608\n",
      "Epoch: 0/8, Batch: 1430/3444, Loss: 0.04861004650592804\n",
      "Epoch: 0/8, Batch: 1440/3444, Loss: 0.02215542085468769\n",
      "Epoch: 0/8, Batch: 1450/3444, Loss: 0.03588296100497246\n",
      "Epoch: 0/8, Batch: 1460/3444, Loss: 0.01349608413875103\n",
      "Epoch: 0/8, Batch: 1470/3444, Loss: 0.0071734716184437275\n",
      "Epoch: 0/8, Batch: 1480/3444, Loss: 0.010560260154306889\n",
      "Epoch: 0/8, Batch: 1490/3444, Loss: 0.046153079718351364\n",
      "Epoch: 0/8, Batch: 1500/3444, Loss: 0.0747973620891571\n",
      "Epoch: 0/8, Batch: 1510/3444, Loss: 0.024304112419486046\n",
      "Epoch: 0/8, Batch: 1520/3444, Loss: 0.01631922274827957\n",
      "Epoch: 0/8, Batch: 1530/3444, Loss: 0.016358569264411926\n",
      "Epoch: 0/8, Batch: 1540/3444, Loss: 0.06969430297613144\n",
      "Epoch: 0/8, Batch: 1550/3444, Loss: 0.04087888449430466\n",
      "Epoch: 0/8, Batch: 1560/3444, Loss: 0.0757916122674942\n",
      "Epoch: 0/8, Batch: 1570/3444, Loss: 0.05067058652639389\n",
      "Epoch: 0/8, Batch: 1580/3444, Loss: 0.03316858783364296\n",
      "Epoch: 0/8, Batch: 1590/3444, Loss: 0.051938775926828384\n",
      "Epoch: 0/8, Batch: 1600/3444, Loss: 0.024943703785538673\n",
      "Epoch: 0/8, Batch: 1610/3444, Loss: 0.05961509793996811\n",
      "Epoch: 0/8, Batch: 1620/3444, Loss: 0.05111745744943619\n",
      "Epoch: 0/8, Batch: 1630/3444, Loss: 0.03997108340263367\n",
      "Epoch: 0/8, Batch: 1640/3444, Loss: 0.02772478014230728\n",
      "Epoch: 0/8, Batch: 1650/3444, Loss: 0.04075215756893158\n",
      "Epoch: 0/8, Batch: 1660/3444, Loss: 0.01824651099741459\n",
      "Epoch: 0/8, Batch: 1670/3444, Loss: 0.02814233861863613\n",
      "Epoch: 0/8, Batch: 1680/3444, Loss: 0.03804156184196472\n",
      "Epoch: 0/8, Batch: 1690/3444, Loss: 0.044655367732048035\n",
      "Epoch: 0/8, Batch: 1700/3444, Loss: 0.04662834852933884\n",
      "Epoch: 0/8, Batch: 1710/3444, Loss: 0.03473149612545967\n",
      "Epoch: 0/8, Batch: 1720/3444, Loss: 0.03571546450257301\n",
      "Epoch: 0/8, Batch: 1730/3444, Loss: 0.12061619758605957\n",
      "Epoch: 0/8, Batch: 1740/3444, Loss: 0.03227778151631355\n",
      "Epoch: 0/8, Batch: 1750/3444, Loss: 0.017489904537796974\n",
      "Epoch: 0/8, Batch: 1760/3444, Loss: 0.014317629858851433\n",
      "Epoch: 0/8, Batch: 1770/3444, Loss: 0.023017656058073044\n",
      "Epoch: 0/8, Batch: 1780/3444, Loss: 0.02836328186094761\n",
      "Epoch: 0/8, Batch: 1790/3444, Loss: 0.01412567775696516\n",
      "Epoch: 0/8, Batch: 1800/3444, Loss: 0.025968527421355247\n",
      "Epoch: 0/8, Batch: 1810/3444, Loss: 0.03340240940451622\n",
      "Epoch: 0/8, Batch: 1820/3444, Loss: 0.04177334904670715\n",
      "Epoch: 0/8, Batch: 1830/3444, Loss: 0.04908929020166397\n",
      "Epoch: 0/8, Batch: 1840/3444, Loss: 0.03908812254667282\n",
      "Epoch: 0/8, Batch: 1850/3444, Loss: 0.04517460614442825\n",
      "Epoch: 0/8, Batch: 1860/3444, Loss: 0.01650637574493885\n",
      "Epoch: 0/8, Batch: 1870/3444, Loss: 0.04635559394955635\n",
      "Epoch: 0/8, Batch: 1880/3444, Loss: 0.024274813011288643\n",
      "Epoch: 0/8, Batch: 1890/3444, Loss: 0.0492868535220623\n",
      "Epoch: 0/8, Batch: 1900/3444, Loss: 0.030509259551763535\n",
      "Epoch: 0/8, Batch: 1910/3444, Loss: 0.010920564644038677\n",
      "Epoch: 0/8, Batch: 1920/3444, Loss: 0.0634307712316513\n",
      "Epoch: 0/8, Batch: 1930/3444, Loss: 0.029538484290242195\n",
      "Epoch: 0/8, Batch: 1940/3444, Loss: 0.017368419095873833\n",
      "Epoch: 0/8, Batch: 1950/3444, Loss: 0.029763326048851013\n",
      "Epoch: 0/8, Batch: 1960/3444, Loss: 0.06237638369202614\n",
      "Epoch: 0/8, Batch: 1970/3444, Loss: 0.02827812172472477\n",
      "Epoch: 0/8, Batch: 1980/3444, Loss: 0.05476230010390282\n",
      "Epoch: 0/8, Batch: 1990/3444, Loss: 0.0225484911352396\n",
      "Epoch: 0/8, Batch: 2000/3444, Loss: 0.017418790608644485\n",
      "Epoch: 0/8, Batch: 2010/3444, Loss: 0.019272901117801666\n",
      "Epoch: 0/8, Batch: 2020/3444, Loss: 0.0767441913485527\n",
      "Epoch: 0/8, Batch: 2030/3444, Loss: 0.05372697114944458\n",
      "Epoch: 0/8, Batch: 2040/3444, Loss: 0.03246848285198212\n",
      "Epoch: 0/8, Batch: 2050/3444, Loss: 0.025944147258996964\n",
      "Epoch: 0/8, Batch: 2060/3444, Loss: 0.04743633419275284\n",
      "Epoch: 0/8, Batch: 2070/3444, Loss: 0.02615390531718731\n",
      "Epoch: 0/8, Batch: 2080/3444, Loss: 0.04618662968277931\n",
      "Epoch: 0/8, Batch: 2090/3444, Loss: 0.011038943193852901\n",
      "Epoch: 0/8, Batch: 2100/3444, Loss: 0.031416475772857666\n",
      "Epoch: 0/8, Batch: 2110/3444, Loss: 0.02544669806957245\n",
      "Epoch: 0/8, Batch: 2120/3444, Loss: 0.049219921231269836\n",
      "Epoch: 0/8, Batch: 2130/3444, Loss: 0.01347297988831997\n",
      "Epoch: 0/8, Batch: 2140/3444, Loss: 0.02739027887582779\n",
      "Epoch: 0/8, Batch: 2150/3444, Loss: 0.01649937406182289\n",
      "Epoch: 0/8, Batch: 2160/3444, Loss: 0.038149137049913406\n",
      "Epoch: 0/8, Batch: 2170/3444, Loss: 0.012362796813249588\n",
      "Epoch: 0/8, Batch: 2180/3444, Loss: 0.015035729855298996\n",
      "Epoch: 0/8, Batch: 2190/3444, Loss: 0.018223877996206284\n",
      "Epoch: 0/8, Batch: 2200/3444, Loss: 0.029899610206484795\n",
      "Epoch: 0/8, Batch: 2210/3444, Loss: 0.08119511604309082\n",
      "Epoch: 0/8, Batch: 2220/3444, Loss: 0.03458567336201668\n",
      "Epoch: 0/8, Batch: 2230/3444, Loss: 0.09267041832208633\n",
      "Epoch: 0/8, Batch: 2240/3444, Loss: 0.05294109508395195\n",
      "Epoch: 0/8, Batch: 2250/3444, Loss: 0.13819018006324768\n",
      "Epoch: 0/8, Batch: 2260/3444, Loss: 0.058246999979019165\n",
      "Epoch: 0/8, Batch: 2270/3444, Loss: 0.06385870277881622\n",
      "Epoch: 0/8, Batch: 2280/3444, Loss: 0.07873206585645676\n",
      "Epoch: 0/8, Batch: 2290/3444, Loss: 0.0446169488132\n",
      "Epoch: 0/8, Batch: 2300/3444, Loss: 0.06958488374948502\n",
      "Epoch: 0/8, Batch: 2310/3444, Loss: 0.026507578790187836\n",
      "Epoch: 0/8, Batch: 2320/3444, Loss: 0.048002369701862335\n",
      "Epoch: 0/8, Batch: 2330/3444, Loss: 0.04768248274922371\n",
      "Epoch: 0/8, Batch: 2340/3444, Loss: 0.057468630373477936\n",
      "Epoch: 0/8, Batch: 2350/3444, Loss: 0.10726656764745712\n",
      "Epoch: 0/8, Batch: 2360/3444, Loss: 0.022416729480028152\n",
      "Epoch: 0/8, Batch: 2370/3444, Loss: 0.03098723106086254\n",
      "Epoch: 0/8, Batch: 2380/3444, Loss: 0.03489645570516586\n",
      "Epoch: 0/8, Batch: 2390/3444, Loss: 0.03139621019363403\n",
      "Epoch: 0/8, Batch: 2400/3444, Loss: 0.043563906103372574\n",
      "Epoch: 0/8, Batch: 2410/3444, Loss: 0.048232220113277435\n",
      "Epoch: 0/8, Batch: 2420/3444, Loss: 0.010729130357503891\n",
      "Epoch: 0/8, Batch: 2430/3444, Loss: 0.028486602008342743\n",
      "Epoch: 0/8, Batch: 2440/3444, Loss: 0.0192186888307333\n",
      "Epoch: 0/8, Batch: 2450/3444, Loss: 0.04450977221131325\n",
      "Epoch: 0/8, Batch: 2460/3444, Loss: 0.03055592253804207\n",
      "Epoch: 0/8, Batch: 2470/3444, Loss: 0.055105164647102356\n",
      "Epoch: 0/8, Batch: 2480/3444, Loss: 0.03077695146203041\n",
      "Epoch: 0/8, Batch: 2490/3444, Loss: 0.014151593670248985\n",
      "Epoch: 0/8, Batch: 2500/3444, Loss: 0.037515342235565186\n",
      "Epoch: 0/8, Batch: 2510/3444, Loss: 0.02053866535425186\n",
      "Epoch: 0/8, Batch: 2520/3444, Loss: 0.024014955386519432\n",
      "Epoch: 0/8, Batch: 2530/3444, Loss: 0.12127292901277542\n",
      "Epoch: 0/8, Batch: 2540/3444, Loss: 0.09679605066776276\n",
      "Epoch: 0/8, Batch: 2550/3444, Loss: 0.10583703219890594\n",
      "Epoch: 0/8, Batch: 2560/3444, Loss: 0.028251707553863525\n",
      "Epoch: 0/8, Batch: 2570/3444, Loss: 0.040078822523355484\n",
      "Epoch: 0/8, Batch: 2580/3444, Loss: 0.044181421399116516\n",
      "Epoch: 0/8, Batch: 2590/3444, Loss: 0.014079791493713856\n",
      "Epoch: 0/8, Batch: 2600/3444, Loss: 0.033540185540914536\n",
      "Epoch: 0/8, Batch: 2610/3444, Loss: 0.061440594494342804\n",
      "Epoch: 0/8, Batch: 2620/3444, Loss: 0.021886633709073067\n",
      "Epoch: 0/8, Batch: 2630/3444, Loss: 0.08683209121227264\n",
      "Epoch: 0/8, Batch: 2640/3444, Loss: 0.03740284591913223\n",
      "Epoch: 0/8, Batch: 2650/3444, Loss: 0.02306029573082924\n",
      "Epoch: 0/8, Batch: 2660/3444, Loss: 0.01707387901842594\n",
      "Epoch: 0/8, Batch: 2670/3444, Loss: 0.030977191403508186\n",
      "Epoch: 0/8, Batch: 2680/3444, Loss: 0.013768819160759449\n",
      "Epoch: 0/8, Batch: 2690/3444, Loss: 0.12172961235046387\n",
      "Epoch: 0/8, Batch: 2700/3444, Loss: 0.018138792365789413\n",
      "Epoch: 0/8, Batch: 2710/3444, Loss: 0.015222927555441856\n",
      "Epoch: 0/8, Batch: 2720/3444, Loss: 0.017309995368123055\n",
      "Epoch: 0/8, Batch: 2730/3444, Loss: 0.009212827309966087\n",
      "Epoch: 0/8, Batch: 2740/3444, Loss: 0.039364662021398544\n",
      "Epoch: 0/8, Batch: 2750/3444, Loss: 0.024023760110139847\n",
      "Epoch: 0/8, Batch: 2760/3444, Loss: 0.035604845732450485\n",
      "Epoch: 0/8, Batch: 2770/3444, Loss: 0.03556891903281212\n",
      "Epoch: 0/8, Batch: 2780/3444, Loss: 0.021152978762984276\n",
      "Epoch: 0/8, Batch: 2790/3444, Loss: 0.0250252652913332\n",
      "Epoch: 0/8, Batch: 2800/3444, Loss: 0.031719520688056946\n",
      "Epoch: 0/8, Batch: 2810/3444, Loss: 0.03981238231062889\n",
      "Epoch: 0/8, Batch: 2820/3444, Loss: 0.016348280012607574\n",
      "Epoch: 0/8, Batch: 2830/3444, Loss: 0.045160114765167236\n",
      "Epoch: 0/8, Batch: 2840/3444, Loss: 0.013456878252327442\n",
      "Epoch: 0/8, Batch: 2850/3444, Loss: 0.007589906919747591\n",
      "Epoch: 0/8, Batch: 2860/3444, Loss: 0.028699448332190514\n",
      "Epoch: 0/8, Batch: 2870/3444, Loss: 0.04014742374420166\n",
      "Epoch: 0/8, Batch: 2880/3444, Loss: 0.11909154057502747\n",
      "Epoch: 0/8, Batch: 2890/3444, Loss: 0.04679775983095169\n",
      "Epoch: 0/8, Batch: 2900/3444, Loss: 0.02353886514902115\n",
      "Epoch: 0/8, Batch: 2910/3444, Loss: 0.038739290088415146\n",
      "Epoch: 0/8, Batch: 2920/3444, Loss: 0.015119730494916439\n",
      "Epoch: 0/8, Batch: 2930/3444, Loss: 0.042030129581689835\n",
      "Epoch: 0/8, Batch: 2940/3444, Loss: 0.013729824684560299\n",
      "Epoch: 0/8, Batch: 2950/3444, Loss: 0.011246193200349808\n",
      "Epoch: 0/8, Batch: 2960/3444, Loss: 0.016926037147641182\n",
      "Epoch: 0/8, Batch: 2970/3444, Loss: 0.03893795982003212\n",
      "Epoch: 0/8, Batch: 2980/3444, Loss: 0.02454446069896221\n",
      "Epoch: 0/8, Batch: 2990/3444, Loss: 0.02882816083729267\n",
      "Epoch: 0/8, Batch: 3000/3444, Loss: 0.017172183841466904\n",
      "Epoch: 0/8, Batch: 3010/3444, Loss: 0.041241709142923355\n",
      "Epoch: 0/8, Batch: 3020/3444, Loss: 0.01945391297340393\n",
      "Epoch: 0/8, Batch: 3030/3444, Loss: 0.051101263612508774\n",
      "Epoch: 0/8, Batch: 3040/3444, Loss: 0.040068577975034714\n",
      "Epoch: 0/8, Batch: 3050/3444, Loss: 0.039682988077402115\n",
      "Epoch: 0/8, Batch: 3060/3444, Loss: 0.014506733044981956\n",
      "Epoch: 0/8, Batch: 3070/3444, Loss: 0.02225111983716488\n",
      "Epoch: 0/8, Batch: 3080/3444, Loss: 0.024028975516557693\n",
      "Epoch: 0/8, Batch: 3090/3444, Loss: 0.029698466882109642\n",
      "Epoch: 0/8, Batch: 3100/3444, Loss: 0.019848590716719627\n",
      "Epoch: 0/8, Batch: 3110/3444, Loss: 0.04788469150662422\n",
      "Epoch: 0/8, Batch: 3120/3444, Loss: 0.08469848334789276\n",
      "Epoch: 0/8, Batch: 3130/3444, Loss: 0.05430838093161583\n",
      "Epoch: 0/8, Batch: 3140/3444, Loss: 0.035864830017089844\n",
      "Epoch: 0/8, Batch: 3150/3444, Loss: 0.03387393057346344\n",
      "Epoch: 0/8, Batch: 3160/3444, Loss: 0.06494712084531784\n",
      "Epoch: 0/8, Batch: 3170/3444, Loss: 0.027039356529712677\n",
      "Epoch: 0/8, Batch: 3180/3444, Loss: 0.041535526514053345\n",
      "Epoch: 0/8, Batch: 3190/3444, Loss: 0.05322076007723808\n",
      "Epoch: 0/8, Batch: 3200/3444, Loss: 0.0713101476430893\n",
      "Epoch: 0/8, Batch: 3210/3444, Loss: 0.05694197863340378\n",
      "Epoch: 0/8, Batch: 3220/3444, Loss: 0.05203663930296898\n",
      "Epoch: 0/8, Batch: 3230/3444, Loss: 0.02102532796561718\n",
      "Epoch: 0/8, Batch: 3240/3444, Loss: 0.023362072184681892\n",
      "Epoch: 0/8, Batch: 3250/3444, Loss: 0.024555806070566177\n",
      "Epoch: 0/8, Batch: 3260/3444, Loss: 0.08649258315563202\n",
      "Epoch: 0/8, Batch: 3270/3444, Loss: 0.02220599539577961\n",
      "Epoch: 0/8, Batch: 3280/3444, Loss: 0.014486528933048248\n",
      "Epoch: 0/8, Batch: 3290/3444, Loss: 0.013538869097828865\n",
      "Epoch: 0/8, Batch: 3300/3444, Loss: 0.030411923304200172\n",
      "Epoch: 0/8, Batch: 3310/3444, Loss: 0.04077939689159393\n",
      "Epoch: 0/8, Batch: 3320/3444, Loss: 0.08588901907205582\n",
      "Epoch: 0/8, Batch: 3330/3444, Loss: 0.04141192510724068\n",
      "Epoch: 0/8, Batch: 3340/3444, Loss: 0.011961841024458408\n",
      "Epoch: 0/8, Batch: 3350/3444, Loss: 0.0347198024392128\n",
      "Epoch: 0/8, Batch: 3360/3444, Loss: 0.008667469024658203\n",
      "Epoch: 0/8, Batch: 3370/3444, Loss: 0.024541687220335007\n",
      "Epoch: 0/8, Batch: 3380/3444, Loss: 0.026516279205679893\n",
      "Epoch: 0/8, Batch: 3390/3444, Loss: 0.031876906752586365\n",
      "Epoch: 0/8, Batch: 3400/3444, Loss: 0.04191305488348007\n",
      "Epoch: 0/8, Batch: 3410/3444, Loss: 0.0241077970713377\n",
      "Epoch: 0/8, Batch: 3420/3444, Loss: 0.06766395270824432\n",
      "Epoch: 0/8, Batch: 3430/3444, Loss: 0.08872194588184357\n",
      "Epoch: 0/8, Batch: 3440/3444, Loss: 0.07456786930561066\n",
      "Epoch: 0/8, Val Loss: 0.07240448715587111\n",
      "Epoch: 1/8, Batch: 10/3444, Loss: 0.031970731914043427\n",
      "Epoch: 1/8, Batch: 20/3444, Loss: 0.015422175638377666\n",
      "Epoch: 1/8, Batch: 30/3444, Loss: 0.008427665568888187\n",
      "Epoch: 1/8, Batch: 40/3444, Loss: 0.014828365296125412\n",
      "Epoch: 1/8, Batch: 50/3444, Loss: 0.0321364589035511\n",
      "Epoch: 1/8, Batch: 60/3444, Loss: 0.02503710798919201\n",
      "Epoch: 1/8, Batch: 70/3444, Loss: 0.05255426838994026\n",
      "Epoch: 1/8, Batch: 80/3444, Loss: 0.06694279611110687\n",
      "Epoch: 1/8, Batch: 90/3444, Loss: 0.08375776559114456\n",
      "Epoch: 1/8, Batch: 100/3444, Loss: 0.029988164082169533\n",
      "Epoch: 1/8, Batch: 110/3444, Loss: 0.019671576097607613\n",
      "Epoch: 1/8, Batch: 120/3444, Loss: 0.05197596922516823\n",
      "Epoch: 1/8, Batch: 130/3444, Loss: 0.03431076928973198\n",
      "Epoch: 1/8, Batch: 140/3444, Loss: 0.02401225082576275\n",
      "Epoch: 1/8, Batch: 150/3444, Loss: 0.020009364932775497\n",
      "Epoch: 1/8, Batch: 160/3444, Loss: 0.041615068912506104\n",
      "Epoch: 1/8, Batch: 170/3444, Loss: 0.018871912732720375\n",
      "Epoch: 1/8, Batch: 180/3444, Loss: 0.033481381833553314\n",
      "Epoch: 1/8, Batch: 190/3444, Loss: 0.031608011573553085\n",
      "Epoch: 1/8, Batch: 200/3444, Loss: 0.035577304661273956\n",
      "Epoch: 1/8, Batch: 210/3444, Loss: 0.021914683282375336\n",
      "Epoch: 1/8, Batch: 220/3444, Loss: 0.02523041144013405\n",
      "Epoch: 1/8, Batch: 230/3444, Loss: 0.01949305273592472\n",
      "Epoch: 1/8, Batch: 240/3444, Loss: 0.07125890254974365\n",
      "Epoch: 1/8, Batch: 250/3444, Loss: 0.04696710407733917\n",
      "Epoch: 1/8, Batch: 260/3444, Loss: 0.017595199868083\n",
      "Epoch: 1/8, Batch: 270/3444, Loss: 0.013981306925415993\n",
      "Epoch: 1/8, Batch: 280/3444, Loss: 0.01763494499027729\n",
      "Epoch: 1/8, Batch: 290/3444, Loss: 0.02315552718937397\n",
      "Epoch: 1/8, Batch: 300/3444, Loss: 0.03531881794333458\n",
      "Epoch: 1/8, Batch: 310/3444, Loss: 0.04331307113170624\n",
      "Epoch: 1/8, Batch: 320/3444, Loss: 0.015339392237365246\n",
      "Epoch: 1/8, Batch: 330/3444, Loss: 0.02355930022895336\n",
      "Epoch: 1/8, Batch: 340/3444, Loss: 0.03220946341753006\n",
      "Epoch: 1/8, Batch: 350/3444, Loss: 0.041030917316675186\n",
      "Epoch: 1/8, Batch: 360/3444, Loss: 0.04558725655078888\n",
      "Epoch: 1/8, Batch: 370/3444, Loss: 0.05194111913442612\n",
      "Epoch: 1/8, Batch: 380/3444, Loss: 0.09332502633333206\n",
      "Epoch: 1/8, Batch: 390/3444, Loss: 0.018565280362963676\n",
      "Epoch: 1/8, Batch: 400/3444, Loss: 0.010138199664652348\n",
      "Epoch: 1/8, Batch: 410/3444, Loss: 0.054958999156951904\n",
      "Epoch: 1/8, Batch: 420/3444, Loss: 0.05844750627875328\n",
      "Epoch: 1/8, Batch: 430/3444, Loss: 0.015000996179878712\n",
      "Epoch: 1/8, Batch: 440/3444, Loss: 0.015587798319756985\n",
      "Epoch: 1/8, Batch: 450/3444, Loss: 0.043807338923215866\n",
      "Epoch: 1/8, Batch: 460/3444, Loss: 0.015247911214828491\n",
      "Epoch: 1/8, Batch: 470/3444, Loss: 0.03835540637373924\n",
      "Epoch: 1/8, Batch: 480/3444, Loss: 0.05877877026796341\n",
      "Epoch: 1/8, Batch: 490/3444, Loss: 0.035132549703121185\n",
      "Epoch: 1/8, Batch: 500/3444, Loss: 0.030898310244083405\n",
      "Epoch: 1/8, Batch: 510/3444, Loss: 0.02937742881476879\n",
      "Epoch: 1/8, Batch: 520/3444, Loss: 0.04654061421751976\n",
      "Epoch: 1/8, Batch: 530/3444, Loss: 0.02943935990333557\n",
      "Epoch: 1/8, Batch: 540/3444, Loss: 0.02779838629066944\n",
      "Epoch: 1/8, Batch: 550/3444, Loss: 0.025077378377318382\n",
      "Epoch: 1/8, Batch: 560/3444, Loss: 0.04780159518122673\n",
      "Epoch: 1/8, Batch: 570/3444, Loss: 0.04679591953754425\n",
      "Epoch: 1/8, Batch: 580/3444, Loss: 0.03805817291140556\n",
      "Epoch: 1/8, Batch: 590/3444, Loss: 0.050019461661577225\n",
      "Epoch: 1/8, Batch: 600/3444, Loss: 0.022064123302698135\n",
      "Epoch: 1/8, Batch: 610/3444, Loss: 0.017590198665857315\n",
      "Epoch: 1/8, Batch: 620/3444, Loss: 0.057645734399557114\n",
      "Epoch: 1/8, Batch: 630/3444, Loss: 0.03530455380678177\n",
      "Epoch: 1/8, Batch: 640/3444, Loss: 0.022957693785429\n",
      "Epoch: 1/8, Batch: 650/3444, Loss: 0.018885327503085136\n",
      "Epoch: 1/8, Batch: 660/3444, Loss: 0.017545215785503387\n",
      "Epoch: 1/8, Batch: 670/3444, Loss: 0.058375198394060135\n",
      "Epoch: 1/8, Batch: 680/3444, Loss: 0.0320957750082016\n",
      "Epoch: 1/8, Batch: 690/3444, Loss: 0.017106682062149048\n",
      "Epoch: 1/8, Batch: 700/3444, Loss: 0.07194976508617401\n",
      "Epoch: 1/8, Batch: 710/3444, Loss: 0.03702531009912491\n",
      "Epoch: 1/8, Batch: 720/3444, Loss: 0.04433704912662506\n",
      "Epoch: 1/8, Batch: 730/3444, Loss: 0.04625123366713524\n",
      "Epoch: 1/8, Batch: 740/3444, Loss: 0.03555236756801605\n",
      "Epoch: 1/8, Batch: 750/3444, Loss: 0.05910969898104668\n",
      "Epoch: 1/8, Batch: 760/3444, Loss: 0.028793033212423325\n",
      "Epoch: 1/8, Batch: 770/3444, Loss: 0.03926495462656021\n",
      "Epoch: 1/8, Batch: 780/3444, Loss: 0.018667995929718018\n",
      "Epoch: 1/8, Batch: 790/3444, Loss: 0.0183145422488451\n",
      "Epoch: 1/8, Batch: 800/3444, Loss: 0.012180140241980553\n",
      "Epoch: 1/8, Batch: 810/3444, Loss: 0.01691233552992344\n",
      "Epoch: 1/8, Batch: 820/3444, Loss: 0.04643641784787178\n",
      "Epoch: 1/8, Batch: 830/3444, Loss: 0.03716430440545082\n",
      "Epoch: 1/8, Batch: 840/3444, Loss: 0.013720273971557617\n",
      "Epoch: 1/8, Batch: 850/3444, Loss: 0.010691512376070023\n",
      "Epoch: 1/8, Batch: 860/3444, Loss: 0.03768375888466835\n",
      "Epoch: 1/8, Batch: 870/3444, Loss: 0.03511243686079979\n",
      "Epoch: 1/8, Batch: 880/3444, Loss: 0.025401078164577484\n",
      "Epoch: 1/8, Batch: 890/3444, Loss: 0.05451728776097298\n",
      "Epoch: 1/8, Batch: 900/3444, Loss: 0.03336913511157036\n",
      "Epoch: 1/8, Batch: 910/3444, Loss: 0.019471317529678345\n",
      "Epoch: 1/8, Batch: 920/3444, Loss: 0.0400693453848362\n",
      "Epoch: 1/8, Batch: 930/3444, Loss: 0.05044056102633476\n",
      "Epoch: 1/8, Batch: 940/3444, Loss: 0.018133580684661865\n",
      "Epoch: 1/8, Batch: 950/3444, Loss: 0.012081638909876347\n",
      "Epoch: 1/8, Batch: 960/3444, Loss: 0.04463185742497444\n",
      "Epoch: 1/8, Batch: 970/3444, Loss: 0.0225764662027359\n",
      "Epoch: 1/8, Batch: 980/3444, Loss: 0.029307737946510315\n",
      "Epoch: 1/8, Batch: 990/3444, Loss: 0.024288499727845192\n",
      "Epoch: 1/8, Batch: 1000/3444, Loss: 0.060778241604566574\n",
      "Epoch: 1/8, Batch: 1010/3444, Loss: 0.016894223168492317\n",
      "Epoch: 1/8, Batch: 1020/3444, Loss: 0.016336770728230476\n",
      "Epoch: 1/8, Batch: 1030/3444, Loss: 0.04551110789179802\n",
      "Epoch: 1/8, Batch: 1040/3444, Loss: 0.021580753847956657\n",
      "Epoch: 1/8, Batch: 1050/3444, Loss: 0.04801488667726517\n",
      "Epoch: 1/8, Batch: 1060/3444, Loss: 0.009991367347538471\n",
      "Epoch: 1/8, Batch: 1070/3444, Loss: 0.023341646417975426\n",
      "Epoch: 1/8, Batch: 1080/3444, Loss: 0.018297147005796432\n",
      "Epoch: 1/8, Batch: 1090/3444, Loss: 0.025977421551942825\n",
      "Epoch: 1/8, Batch: 1100/3444, Loss: 0.03461664915084839\n",
      "Epoch: 1/8, Batch: 1110/3444, Loss: 0.02626730501651764\n",
      "Epoch: 1/8, Batch: 1120/3444, Loss: 0.016599226742982864\n",
      "Epoch: 1/8, Batch: 1130/3444, Loss: 0.038118764758110046\n",
      "Epoch: 1/8, Batch: 1140/3444, Loss: 0.01974194124341011\n",
      "Epoch: 1/8, Batch: 1150/3444, Loss: 0.03045021928846836\n",
      "Epoch: 1/8, Batch: 1160/3444, Loss: 0.03699189051985741\n",
      "Epoch: 1/8, Batch: 1170/3444, Loss: 0.030850769951939583\n",
      "Epoch: 1/8, Batch: 1180/3444, Loss: 0.04652344807982445\n",
      "Epoch: 1/8, Batch: 1190/3444, Loss: 0.047071970999240875\n",
      "Epoch: 1/8, Batch: 1200/3444, Loss: 0.024936284869909286\n",
      "Epoch: 1/8, Batch: 1210/3444, Loss: 0.04823692515492439\n",
      "Epoch: 1/8, Batch: 1220/3444, Loss: 0.08874175697565079\n",
      "Epoch: 1/8, Batch: 1230/3444, Loss: 0.0444139763712883\n",
      "Epoch: 1/8, Batch: 1240/3444, Loss: 0.03904484584927559\n",
      "Epoch: 1/8, Batch: 1250/3444, Loss: 0.06390559673309326\n",
      "Epoch: 1/8, Batch: 1260/3444, Loss: 0.017912905663251877\n",
      "Epoch: 1/8, Batch: 1270/3444, Loss: 0.04159273952245712\n",
      "Epoch: 1/8, Batch: 1280/3444, Loss: 0.02273266576230526\n",
      "Epoch: 1/8, Batch: 1290/3444, Loss: 0.02107064612209797\n",
      "Epoch: 1/8, Batch: 1300/3444, Loss: 0.09195748716592789\n",
      "Epoch: 1/8, Batch: 1310/3444, Loss: 0.04920915514230728\n",
      "Epoch: 1/8, Batch: 1320/3444, Loss: 0.019869068637490273\n",
      "Epoch: 1/8, Batch: 1330/3444, Loss: 0.016746774315834045\n",
      "Epoch: 1/8, Batch: 1340/3444, Loss: 0.035950008779764175\n",
      "Epoch: 1/8, Batch: 1350/3444, Loss: 0.02441432885825634\n",
      "Epoch: 1/8, Batch: 1360/3444, Loss: 0.04972996190190315\n",
      "Epoch: 1/8, Batch: 1370/3444, Loss: 0.015177755616605282\n",
      "Epoch: 1/8, Batch: 1380/3444, Loss: 0.01707669347524643\n",
      "Epoch: 1/8, Batch: 1390/3444, Loss: 0.03629671037197113\n",
      "Epoch: 1/8, Batch: 1400/3444, Loss: 0.04309246316552162\n",
      "Epoch: 1/8, Batch: 1410/3444, Loss: 0.018196726217865944\n",
      "Epoch: 1/8, Batch: 1420/3444, Loss: 0.022468168288469315\n",
      "Epoch: 1/8, Batch: 1430/3444, Loss: 0.06337068974971771\n",
      "Epoch: 1/8, Batch: 1440/3444, Loss: 0.039374012500047684\n",
      "Epoch: 1/8, Batch: 1450/3444, Loss: 0.024167995899915695\n",
      "Epoch: 1/8, Batch: 1460/3444, Loss: 0.010128499008715153\n",
      "Epoch: 1/8, Batch: 1470/3444, Loss: 0.014648966491222382\n",
      "Epoch: 1/8, Batch: 1480/3444, Loss: 0.028800582513213158\n",
      "Epoch: 1/8, Batch: 1490/3444, Loss: 0.030046628788113594\n",
      "Epoch: 1/8, Batch: 1500/3444, Loss: 0.023364949971437454\n",
      "Epoch: 1/8, Batch: 1510/3444, Loss: 0.03316638246178627\n",
      "Epoch: 1/8, Batch: 1520/3444, Loss: 0.04236816614866257\n",
      "Epoch: 1/8, Batch: 1530/3444, Loss: 0.02698521316051483\n",
      "Epoch: 1/8, Batch: 1540/3444, Loss: 0.028295790776610374\n",
      "Epoch: 1/8, Batch: 1550/3444, Loss: 0.06680600345134735\n",
      "Epoch: 1/8, Batch: 1560/3444, Loss: 0.04806797206401825\n",
      "Epoch: 1/8, Batch: 1570/3444, Loss: 0.01574758253991604\n",
      "Epoch: 1/8, Batch: 1580/3444, Loss: 0.022776920348405838\n",
      "Epoch: 1/8, Batch: 1590/3444, Loss: 0.0168544203042984\n",
      "Epoch: 1/8, Batch: 1600/3444, Loss: 0.06017846241593361\n",
      "Epoch: 1/8, Batch: 1610/3444, Loss: 0.058623865246772766\n",
      "Epoch: 1/8, Batch: 1620/3444, Loss: 0.021854277700185776\n",
      "Epoch: 1/8, Batch: 1630/3444, Loss: 0.030930140987038612\n",
      "Epoch: 1/8, Batch: 1640/3444, Loss: 0.02356078289449215\n",
      "Epoch: 1/8, Batch: 1650/3444, Loss: 0.024985941126942635\n",
      "Epoch: 1/8, Batch: 1660/3444, Loss: 0.05045667663216591\n",
      "Epoch: 1/8, Batch: 1670/3444, Loss: 0.036644820123910904\n",
      "Epoch: 1/8, Batch: 1680/3444, Loss: 0.06722604483366013\n",
      "Epoch: 1/8, Batch: 1690/3444, Loss: 0.02659250795841217\n",
      "Epoch: 1/8, Batch: 1700/3444, Loss: 0.05041450634598732\n",
      "Epoch: 1/8, Batch: 1710/3444, Loss: 0.06335464864969254\n",
      "Epoch: 1/8, Batch: 1720/3444, Loss: 0.04205935820937157\n",
      "Epoch: 1/8, Batch: 1730/3444, Loss: 0.037752218544483185\n",
      "Epoch: 1/8, Batch: 1740/3444, Loss: 0.021529754623770714\n",
      "Epoch: 1/8, Batch: 1750/3444, Loss: 0.05520639941096306\n",
      "Epoch: 1/8, Batch: 1760/3444, Loss: 0.11180181056261063\n",
      "Epoch: 1/8, Batch: 1770/3444, Loss: 0.03825126960873604\n",
      "Epoch: 1/8, Batch: 1780/3444, Loss: 0.029753373935818672\n",
      "Epoch: 1/8, Batch: 1790/3444, Loss: 0.019357316195964813\n",
      "Epoch: 1/8, Batch: 1800/3444, Loss: 0.04447304457426071\n",
      "Epoch: 1/8, Batch: 1810/3444, Loss: 0.027341000735759735\n",
      "Epoch: 1/8, Batch: 1820/3444, Loss: 0.0317457839846611\n",
      "Epoch: 1/8, Batch: 1830/3444, Loss: 0.011823348701000214\n",
      "Epoch: 1/8, Batch: 1840/3444, Loss: 0.10157084465026855\n",
      "Epoch: 1/8, Batch: 1850/3444, Loss: 0.02313576266169548\n",
      "Epoch: 1/8, Batch: 1860/3444, Loss: 0.10736425966024399\n",
      "Epoch: 1/8, Batch: 1870/3444, Loss: 0.020001590251922607\n",
      "Epoch: 1/8, Batch: 1880/3444, Loss: 0.023981062695384026\n",
      "Epoch: 1/8, Batch: 1890/3444, Loss: 0.04177935793995857\n",
      "Epoch: 1/8, Batch: 1900/3444, Loss: 0.07213997095823288\n",
      "Epoch: 1/8, Batch: 1910/3444, Loss: 0.02089623734354973\n",
      "Epoch: 1/8, Batch: 1920/3444, Loss: 0.023247411474585533\n",
      "Epoch: 1/8, Batch: 1930/3444, Loss: 0.016867026686668396\n",
      "Epoch: 1/8, Batch: 1940/3444, Loss: 0.03660597279667854\n",
      "Epoch: 1/8, Batch: 1950/3444, Loss: 0.0201442688703537\n",
      "Epoch: 1/8, Batch: 1960/3444, Loss: 0.04834724962711334\n",
      "Epoch: 1/8, Batch: 1970/3444, Loss: 0.0347493439912796\n",
      "Epoch: 1/8, Batch: 1980/3444, Loss: 0.016234463080763817\n",
      "Epoch: 1/8, Batch: 1990/3444, Loss: 0.02897828444838524\n",
      "Epoch: 1/8, Batch: 2000/3444, Loss: 0.03556438162922859\n",
      "Epoch: 1/8, Batch: 2010/3444, Loss: 0.02925620973110199\n",
      "Epoch: 1/8, Batch: 2020/3444, Loss: 0.031115805730223656\n",
      "Epoch: 1/8, Batch: 2030/3444, Loss: 0.05108444765210152\n",
      "Epoch: 1/8, Batch: 2040/3444, Loss: 0.020252160727977753\n",
      "Epoch: 1/8, Batch: 2050/3444, Loss: 0.06701092422008514\n",
      "Epoch: 1/8, Batch: 2060/3444, Loss: 0.04019637405872345\n",
      "Epoch: 1/8, Batch: 2070/3444, Loss: 0.051656946539878845\n",
      "Epoch: 1/8, Batch: 2080/3444, Loss: 0.01697237603366375\n",
      "Epoch: 1/8, Batch: 2090/3444, Loss: 0.0699082538485527\n",
      "Epoch: 1/8, Batch: 2100/3444, Loss: 0.06666546314954758\n",
      "Epoch: 1/8, Batch: 2110/3444, Loss: 0.041899360716342926\n",
      "Epoch: 1/8, Batch: 2120/3444, Loss: 0.04517798498272896\n",
      "Epoch: 1/8, Batch: 2130/3444, Loss: 0.02698901854455471\n",
      "Epoch: 1/8, Batch: 2140/3444, Loss: 0.028308702632784843\n",
      "Epoch: 1/8, Batch: 2150/3444, Loss: 0.023259568959474564\n",
      "Epoch: 1/8, Batch: 2160/3444, Loss: 0.019970478489995003\n",
      "Epoch: 1/8, Batch: 2170/3444, Loss: 0.018551994115114212\n",
      "Epoch: 1/8, Batch: 2180/3444, Loss: 0.019402865320444107\n",
      "Epoch: 1/8, Batch: 2190/3444, Loss: 0.041490208357572556\n",
      "Epoch: 1/8, Batch: 2200/3444, Loss: 0.08968360722064972\n",
      "Epoch: 1/8, Batch: 2210/3444, Loss: 0.04451175406575203\n",
      "Epoch: 1/8, Batch: 2220/3444, Loss: 0.025701580569148064\n",
      "Epoch: 1/8, Batch: 2230/3444, Loss: 0.021782565861940384\n",
      "Epoch: 1/8, Batch: 2240/3444, Loss: 0.052107639610767365\n",
      "Epoch: 1/8, Batch: 2250/3444, Loss: 0.01988288015127182\n",
      "Epoch: 1/8, Batch: 2260/3444, Loss: 0.0705646201968193\n",
      "Epoch: 1/8, Batch: 2270/3444, Loss: 0.05256016552448273\n",
      "Epoch: 1/8, Batch: 2280/3444, Loss: 0.022312892600893974\n",
      "Epoch: 1/8, Batch: 2290/3444, Loss: 0.008755073882639408\n",
      "Epoch: 1/8, Batch: 2300/3444, Loss: 0.022442318499088287\n",
      "Epoch: 1/8, Batch: 2310/3444, Loss: 0.027616729959845543\n",
      "Epoch: 1/8, Batch: 2320/3444, Loss: 0.039290010929107666\n",
      "Epoch: 1/8, Batch: 2330/3444, Loss: 0.02077016606926918\n",
      "Epoch: 1/8, Batch: 2340/3444, Loss: 0.07799173891544342\n",
      "Epoch: 1/8, Batch: 2350/3444, Loss: 0.018368983641266823\n",
      "Epoch: 1/8, Batch: 2360/3444, Loss: 0.011104718782007694\n",
      "Epoch: 1/8, Batch: 2370/3444, Loss: 0.04395817592740059\n",
      "Epoch: 1/8, Batch: 2380/3444, Loss: 0.03435363620519638\n",
      "Epoch: 1/8, Batch: 2390/3444, Loss: 0.02169194258749485\n",
      "Epoch: 1/8, Batch: 2400/3444, Loss: 0.029239686205983162\n",
      "Epoch: 1/8, Batch: 2410/3444, Loss: 0.026353877037763596\n",
      "Epoch: 1/8, Batch: 2420/3444, Loss: 0.03354999050498009\n",
      "Epoch: 1/8, Batch: 2430/3444, Loss: 0.06045691668987274\n",
      "Epoch: 1/8, Batch: 2440/3444, Loss: 0.05025298520922661\n",
      "Epoch: 1/8, Batch: 2450/3444, Loss: 0.026896405965089798\n",
      "Epoch: 1/8, Batch: 2460/3444, Loss: 0.029259493574500084\n",
      "Epoch: 1/8, Batch: 2470/3444, Loss: 0.017350610345602036\n",
      "Epoch: 1/8, Batch: 2480/3444, Loss: 0.036829590797424316\n",
      "Epoch: 1/8, Batch: 2490/3444, Loss: 0.011746488511562347\n",
      "Epoch: 1/8, Batch: 2500/3444, Loss: 0.055648524314165115\n",
      "Epoch: 1/8, Batch: 2510/3444, Loss: 0.015643946826457977\n",
      "Epoch: 1/8, Batch: 2520/3444, Loss: 0.00620067585259676\n",
      "Epoch: 1/8, Batch: 2530/3444, Loss: 0.02678959257900715\n",
      "Epoch: 1/8, Batch: 2540/3444, Loss: 0.04857376217842102\n",
      "Epoch: 1/8, Batch: 2550/3444, Loss: 0.057783856987953186\n",
      "Epoch: 1/8, Batch: 2560/3444, Loss: 0.02074482850730419\n",
      "Epoch: 1/8, Batch: 2570/3444, Loss: 0.0434345081448555\n",
      "Epoch: 1/8, Batch: 2580/3444, Loss: 0.018032256513834\n",
      "Epoch: 1/8, Batch: 2590/3444, Loss: 0.014510721899569035\n",
      "Epoch: 1/8, Batch: 2600/3444, Loss: 0.02952147088944912\n",
      "Epoch: 1/8, Batch: 2610/3444, Loss: 0.036138568073511124\n",
      "Epoch: 1/8, Batch: 2620/3444, Loss: 0.0276335459202528\n",
      "Epoch: 1/8, Batch: 2630/3444, Loss: 0.00817836169153452\n",
      "Epoch: 1/8, Batch: 2640/3444, Loss: 0.05688873305916786\n",
      "Epoch: 1/8, Batch: 2650/3444, Loss: 0.08196289092302322\n",
      "Epoch: 1/8, Batch: 2660/3444, Loss: 0.09157142043113708\n",
      "Epoch: 1/8, Batch: 2670/3444, Loss: 0.03890285640954971\n",
      "Epoch: 1/8, Batch: 2680/3444, Loss: 0.03373695909976959\n",
      "Epoch: 1/8, Batch: 2690/3444, Loss: 0.0349171906709671\n",
      "Epoch: 1/8, Batch: 2700/3444, Loss: 0.02899027243256569\n",
      "Epoch: 1/8, Batch: 2710/3444, Loss: 0.01965642347931862\n",
      "Epoch: 1/8, Batch: 2720/3444, Loss: 0.10667143762111664\n",
      "Epoch: 1/8, Batch: 2730/3444, Loss: 0.032532431185245514\n",
      "Epoch: 1/8, Batch: 2740/3444, Loss: 0.018366677686572075\n",
      "Epoch: 1/8, Batch: 2750/3444, Loss: 0.15029510855674744\n",
      "Epoch: 1/8, Batch: 2760/3444, Loss: 0.022219309583306313\n",
      "Epoch: 1/8, Batch: 2770/3444, Loss: 0.049045782536268234\n",
      "Epoch: 1/8, Batch: 2780/3444, Loss: 0.025568867102265358\n",
      "Epoch: 1/8, Batch: 2790/3444, Loss: 0.04235374554991722\n",
      "Epoch: 1/8, Batch: 2800/3444, Loss: 0.012986885383725166\n",
      "Epoch: 1/8, Batch: 2810/3444, Loss: 0.027279004454612732\n",
      "Epoch: 1/8, Batch: 2820/3444, Loss: 0.02595902420580387\n",
      "Epoch: 1/8, Batch: 2830/3444, Loss: 0.025136111304163933\n",
      "Epoch: 1/8, Batch: 2840/3444, Loss: 0.04277275875210762\n",
      "Epoch: 1/8, Batch: 2850/3444, Loss: 0.02475871331989765\n",
      "Epoch: 1/8, Batch: 2860/3444, Loss: 0.033833470195531845\n",
      "Epoch: 1/8, Batch: 2870/3444, Loss: 0.0323190838098526\n",
      "Epoch: 1/8, Batch: 2880/3444, Loss: 0.027534574270248413\n",
      "Epoch: 1/8, Batch: 2890/3444, Loss: 0.030385879799723625\n",
      "Epoch: 1/8, Batch: 2900/3444, Loss: 0.02802450768649578\n",
      "Epoch: 1/8, Batch: 2910/3444, Loss: 0.02988210693001747\n",
      "Epoch: 1/8, Batch: 2920/3444, Loss: 0.05785653367638588\n",
      "Epoch: 1/8, Batch: 2930/3444, Loss: 0.06679272651672363\n",
      "Epoch: 1/8, Batch: 2940/3444, Loss: 0.02724081091582775\n",
      "Epoch: 1/8, Batch: 2950/3444, Loss: 0.057522036135196686\n",
      "Epoch: 1/8, Batch: 2960/3444, Loss: 0.03752035275101662\n",
      "Epoch: 1/8, Batch: 2970/3444, Loss: 0.0399993397295475\n",
      "Epoch: 1/8, Batch: 2980/3444, Loss: 0.04389243945479393\n",
      "Epoch: 1/8, Batch: 2990/3444, Loss: 0.028055518865585327\n",
      "Epoch: 1/8, Batch: 3000/3444, Loss: 0.010652524419128895\n",
      "Epoch: 1/8, Batch: 3010/3444, Loss: 0.046059053391218185\n",
      "Epoch: 1/8, Batch: 3020/3444, Loss: 0.0695185586810112\n",
      "Epoch: 1/8, Batch: 3030/3444, Loss: 0.02768874540925026\n",
      "Epoch: 1/8, Batch: 3040/3444, Loss: 0.0279012992978096\n",
      "Epoch: 1/8, Batch: 3050/3444, Loss: 0.02784617803990841\n",
      "Epoch: 1/8, Batch: 3060/3444, Loss: 0.05762633681297302\n",
      "Epoch: 1/8, Batch: 3070/3444, Loss: 0.04373132437467575\n",
      "Epoch: 1/8, Batch: 3080/3444, Loss: 0.06272085011005402\n",
      "Epoch: 1/8, Batch: 3090/3444, Loss: 0.018616730347275734\n",
      "Epoch: 1/8, Batch: 3100/3444, Loss: 0.06013442948460579\n",
      "Epoch: 1/8, Batch: 3110/3444, Loss: 0.034432683140039444\n",
      "Epoch: 1/8, Batch: 3120/3444, Loss: 0.0338282436132431\n",
      "Epoch: 1/8, Batch: 3130/3444, Loss: 0.03490154817700386\n",
      "Epoch: 1/8, Batch: 3140/3444, Loss: 0.02462056465446949\n",
      "Epoch: 1/8, Batch: 3150/3444, Loss: 0.016117960214614868\n",
      "Epoch: 1/8, Batch: 3160/3444, Loss: 0.008472229354083538\n",
      "Epoch: 1/8, Batch: 3170/3444, Loss: 0.025508098304271698\n",
      "Epoch: 1/8, Batch: 3180/3444, Loss: 0.042098671197891235\n",
      "Epoch: 1/8, Batch: 3190/3444, Loss: 0.07480654120445251\n",
      "Epoch: 1/8, Batch: 3200/3444, Loss: 0.024050965905189514\n",
      "Epoch: 1/8, Batch: 3210/3444, Loss: 0.04014353081583977\n",
      "Epoch: 1/8, Batch: 3220/3444, Loss: 0.017546119168400764\n",
      "Epoch: 1/8, Batch: 3230/3444, Loss: 0.024040142074227333\n",
      "Epoch: 1/8, Batch: 3240/3444, Loss: 0.025670871138572693\n",
      "Epoch: 1/8, Batch: 3250/3444, Loss: 0.011242745444178581\n",
      "Epoch: 1/8, Batch: 3260/3444, Loss: 0.018309198319911957\n",
      "Epoch: 1/8, Batch: 3270/3444, Loss: 0.017522817477583885\n",
      "Epoch: 1/8, Batch: 3280/3444, Loss: 0.04692627117037773\n",
      "Epoch: 1/8, Batch: 3290/3444, Loss: 0.019687017425894737\n",
      "Epoch: 1/8, Batch: 3300/3444, Loss: 0.014368347823619843\n",
      "Epoch: 1/8, Batch: 3310/3444, Loss: 0.03336343169212341\n",
      "Epoch: 1/8, Batch: 3320/3444, Loss: 0.009848427027463913\n",
      "Epoch: 1/8, Batch: 3330/3444, Loss: 0.013529844582080841\n",
      "Epoch: 1/8, Batch: 3340/3444, Loss: 0.04153741896152496\n",
      "Epoch: 1/8, Batch: 3350/3444, Loss: 0.0670468807220459\n",
      "Epoch: 1/8, Batch: 3360/3444, Loss: 0.027925023809075356\n",
      "Epoch: 1/8, Batch: 3370/3444, Loss: 0.038109391927719116\n",
      "Epoch: 1/8, Batch: 3380/3444, Loss: 0.031425852328538895\n",
      "Epoch: 1/8, Batch: 3390/3444, Loss: 0.06005040556192398\n",
      "Epoch: 1/8, Batch: 3400/3444, Loss: 0.06203966215252876\n",
      "Epoch: 1/8, Batch: 3410/3444, Loss: 0.03915292024612427\n",
      "Epoch: 1/8, Batch: 3420/3444, Loss: 0.029727214947342873\n",
      "Epoch: 1/8, Batch: 3430/3444, Loss: 0.032599885016679764\n",
      "Epoch: 1/8, Batch: 3440/3444, Loss: 0.05550194904208183\n",
      "Epoch: 1/8, Val Loss: 0.03881443792610115\n",
      "Epoch: 2/8, Batch: 10/3444, Loss: 0.024357326328754425\n",
      "Epoch: 2/8, Batch: 20/3444, Loss: 0.02262059599161148\n",
      "Epoch: 2/8, Batch: 30/3444, Loss: 0.021128399297595024\n",
      "Epoch: 2/8, Batch: 40/3444, Loss: 0.03728045895695686\n",
      "Epoch: 2/8, Batch: 50/3444, Loss: 0.013567238114774227\n",
      "Epoch: 2/8, Batch: 60/3444, Loss: 0.016587859019637108\n",
      "Epoch: 2/8, Batch: 70/3444, Loss: 0.07174741476774216\n",
      "Epoch: 2/8, Batch: 80/3444, Loss: 0.00957037415355444\n",
      "Epoch: 2/8, Batch: 90/3444, Loss: 0.01390917133539915\n",
      "Epoch: 2/8, Batch: 100/3444, Loss: 0.04379879683256149\n",
      "Epoch: 2/8, Batch: 110/3444, Loss: 0.038267552852630615\n",
      "Epoch: 2/8, Batch: 120/3444, Loss: 0.021953649818897247\n",
      "Epoch: 2/8, Batch: 130/3444, Loss: 0.03248962387442589\n",
      "Epoch: 2/8, Batch: 140/3444, Loss: 0.022476060315966606\n",
      "Epoch: 2/8, Batch: 150/3444, Loss: 0.025083918124437332\n",
      "Epoch: 2/8, Batch: 160/3444, Loss: 0.07630524039268494\n",
      "Epoch: 2/8, Batch: 170/3444, Loss: 0.021453041583299637\n",
      "Epoch: 2/8, Batch: 180/3444, Loss: 0.02992836944758892\n",
      "Epoch: 2/8, Batch: 190/3444, Loss: 0.05685887113213539\n",
      "Epoch: 2/8, Batch: 200/3444, Loss: 0.01959674060344696\n",
      "Epoch: 2/8, Batch: 210/3444, Loss: 0.032338667660951614\n",
      "Epoch: 2/8, Batch: 220/3444, Loss: 0.035385455936193466\n",
      "Epoch: 2/8, Batch: 230/3444, Loss: 0.03203712776303291\n",
      "Epoch: 2/8, Batch: 240/3444, Loss: 0.018269237130880356\n",
      "Epoch: 2/8, Batch: 250/3444, Loss: 0.03477330878376961\n",
      "Epoch: 2/8, Batch: 260/3444, Loss: 0.032151103019714355\n",
      "Epoch: 2/8, Batch: 270/3444, Loss: 0.04213054105639458\n",
      "Epoch: 2/8, Batch: 280/3444, Loss: 0.02786712720990181\n",
      "Epoch: 2/8, Batch: 290/3444, Loss: 0.02799655683338642\n",
      "Epoch: 2/8, Batch: 300/3444, Loss: 0.02054278552532196\n",
      "Epoch: 2/8, Batch: 310/3444, Loss: 0.011439874768257141\n",
      "Epoch: 2/8, Batch: 320/3444, Loss: 0.0260546263307333\n",
      "Epoch: 2/8, Batch: 330/3444, Loss: 0.02582087554037571\n",
      "Epoch: 2/8, Batch: 340/3444, Loss: 0.012257829308509827\n",
      "Epoch: 2/8, Batch: 350/3444, Loss: 0.03404025360941887\n",
      "Epoch: 2/8, Batch: 360/3444, Loss: 0.07549058645963669\n",
      "Epoch: 2/8, Batch: 370/3444, Loss: 0.04177027940750122\n",
      "Epoch: 2/8, Batch: 380/3444, Loss: 0.01564173772931099\n",
      "Epoch: 2/8, Batch: 390/3444, Loss: 0.022321008145809174\n",
      "Epoch: 2/8, Batch: 400/3444, Loss: 0.040231455117464066\n",
      "Epoch: 2/8, Batch: 410/3444, Loss: 0.04810282588005066\n",
      "Epoch: 2/8, Batch: 420/3444, Loss: 0.02318493276834488\n",
      "Epoch: 2/8, Batch: 430/3444, Loss: 0.029306014999747276\n",
      "Epoch: 2/8, Batch: 440/3444, Loss: 0.04720224067568779\n",
      "Epoch: 2/8, Batch: 450/3444, Loss: 0.02640432119369507\n",
      "Epoch: 2/8, Batch: 460/3444, Loss: 0.011774190701544285\n",
      "Epoch: 2/8, Batch: 470/3444, Loss: 0.022434638813138008\n",
      "Epoch: 2/8, Batch: 480/3444, Loss: 0.025168390944600105\n",
      "Epoch: 2/8, Batch: 490/3444, Loss: 0.027673965319991112\n",
      "Epoch: 2/8, Batch: 500/3444, Loss: 0.020191794261336327\n",
      "Epoch: 2/8, Batch: 510/3444, Loss: 0.02712707780301571\n",
      "Epoch: 2/8, Batch: 520/3444, Loss: 0.019269775599241257\n",
      "Epoch: 2/8, Batch: 530/3444, Loss: 0.018045969307422638\n",
      "Epoch: 2/8, Batch: 540/3444, Loss: 0.02401556633412838\n",
      "Epoch: 2/8, Batch: 550/3444, Loss: 0.01069705467671156\n",
      "Epoch: 2/8, Batch: 560/3444, Loss: 0.026645753532648087\n",
      "Epoch: 2/8, Batch: 570/3444, Loss: 0.04957073554396629\n",
      "Epoch: 2/8, Batch: 580/3444, Loss: 0.05313519388437271\n",
      "Epoch: 2/8, Batch: 590/3444, Loss: 0.06314955651760101\n",
      "Epoch: 2/8, Batch: 600/3444, Loss: 0.07805155217647552\n",
      "Epoch: 2/8, Batch: 610/3444, Loss: 0.010796698741614819\n",
      "Epoch: 2/8, Batch: 620/3444, Loss: 0.028250856325030327\n",
      "Epoch: 2/8, Batch: 630/3444, Loss: 0.01832665503025055\n",
      "Epoch: 2/8, Batch: 640/3444, Loss: 0.03087540902197361\n",
      "Epoch: 2/8, Batch: 650/3444, Loss: 0.1113787367939949\n",
      "Epoch: 2/8, Batch: 660/3444, Loss: 0.01643865928053856\n",
      "Epoch: 2/8, Batch: 670/3444, Loss: 0.0627698227763176\n",
      "Epoch: 2/8, Batch: 680/3444, Loss: 0.03887065500020981\n",
      "Epoch: 2/8, Batch: 690/3444, Loss: 0.020024042576551437\n",
      "Epoch: 2/8, Batch: 700/3444, Loss: 0.03330610319972038\n",
      "Epoch: 2/8, Batch: 710/3444, Loss: 0.01436612755060196\n",
      "Epoch: 2/8, Batch: 720/3444, Loss: 0.04612674191594124\n",
      "Epoch: 2/8, Batch: 730/3444, Loss: 0.04631417617201805\n",
      "Epoch: 2/8, Batch: 740/3444, Loss: 0.02636694349348545\n",
      "Epoch: 2/8, Batch: 750/3444, Loss: 0.0508621484041214\n",
      "Epoch: 2/8, Batch: 760/3444, Loss: 0.049072105437517166\n",
      "Epoch: 2/8, Batch: 770/3444, Loss: 0.09707149863243103\n",
      "Epoch: 2/8, Batch: 780/3444, Loss: 0.024253012612462044\n",
      "Epoch: 2/8, Batch: 790/3444, Loss: 0.032896365970373154\n",
      "Epoch: 2/8, Batch: 800/3444, Loss: 0.042748674750328064\n",
      "Epoch: 2/8, Batch: 810/3444, Loss: 0.023303210735321045\n",
      "Epoch: 2/8, Batch: 820/3444, Loss: 0.05595310777425766\n",
      "Epoch: 2/8, Batch: 830/3444, Loss: 0.01856357604265213\n",
      "Epoch: 2/8, Batch: 840/3444, Loss: 0.016389505937695503\n",
      "Epoch: 2/8, Batch: 850/3444, Loss: 0.01954556815326214\n",
      "Epoch: 2/8, Batch: 860/3444, Loss: 0.02349136583507061\n",
      "Epoch: 2/8, Batch: 870/3444, Loss: 0.04344696179032326\n",
      "Epoch: 2/8, Batch: 880/3444, Loss: 0.09506016969680786\n",
      "Epoch: 2/8, Batch: 890/3444, Loss: 0.06049119308590889\n",
      "Epoch: 2/8, Batch: 900/3444, Loss: 0.02928078919649124\n",
      "Epoch: 2/8, Batch: 910/3444, Loss: 0.047171514481306076\n",
      "Epoch: 2/8, Batch: 920/3444, Loss: 0.05183548107743263\n",
      "Epoch: 2/8, Batch: 930/3444, Loss: 0.027268826961517334\n",
      "Epoch: 2/8, Batch: 940/3444, Loss: 0.0334244966506958\n",
      "Epoch: 2/8, Batch: 950/3444, Loss: 0.02065211534500122\n",
      "Epoch: 2/8, Batch: 960/3444, Loss: 0.022898662835359573\n",
      "Epoch: 2/8, Batch: 970/3444, Loss: 0.04193352535367012\n",
      "Epoch: 2/8, Batch: 980/3444, Loss: 0.0496751070022583\n",
      "Epoch: 2/8, Batch: 990/3444, Loss: 0.0367254875600338\n",
      "Epoch: 2/8, Batch: 1000/3444, Loss: 0.012147245928645134\n",
      "Epoch: 2/8, Batch: 1010/3444, Loss: 0.023290710523724556\n",
      "Epoch: 2/8, Batch: 1020/3444, Loss: 0.024253949522972107\n",
      "Epoch: 2/8, Batch: 1030/3444, Loss: 0.07569094002246857\n",
      "Epoch: 2/8, Batch: 1040/3444, Loss: 0.019402148202061653\n",
      "Epoch: 2/8, Batch: 1050/3444, Loss: 0.02113930508494377\n",
      "Epoch: 2/8, Batch: 1060/3444, Loss: 0.03141624107956886\n",
      "Epoch: 2/8, Batch: 1070/3444, Loss: 0.021241063252091408\n",
      "Epoch: 2/8, Batch: 1080/3444, Loss: 0.030593344941735268\n",
      "Epoch: 2/8, Batch: 1090/3444, Loss: 0.08184781670570374\n",
      "Epoch: 2/8, Batch: 1100/3444, Loss: 0.04396834969520569\n",
      "Epoch: 2/8, Batch: 1110/3444, Loss: 0.021886320784687996\n",
      "Epoch: 2/8, Batch: 1120/3444, Loss: 0.0399613194167614\n",
      "Epoch: 2/8, Batch: 1130/3444, Loss: 0.007512712385505438\n",
      "Epoch: 2/8, Batch: 1140/3444, Loss: 0.09911297261714935\n",
      "Epoch: 2/8, Batch: 1150/3444, Loss: 0.04574091359972954\n",
      "Epoch: 2/8, Batch: 1160/3444, Loss: 0.03084930032491684\n",
      "Epoch: 2/8, Batch: 1170/3444, Loss: 0.019329708069562912\n",
      "Epoch: 2/8, Batch: 1180/3444, Loss: 0.05773521587252617\n",
      "Epoch: 2/8, Batch: 1190/3444, Loss: 0.0402740016579628\n",
      "Epoch: 2/8, Batch: 1200/3444, Loss: 0.03289743885397911\n",
      "Epoch: 2/8, Batch: 1210/3444, Loss: 0.030825195834040642\n",
      "Epoch: 2/8, Batch: 1220/3444, Loss: 0.018514638766646385\n",
      "Epoch: 2/8, Batch: 1230/3444, Loss: 0.01839718222618103\n",
      "Epoch: 2/8, Batch: 1240/3444, Loss: 0.033837590366601944\n",
      "Epoch: 2/8, Batch: 1250/3444, Loss: 0.03601938858628273\n",
      "Epoch: 2/8, Batch: 1260/3444, Loss: 0.04809197038412094\n",
      "Epoch: 2/8, Batch: 1270/3444, Loss: 0.04201715812087059\n",
      "Epoch: 2/8, Batch: 1280/3444, Loss: 0.015727048739790916\n",
      "Epoch: 2/8, Batch: 1290/3444, Loss: 0.04612867161631584\n",
      "Epoch: 2/8, Batch: 1300/3444, Loss: 0.047238729894161224\n",
      "Epoch: 2/8, Batch: 1310/3444, Loss: 0.028187621384859085\n",
      "Epoch: 2/8, Batch: 1320/3444, Loss: 0.051192667335271835\n",
      "Epoch: 2/8, Batch: 1330/3444, Loss: 0.01661713421344757\n",
      "Epoch: 2/8, Batch: 1340/3444, Loss: 0.025017645210027695\n",
      "Epoch: 2/8, Batch: 1350/3444, Loss: 0.01183799747377634\n",
      "Epoch: 2/8, Batch: 1360/3444, Loss: 0.03535068780183792\n",
      "Epoch: 2/8, Batch: 1370/3444, Loss: 0.020884772762656212\n",
      "Epoch: 2/8, Batch: 1380/3444, Loss: 0.047968894243240356\n",
      "Epoch: 2/8, Batch: 1390/3444, Loss: 0.08611550182104111\n",
      "Epoch: 2/8, Batch: 1400/3444, Loss: 0.044978395104408264\n",
      "Epoch: 2/8, Batch: 1410/3444, Loss: 0.027438342571258545\n",
      "Epoch: 2/8, Batch: 1420/3444, Loss: 0.03905702754855156\n",
      "Epoch: 2/8, Batch: 1430/3444, Loss: 0.04380374774336815\n",
      "Epoch: 2/8, Batch: 1440/3444, Loss: 0.029210757464170456\n",
      "Epoch: 2/8, Batch: 1450/3444, Loss: 0.028586558997631073\n",
      "Epoch: 2/8, Batch: 1460/3444, Loss: 0.03313892334699631\n",
      "Epoch: 2/8, Batch: 1470/3444, Loss: 0.03151894733309746\n",
      "Epoch: 2/8, Batch: 1480/3444, Loss: 0.0277886763215065\n",
      "Epoch: 2/8, Batch: 1490/3444, Loss: 0.02034713886678219\n",
      "Epoch: 2/8, Batch: 1500/3444, Loss: 0.020176393911242485\n",
      "Epoch: 2/8, Batch: 1510/3444, Loss: 0.053885575383901596\n",
      "Epoch: 2/8, Batch: 1520/3444, Loss: 0.007851666770875454\n",
      "Epoch: 2/8, Batch: 1530/3444, Loss: 0.015228619799017906\n",
      "Epoch: 2/8, Batch: 1540/3444, Loss: 0.03263559192419052\n",
      "Epoch: 2/8, Batch: 1550/3444, Loss: 0.05413258075714111\n",
      "Epoch: 2/8, Batch: 1560/3444, Loss: 0.02992544136941433\n",
      "Epoch: 2/8, Batch: 1570/3444, Loss: 0.046111494302749634\n",
      "Epoch: 2/8, Batch: 1580/3444, Loss: 0.03529578447341919\n",
      "Epoch: 2/8, Batch: 1590/3444, Loss: 0.02143137902021408\n",
      "Epoch: 2/8, Batch: 1600/3444, Loss: 0.023802252486348152\n",
      "Epoch: 2/8, Batch: 1610/3444, Loss: 0.06289732456207275\n",
      "Epoch: 2/8, Batch: 1620/3444, Loss: 0.019558293744921684\n",
      "Epoch: 2/8, Batch: 1630/3444, Loss: 0.022081276401877403\n",
      "Epoch: 2/8, Batch: 1640/3444, Loss: 0.016181612387299538\n",
      "Epoch: 2/8, Batch: 1650/3444, Loss: 0.04154206067323685\n",
      "Epoch: 2/8, Batch: 1660/3444, Loss: 0.03490210697054863\n",
      "Epoch: 2/8, Batch: 1670/3444, Loss: 0.07147951424121857\n",
      "Epoch: 2/8, Batch: 1680/3444, Loss: 0.010439012199640274\n",
      "Epoch: 2/8, Batch: 1690/3444, Loss: 0.023222992196679115\n",
      "Epoch: 2/8, Batch: 1700/3444, Loss: 0.052586980164051056\n",
      "Epoch: 2/8, Batch: 1710/3444, Loss: 0.025324176996946335\n",
      "Epoch: 2/8, Batch: 1720/3444, Loss: 0.031044919043779373\n",
      "Epoch: 2/8, Batch: 1730/3444, Loss: 0.03605492413043976\n",
      "Epoch: 2/8, Batch: 1740/3444, Loss: 0.03799949213862419\n",
      "Epoch: 2/8, Batch: 1750/3444, Loss: 0.012315396219491959\n",
      "Epoch: 2/8, Batch: 1760/3444, Loss: 0.012503269128501415\n",
      "Epoch: 2/8, Batch: 1770/3444, Loss: 0.09497667849063873\n",
      "Epoch: 2/8, Batch: 1780/3444, Loss: 0.008281691931188107\n",
      "Epoch: 2/8, Batch: 1790/3444, Loss: 0.03976216912269592\n",
      "Epoch: 2/8, Batch: 1800/3444, Loss: 0.018216177821159363\n",
      "Epoch: 2/8, Batch: 1810/3444, Loss: 0.024208495393395424\n",
      "Epoch: 2/8, Batch: 1820/3444, Loss: 0.015277277678251266\n",
      "Epoch: 2/8, Batch: 1830/3444, Loss: 0.01771114394068718\n",
      "Epoch: 2/8, Batch: 1840/3444, Loss: 0.024596011266112328\n",
      "Epoch: 2/8, Batch: 1850/3444, Loss: 0.04169318452477455\n",
      "Epoch: 2/8, Batch: 1860/3444, Loss: 0.027442891150712967\n",
      "Epoch: 2/8, Batch: 1870/3444, Loss: 0.014356711879372597\n",
      "Epoch: 2/8, Batch: 1880/3444, Loss: 0.0639633908867836\n",
      "Epoch: 2/8, Batch: 1890/3444, Loss: 0.04374339058995247\n",
      "Epoch: 2/8, Batch: 1900/3444, Loss: 0.06463293731212616\n",
      "Epoch: 2/8, Batch: 1910/3444, Loss: 0.020251326262950897\n",
      "Epoch: 2/8, Batch: 1920/3444, Loss: 0.012193631380796432\n",
      "Epoch: 2/8, Batch: 1930/3444, Loss: 0.024201324209570885\n",
      "Epoch: 2/8, Batch: 1940/3444, Loss: 0.048079051077365875\n",
      "Epoch: 2/8, Batch: 1950/3444, Loss: 0.013592998497188091\n",
      "Epoch: 2/8, Batch: 1960/3444, Loss: 0.048189084976911545\n",
      "Epoch: 2/8, Batch: 1970/3444, Loss: 0.013188953511416912\n",
      "Epoch: 2/8, Batch: 1980/3444, Loss: 0.025006016716361046\n",
      "Epoch: 2/8, Batch: 1990/3444, Loss: 0.018014026805758476\n",
      "Epoch: 2/8, Batch: 2000/3444, Loss: 0.021268682554364204\n",
      "Epoch: 2/8, Batch: 2010/3444, Loss: 0.020571865141391754\n",
      "Epoch: 2/8, Batch: 2020/3444, Loss: 0.020657816901803017\n",
      "Epoch: 2/8, Batch: 2030/3444, Loss: 0.04092139005661011\n",
      "Epoch: 2/8, Batch: 2040/3444, Loss: 0.04533473029732704\n",
      "Epoch: 2/8, Batch: 2050/3444, Loss: 0.039156723767519\n",
      "Epoch: 2/8, Batch: 2060/3444, Loss: 0.021557385101914406\n",
      "Epoch: 2/8, Batch: 2070/3444, Loss: 0.033385034650564194\n",
      "Epoch: 2/8, Batch: 2080/3444, Loss: 0.04968423768877983\n",
      "Epoch: 2/8, Batch: 2090/3444, Loss: 0.050518617033958435\n",
      "Epoch: 2/8, Batch: 2100/3444, Loss: 0.022886475548148155\n",
      "Epoch: 2/8, Batch: 2110/3444, Loss: 0.022235918790102005\n",
      "Epoch: 2/8, Batch: 2120/3444, Loss: 0.02627769112586975\n",
      "Epoch: 2/8, Batch: 2130/3444, Loss: 0.036730051040649414\n",
      "Epoch: 2/8, Batch: 2140/3444, Loss: 0.02617858536541462\n",
      "Epoch: 2/8, Batch: 2150/3444, Loss: 0.019405832514166832\n",
      "Epoch: 2/8, Batch: 2160/3444, Loss: 0.018729159608483315\n",
      "Epoch: 2/8, Batch: 2170/3444, Loss: 0.01476652454584837\n",
      "Epoch: 2/8, Batch: 2180/3444, Loss: 0.014438805170357227\n",
      "Epoch: 2/8, Batch: 2190/3444, Loss: 0.03137907013297081\n",
      "Epoch: 2/8, Batch: 2200/3444, Loss: 0.054365191608667374\n",
      "Epoch: 2/8, Batch: 2210/3444, Loss: 0.03259170800447464\n",
      "Epoch: 2/8, Batch: 2220/3444, Loss: 0.012901228852570057\n",
      "Epoch: 2/8, Batch: 2230/3444, Loss: 0.03584672510623932\n",
      "Epoch: 2/8, Batch: 2240/3444, Loss: 0.014410343021154404\n",
      "Epoch: 2/8, Batch: 2250/3444, Loss: 0.07789534330368042\n",
      "Epoch: 2/8, Batch: 2260/3444, Loss: 0.023407086730003357\n",
      "Epoch: 2/8, Batch: 2270/3444, Loss: 0.03668437898159027\n",
      "Epoch: 2/8, Batch: 2280/3444, Loss: 0.050660666078329086\n",
      "Epoch: 2/8, Batch: 2290/3444, Loss: 0.038287486881017685\n",
      "Epoch: 2/8, Batch: 2300/3444, Loss: 0.043501824140548706\n",
      "Epoch: 2/8, Batch: 2310/3444, Loss: 0.05924048274755478\n",
      "Epoch: 2/8, Batch: 2320/3444, Loss: 0.06862390786409378\n",
      "Epoch: 2/8, Batch: 2330/3444, Loss: 0.039329349994659424\n",
      "Epoch: 2/8, Batch: 2340/3444, Loss: 0.03593466058373451\n",
      "Epoch: 2/8, Batch: 2350/3444, Loss: 0.020396769046783447\n",
      "Epoch: 2/8, Batch: 2360/3444, Loss: 0.03219473361968994\n",
      "Epoch: 2/8, Batch: 2370/3444, Loss: 0.025869550183415413\n",
      "Epoch: 2/8, Batch: 2380/3444, Loss: 0.04138678312301636\n",
      "Epoch: 2/8, Batch: 2390/3444, Loss: 0.023492321372032166\n",
      "Epoch: 2/8, Batch: 2400/3444, Loss: 0.02696862630546093\n",
      "Epoch: 2/8, Batch: 2410/3444, Loss: 0.02196849137544632\n",
      "Epoch: 2/8, Batch: 2420/3444, Loss: 0.03388847038149834\n",
      "Epoch: 2/8, Batch: 2430/3444, Loss: 0.028076747432351112\n",
      "Epoch: 2/8, Batch: 2440/3444, Loss: 0.03018650971353054\n",
      "Epoch: 2/8, Batch: 2450/3444, Loss: 0.008230081759393215\n",
      "Epoch: 2/8, Batch: 2460/3444, Loss: 0.04098764434456825\n",
      "Epoch: 2/8, Batch: 2470/3444, Loss: 0.04816760867834091\n",
      "Epoch: 2/8, Batch: 2480/3444, Loss: 0.039709389209747314\n",
      "Epoch: 2/8, Batch: 2490/3444, Loss: 0.05040346086025238\n",
      "Epoch: 2/8, Batch: 2500/3444, Loss: 0.016352344304323196\n",
      "Epoch: 2/8, Batch: 2510/3444, Loss: 0.04242032393813133\n",
      "Epoch: 2/8, Batch: 2520/3444, Loss: 0.017318226397037506\n",
      "Epoch: 2/8, Batch: 2530/3444, Loss: 0.03674670681357384\n",
      "Epoch: 2/8, Batch: 2540/3444, Loss: 0.05850358307361603\n",
      "Epoch: 2/8, Batch: 2550/3444, Loss: 0.01595013029873371\n",
      "Epoch: 2/8, Batch: 2560/3444, Loss: 0.040751419961452484\n",
      "Epoch: 2/8, Batch: 2570/3444, Loss: 0.019939489662647247\n",
      "Epoch: 2/8, Batch: 2580/3444, Loss: 0.04897943139076233\n",
      "Epoch: 2/8, Batch: 2590/3444, Loss: 0.03384314849972725\n",
      "Epoch: 2/8, Batch: 2600/3444, Loss: 0.01807519979774952\n",
      "Epoch: 2/8, Batch: 2610/3444, Loss: 0.023517075926065445\n",
      "Epoch: 2/8, Batch: 2620/3444, Loss: 0.0424019880592823\n",
      "Epoch: 2/8, Batch: 2630/3444, Loss: 0.09173380583524704\n",
      "Epoch: 2/8, Batch: 2640/3444, Loss: 0.030438369140028954\n",
      "Epoch: 2/8, Batch: 2650/3444, Loss: 0.03388752043247223\n",
      "Epoch: 2/8, Batch: 2660/3444, Loss: 0.07946460694074631\n",
      "Epoch: 2/8, Batch: 2670/3444, Loss: 0.02915087155997753\n",
      "Epoch: 2/8, Batch: 2680/3444, Loss: 0.0188897792249918\n",
      "Epoch: 2/8, Batch: 2690/3444, Loss: 0.020193787291646004\n",
      "Epoch: 2/8, Batch: 2700/3444, Loss: 0.045205067843198776\n",
      "Epoch: 2/8, Batch: 2710/3444, Loss: 0.019561035558581352\n",
      "Epoch: 2/8, Batch: 2720/3444, Loss: 0.01979871094226837\n",
      "Epoch: 2/8, Batch: 2730/3444, Loss: 0.03478677570819855\n",
      "Epoch: 2/8, Batch: 2740/3444, Loss: 0.03995613008737564\n",
      "Epoch: 2/8, Batch: 2750/3444, Loss: 0.04788824915885925\n",
      "Epoch: 2/8, Batch: 2760/3444, Loss: 0.09241373091936111\n",
      "Epoch: 2/8, Batch: 2770/3444, Loss: 0.027903616428375244\n",
      "Epoch: 2/8, Batch: 2780/3444, Loss: 0.02753186784684658\n",
      "Epoch: 2/8, Batch: 2790/3444, Loss: 0.02040359564125538\n",
      "Epoch: 2/8, Batch: 2800/3444, Loss: 0.06357178092002869\n",
      "Epoch: 2/8, Batch: 2810/3444, Loss: 0.017291270196437836\n",
      "Epoch: 2/8, Batch: 2820/3444, Loss: 0.028135618194937706\n",
      "Epoch: 2/8, Batch: 2830/3444, Loss: 0.015609576366841793\n",
      "Epoch: 2/8, Batch: 2840/3444, Loss: 0.015575191006064415\n",
      "Epoch: 2/8, Batch: 2850/3444, Loss: 0.022169217467308044\n",
      "Epoch: 2/8, Batch: 2860/3444, Loss: 0.0393674410879612\n",
      "Epoch: 2/8, Batch: 2870/3444, Loss: 0.024622473865747452\n",
      "Epoch: 2/8, Batch: 2880/3444, Loss: 0.028870217502117157\n",
      "Epoch: 2/8, Batch: 2890/3444, Loss: 0.027074310928583145\n",
      "Epoch: 2/8, Batch: 2900/3444, Loss: 0.015032488852739334\n",
      "Epoch: 2/8, Batch: 2910/3444, Loss: 0.10030163824558258\n",
      "Epoch: 2/8, Batch: 2920/3444, Loss: 0.019449792802333832\n",
      "Epoch: 2/8, Batch: 2930/3444, Loss: 0.03187447413802147\n",
      "Epoch: 2/8, Batch: 2940/3444, Loss: 0.0904979258775711\n",
      "Epoch: 2/8, Batch: 2950/3444, Loss: 0.00726704578846693\n",
      "Epoch: 2/8, Batch: 2960/3444, Loss: 0.02633586712181568\n",
      "Epoch: 2/8, Batch: 2970/3444, Loss: 0.02121243067085743\n",
      "Epoch: 2/8, Batch: 2980/3444, Loss: 0.01515919342637062\n",
      "Epoch: 2/8, Batch: 2990/3444, Loss: 0.01927344873547554\n",
      "Epoch: 2/8, Batch: 3000/3444, Loss: 0.05010124668478966\n",
      "Epoch: 2/8, Batch: 3010/3444, Loss: 0.03162723407149315\n",
      "Epoch: 2/8, Batch: 3020/3444, Loss: 0.01941588893532753\n",
      "Epoch: 2/8, Batch: 3030/3444, Loss: 0.023606522008776665\n",
      "Epoch: 2/8, Batch: 3040/3444, Loss: 0.05579958111047745\n",
      "Epoch: 2/8, Batch: 3050/3444, Loss: 0.02614324912428856\n",
      "Epoch: 2/8, Batch: 3060/3444, Loss: 0.05498034134507179\n",
      "Epoch: 2/8, Batch: 3070/3444, Loss: 0.03171277418732643\n",
      "Epoch: 2/8, Batch: 3080/3444, Loss: 0.026353470981121063\n",
      "Epoch: 2/8, Batch: 3090/3444, Loss: 0.012176348827779293\n",
      "Epoch: 2/8, Batch: 3100/3444, Loss: 0.022997884079813957\n",
      "Epoch: 2/8, Batch: 3110/3444, Loss: 0.021516013890504837\n",
      "Epoch: 2/8, Batch: 3120/3444, Loss: 0.02138134464621544\n",
      "Epoch: 2/8, Batch: 3130/3444, Loss: 0.01881459169089794\n",
      "Epoch: 2/8, Batch: 3140/3444, Loss: 0.05052410811185837\n",
      "Epoch: 2/8, Batch: 3150/3444, Loss: 0.022321969270706177\n",
      "Epoch: 2/8, Batch: 3160/3444, Loss: 0.04183664917945862\n",
      "Epoch: 2/8, Batch: 3170/3444, Loss: 0.02089182659983635\n",
      "Epoch: 2/8, Batch: 3180/3444, Loss: 0.028635667636990547\n",
      "Epoch: 2/8, Batch: 3190/3444, Loss: 0.026883414015173912\n",
      "Epoch: 2/8, Batch: 3200/3444, Loss: 0.01128827128559351\n",
      "Epoch: 2/8, Batch: 3210/3444, Loss: 0.012179137207567692\n",
      "Epoch: 2/8, Batch: 3220/3444, Loss: 0.02892513945698738\n",
      "Epoch: 2/8, Batch: 3230/3444, Loss: 0.02018488384783268\n",
      "Epoch: 2/8, Batch: 3240/3444, Loss: 0.09535330533981323\n",
      "Epoch: 2/8, Batch: 3250/3444, Loss: 0.014324012212455273\n",
      "Epoch: 2/8, Batch: 3260/3444, Loss: 0.06502842903137207\n",
      "Epoch: 2/8, Batch: 3270/3444, Loss: 0.02697914093732834\n",
      "Epoch: 2/8, Batch: 3280/3444, Loss: 0.04078415036201477\n",
      "Epoch: 2/8, Batch: 3290/3444, Loss: 0.03732927888631821\n",
      "Epoch: 2/8, Batch: 3300/3444, Loss: 0.011597181670367718\n",
      "Epoch: 2/8, Batch: 3310/3444, Loss: 0.013633163645863533\n",
      "Epoch: 2/8, Batch: 3320/3444, Loss: 0.019594205543398857\n",
      "Epoch: 2/8, Batch: 3330/3444, Loss: 0.008866983465850353\n",
      "Epoch: 2/8, Batch: 3340/3444, Loss: 0.02694340981543064\n",
      "Epoch: 2/8, Batch: 3350/3444, Loss: 0.0318308025598526\n",
      "Epoch: 2/8, Batch: 3360/3444, Loss: 0.018606306985020638\n",
      "Epoch: 2/8, Batch: 3370/3444, Loss: 0.05032486096024513\n",
      "Epoch: 2/8, Batch: 3380/3444, Loss: 0.03931266441941261\n",
      "Epoch: 2/8, Batch: 3390/3444, Loss: 0.02624610997736454\n",
      "Epoch: 2/8, Batch: 3400/3444, Loss: 0.019777711480855942\n",
      "Epoch: 2/8, Batch: 3410/3444, Loss: 0.021870722994208336\n",
      "Epoch: 2/8, Batch: 3420/3444, Loss: 0.01675979048013687\n",
      "Epoch: 2/8, Batch: 3430/3444, Loss: 0.029526369646191597\n",
      "Epoch: 2/8, Batch: 3440/3444, Loss: 0.013643339276313782\n",
      "Epoch: 2/8, Val Loss: 0.0713109210518629\n",
      "Epoch: 3/8, Batch: 10/3444, Loss: 0.00638705724850297\n",
      "Epoch: 3/8, Batch: 20/3444, Loss: 0.013862717896699905\n",
      "Epoch: 3/8, Batch: 30/3444, Loss: 0.018535440787672997\n",
      "Epoch: 3/8, Batch: 40/3444, Loss: 0.039758697152137756\n",
      "Epoch: 3/8, Batch: 50/3444, Loss: 0.03454047441482544\n",
      "Epoch: 3/8, Batch: 60/3444, Loss: 0.02404645085334778\n",
      "Epoch: 3/8, Batch: 70/3444, Loss: 0.05139537528157234\n",
      "Epoch: 3/8, Batch: 80/3444, Loss: 0.02386685460805893\n",
      "Epoch: 3/8, Batch: 90/3444, Loss: 0.02171475999057293\n",
      "Epoch: 3/8, Batch: 100/3444, Loss: 0.03816971927881241\n",
      "Epoch: 3/8, Batch: 110/3444, Loss: 0.02517104707658291\n",
      "Epoch: 3/8, Batch: 120/3444, Loss: 0.0320575013756752\n",
      "Epoch: 3/8, Batch: 130/3444, Loss: 0.033337656408548355\n",
      "Epoch: 3/8, Batch: 140/3444, Loss: 0.025176899507641792\n",
      "Epoch: 3/8, Batch: 150/3444, Loss: 0.026448579505085945\n",
      "Epoch: 3/8, Batch: 160/3444, Loss: 0.036351993680000305\n",
      "Epoch: 3/8, Batch: 170/3444, Loss: 0.032399024814367294\n",
      "Epoch: 3/8, Batch: 180/3444, Loss: 0.01754363812506199\n",
      "Epoch: 3/8, Batch: 190/3444, Loss: 0.011629262007772923\n",
      "Epoch: 3/8, Batch: 200/3444, Loss: 0.03673592209815979\n",
      "Epoch: 3/8, Batch: 210/3444, Loss: 0.012277682311832905\n",
      "Epoch: 3/8, Batch: 220/3444, Loss: 0.04638747498393059\n",
      "Epoch: 3/8, Batch: 230/3444, Loss: 0.04787448048591614\n",
      "Epoch: 3/8, Batch: 240/3444, Loss: 0.05870651826262474\n",
      "Epoch: 3/8, Batch: 250/3444, Loss: 0.03859562799334526\n",
      "Epoch: 3/8, Batch: 260/3444, Loss: 0.05703152343630791\n",
      "Epoch: 3/8, Batch: 270/3444, Loss: 0.018839038908481598\n",
      "Epoch: 3/8, Batch: 280/3444, Loss: 0.016665244475007057\n",
      "Epoch: 3/8, Batch: 290/3444, Loss: 0.03504297137260437\n",
      "Epoch: 3/8, Batch: 300/3444, Loss: 0.01238709595054388\n",
      "Epoch: 3/8, Batch: 310/3444, Loss: 0.05030682310461998\n",
      "Epoch: 3/8, Batch: 320/3444, Loss: 0.027790263295173645\n",
      "Epoch: 3/8, Batch: 330/3444, Loss: 0.056751545518636703\n",
      "Epoch: 3/8, Batch: 340/3444, Loss: 0.01450214721262455\n",
      "Epoch: 3/8, Batch: 350/3444, Loss: 0.010969780385494232\n",
      "Epoch: 3/8, Batch: 360/3444, Loss: 0.054847635328769684\n",
      "Epoch: 3/8, Batch: 370/3444, Loss: 0.020467596128582954\n",
      "Epoch: 3/8, Batch: 380/3444, Loss: 0.03778662532567978\n",
      "Epoch: 3/8, Batch: 390/3444, Loss: 0.02615588903427124\n",
      "Epoch: 3/8, Batch: 400/3444, Loss: 0.028990013524889946\n",
      "Epoch: 3/8, Batch: 410/3444, Loss: 0.032437991350889206\n",
      "Epoch: 3/8, Batch: 420/3444, Loss: 0.01767691783607006\n",
      "Epoch: 3/8, Batch: 430/3444, Loss: 0.02107403054833412\n",
      "Epoch: 3/8, Batch: 440/3444, Loss: 0.04646321386098862\n",
      "Epoch: 3/8, Batch: 450/3444, Loss: 0.0094196991994977\n",
      "Epoch: 3/8, Batch: 460/3444, Loss: 0.05943021923303604\n",
      "Epoch: 3/8, Batch: 470/3444, Loss: 0.04008679836988449\n",
      "Epoch: 3/8, Batch: 480/3444, Loss: 0.06543390452861786\n",
      "Epoch: 3/8, Batch: 490/3444, Loss: 0.05151433125138283\n",
      "Epoch: 3/8, Batch: 500/3444, Loss: 0.03663206472992897\n",
      "Epoch: 3/8, Batch: 510/3444, Loss: 0.03154166042804718\n",
      "Epoch: 3/8, Batch: 520/3444, Loss: 0.08524932712316513\n",
      "Epoch: 3/8, Batch: 530/3444, Loss: 0.054089635610580444\n",
      "Epoch: 3/8, Batch: 540/3444, Loss: 0.021187201142311096\n",
      "Epoch: 3/8, Batch: 550/3444, Loss: 0.039035603404045105\n",
      "Epoch: 3/8, Batch: 560/3444, Loss: 0.042871881276369095\n",
      "Epoch: 3/8, Batch: 570/3444, Loss: 0.03301330655813217\n",
      "Epoch: 3/8, Batch: 580/3444, Loss: 0.07794886082410812\n",
      "Epoch: 3/8, Batch: 590/3444, Loss: 0.03340175747871399\n",
      "Epoch: 3/8, Batch: 600/3444, Loss: 0.023700954392552376\n",
      "Epoch: 3/8, Batch: 610/3444, Loss: 0.02007555030286312\n",
      "Epoch: 3/8, Batch: 620/3444, Loss: 0.027360141277313232\n",
      "Epoch: 3/8, Batch: 630/3444, Loss: 0.02488665282726288\n",
      "Epoch: 3/8, Batch: 640/3444, Loss: 0.02200193703174591\n",
      "Epoch: 3/8, Batch: 650/3444, Loss: 0.037326157093048096\n",
      "Epoch: 3/8, Batch: 660/3444, Loss: 0.040222495794296265\n",
      "Epoch: 3/8, Batch: 670/3444, Loss: 0.01860639825463295\n",
      "Epoch: 3/8, Batch: 680/3444, Loss: 0.06021014600992203\n",
      "Epoch: 3/8, Batch: 690/3444, Loss: 0.03321577236056328\n",
      "Epoch: 3/8, Batch: 700/3444, Loss: 0.04590029641985893\n",
      "Epoch: 3/8, Batch: 710/3444, Loss: 0.023853808641433716\n",
      "Epoch: 3/8, Batch: 720/3444, Loss: 0.05579224228858948\n",
      "Epoch: 3/8, Batch: 730/3444, Loss: 0.03751925751566887\n",
      "Epoch: 3/8, Batch: 740/3444, Loss: 0.037494610995054245\n",
      "Epoch: 3/8, Batch: 750/3444, Loss: 0.0754004567861557\n",
      "Epoch: 3/8, Batch: 760/3444, Loss: 0.02479403279721737\n",
      "Epoch: 3/8, Batch: 770/3444, Loss: 0.03421591594815254\n",
      "Epoch: 3/8, Batch: 780/3444, Loss: 0.044787198305130005\n",
      "Epoch: 3/8, Batch: 790/3444, Loss: 0.01608728989958763\n",
      "Epoch: 3/8, Batch: 800/3444, Loss: 0.021581022068858147\n",
      "Epoch: 3/8, Batch: 810/3444, Loss: 0.045751165598630905\n",
      "Epoch: 3/8, Batch: 820/3444, Loss: 0.008892099373042583\n",
      "Epoch: 3/8, Batch: 830/3444, Loss: 0.061066627502441406\n",
      "Epoch: 3/8, Batch: 840/3444, Loss: 0.0333973653614521\n",
      "Epoch: 3/8, Batch: 850/3444, Loss: 0.017774639651179314\n",
      "Epoch: 3/8, Batch: 860/3444, Loss: 0.036249563097953796\n",
      "Epoch: 3/8, Batch: 870/3444, Loss: 0.032602183520793915\n",
      "Epoch: 3/8, Batch: 880/3444, Loss: 0.017487237229943275\n",
      "Epoch: 3/8, Batch: 890/3444, Loss: 0.06975226104259491\n",
      "Epoch: 3/8, Batch: 900/3444, Loss: 0.1237635388970375\n",
      "Epoch: 3/8, Batch: 910/3444, Loss: 0.028963902965188026\n",
      "Epoch: 3/8, Batch: 920/3444, Loss: 0.03657393157482147\n",
      "Epoch: 3/8, Batch: 930/3444, Loss: 0.02750982902944088\n",
      "Epoch: 3/8, Batch: 940/3444, Loss: 0.025049705058336258\n",
      "Epoch: 3/8, Batch: 950/3444, Loss: 0.036980416625738144\n",
      "Epoch: 3/8, Batch: 960/3444, Loss: 0.02791186049580574\n",
      "Epoch: 3/8, Batch: 970/3444, Loss: 0.024064747616648674\n",
      "Epoch: 3/8, Batch: 980/3444, Loss: 0.029602505266666412\n",
      "Epoch: 3/8, Batch: 990/3444, Loss: 0.026956832036376\n",
      "Epoch: 3/8, Batch: 1000/3444, Loss: 0.03326954320073128\n",
      "Epoch: 3/8, Batch: 1010/3444, Loss: 0.06131431460380554\n",
      "Epoch: 3/8, Batch: 1020/3444, Loss: 0.04264995828270912\n",
      "Epoch: 3/8, Batch: 1030/3444, Loss: 0.05325695872306824\n",
      "Epoch: 3/8, Batch: 1040/3444, Loss: 0.025222675874829292\n",
      "Epoch: 3/8, Batch: 1050/3444, Loss: 0.05474526062607765\n",
      "Epoch: 3/8, Batch: 1060/3444, Loss: 0.16333013772964478\n",
      "Epoch: 3/8, Batch: 1070/3444, Loss: 0.05001115798950195\n",
      "Epoch: 3/8, Batch: 1080/3444, Loss: 0.047382060438394547\n",
      "Epoch: 3/8, Batch: 1090/3444, Loss: 0.09611926972866058\n",
      "Epoch: 3/8, Batch: 1100/3444, Loss: 0.036587703973054886\n",
      "Epoch: 3/8, Batch: 1110/3444, Loss: 0.04145633056759834\n",
      "Epoch: 3/8, Batch: 1120/3444, Loss: 0.015685230493545532\n",
      "Epoch: 3/8, Batch: 1130/3444, Loss: 0.044193070381879807\n",
      "Epoch: 3/8, Batch: 1140/3444, Loss: 0.03531016781926155\n",
      "Epoch: 3/8, Batch: 1150/3444, Loss: 0.009200256317853928\n",
      "Epoch: 3/8, Batch: 1160/3444, Loss: 0.030378278344869614\n",
      "Epoch: 3/8, Batch: 1170/3444, Loss: 0.026999203488230705\n",
      "Epoch: 3/8, Batch: 1180/3444, Loss: 0.018353551626205444\n",
      "Epoch: 3/8, Batch: 1190/3444, Loss: 0.03626410663127899\n",
      "Epoch: 3/8, Batch: 1200/3444, Loss: 0.08890501409769058\n",
      "Epoch: 3/8, Batch: 1210/3444, Loss: 0.02681836672127247\n",
      "Epoch: 3/8, Batch: 1220/3444, Loss: 0.017485154792666435\n",
      "Epoch: 3/8, Batch: 1230/3444, Loss: 0.045012302696704865\n",
      "Epoch: 3/8, Batch: 1240/3444, Loss: 0.014174661599099636\n",
      "Epoch: 3/8, Batch: 1250/3444, Loss: 0.01732165738940239\n",
      "Epoch: 3/8, Batch: 1260/3444, Loss: 0.054719746112823486\n",
      "Epoch: 3/8, Batch: 1270/3444, Loss: 0.015330735594034195\n",
      "Epoch: 3/8, Batch: 1280/3444, Loss: 0.04702640697360039\n",
      "Epoch: 3/8, Batch: 1290/3444, Loss: 0.023410100489854813\n",
      "Epoch: 3/8, Batch: 1300/3444, Loss: 0.02949012815952301\n",
      "Epoch: 3/8, Batch: 1310/3444, Loss: 0.0465957373380661\n",
      "Epoch: 3/8, Batch: 1320/3444, Loss: 0.023598747327923775\n",
      "Epoch: 3/8, Batch: 1330/3444, Loss: 0.029257571324706078\n",
      "Epoch: 3/8, Batch: 1340/3444, Loss: 0.01190169993788004\n",
      "Epoch: 3/8, Batch: 1350/3444, Loss: 0.017159074544906616\n",
      "Epoch: 3/8, Batch: 1360/3444, Loss: 0.017364250496029854\n",
      "Epoch: 3/8, Batch: 1370/3444, Loss: 0.051043007522821426\n",
      "Epoch: 3/8, Batch: 1380/3444, Loss: 0.019679827615618706\n",
      "Epoch: 3/8, Batch: 1390/3444, Loss: 0.07453709840774536\n",
      "Epoch: 3/8, Batch: 1400/3444, Loss: 0.06360853463411331\n",
      "Epoch: 3/8, Batch: 1410/3444, Loss: 0.04654470458626747\n",
      "Epoch: 3/8, Batch: 1420/3444, Loss: 0.03560187295079231\n",
      "Epoch: 3/8, Batch: 1430/3444, Loss: 0.06141475588083267\n",
      "Epoch: 3/8, Batch: 1440/3444, Loss: 0.013803812675178051\n",
      "Epoch: 3/8, Batch: 1450/3444, Loss: 0.0547037199139595\n",
      "Epoch: 3/8, Batch: 1460/3444, Loss: 0.009038043208420277\n",
      "Epoch: 3/8, Batch: 1470/3444, Loss: 0.029677320271730423\n",
      "Epoch: 3/8, Batch: 1480/3444, Loss: 0.020830897614359856\n",
      "Epoch: 3/8, Batch: 1490/3444, Loss: 0.026786766946315765\n",
      "Epoch: 3/8, Batch: 1500/3444, Loss: 0.06995909661054611\n",
      "Epoch: 3/8, Batch: 1510/3444, Loss: 0.010379262268543243\n",
      "Epoch: 3/8, Batch: 1520/3444, Loss: 0.0251956544816494\n",
      "Epoch: 3/8, Batch: 1530/3444, Loss: 0.04432246461510658\n",
      "Epoch: 3/8, Batch: 1540/3444, Loss: 0.062160708010196686\n",
      "Epoch: 3/8, Batch: 1550/3444, Loss: 0.06509718298912048\n",
      "Epoch: 3/8, Batch: 1560/3444, Loss: 0.028838351368904114\n",
      "Epoch: 3/8, Batch: 1570/3444, Loss: 0.013460874557495117\n",
      "Epoch: 3/8, Batch: 1580/3444, Loss: 0.016693115234375\n",
      "Epoch: 3/8, Batch: 1590/3444, Loss: 0.019621234387159348\n",
      "Epoch: 3/8, Batch: 1600/3444, Loss: 0.049022968858480453\n",
      "Epoch: 3/8, Batch: 1610/3444, Loss: 0.01668151468038559\n",
      "Epoch: 3/8, Batch: 1620/3444, Loss: 0.059231363236904144\n",
      "Epoch: 3/8, Batch: 1630/3444, Loss: 0.03527621552348137\n",
      "Epoch: 3/8, Batch: 1640/3444, Loss: 0.01562856324017048\n",
      "Epoch: 3/8, Batch: 1650/3444, Loss: 0.015809889882802963\n",
      "Epoch: 3/8, Batch: 1660/3444, Loss: 0.022003518417477608\n",
      "Epoch: 3/8, Batch: 1670/3444, Loss: 0.03749750554561615\n",
      "Epoch: 3/8, Batch: 1680/3444, Loss: 0.031402427703142166\n",
      "Epoch: 3/8, Batch: 1690/3444, Loss: 0.023602254688739777\n",
      "Epoch: 3/8, Batch: 1700/3444, Loss: 0.02785370871424675\n",
      "Epoch: 3/8, Batch: 1710/3444, Loss: 0.05797971785068512\n",
      "Epoch: 3/8, Batch: 1720/3444, Loss: 0.04001164436340332\n",
      "Epoch: 3/8, Batch: 1730/3444, Loss: 0.0312164556235075\n",
      "Epoch: 3/8, Batch: 1740/3444, Loss: 0.03420160338282585\n",
      "Epoch: 3/8, Batch: 1750/3444, Loss: 0.03377784416079521\n",
      "Epoch: 3/8, Batch: 1760/3444, Loss: 0.01485960278660059\n",
      "Epoch: 3/8, Batch: 1770/3444, Loss: 0.040580589324235916\n",
      "Epoch: 3/8, Batch: 1780/3444, Loss: 0.04452762007713318\n",
      "Epoch: 3/8, Batch: 1790/3444, Loss: 0.015280399471521378\n",
      "Epoch: 3/8, Batch: 1800/3444, Loss: 0.026143530383706093\n",
      "Epoch: 3/8, Batch: 1810/3444, Loss: 0.009080651216208935\n",
      "Epoch: 3/8, Batch: 1820/3444, Loss: 0.010773935355246067\n",
      "Epoch: 3/8, Batch: 1830/3444, Loss: 0.027996988967061043\n",
      "Epoch: 3/8, Batch: 1840/3444, Loss: 0.07105419039726257\n",
      "Epoch: 3/8, Batch: 1850/3444, Loss: 0.019950777292251587\n",
      "Epoch: 3/8, Batch: 1860/3444, Loss: 0.042609404772520065\n",
      "Epoch: 3/8, Batch: 1870/3444, Loss: 0.03636370226740837\n",
      "Epoch: 3/8, Batch: 1880/3444, Loss: 0.04127320274710655\n",
      "Epoch: 3/8, Batch: 1890/3444, Loss: 0.024996744468808174\n",
      "Epoch: 3/8, Batch: 1900/3444, Loss: 0.029023360460996628\n",
      "Epoch: 3/8, Batch: 1910/3444, Loss: 0.026423132047057152\n",
      "Epoch: 3/8, Batch: 1920/3444, Loss: 0.020373862236738205\n",
      "Epoch: 3/8, Batch: 1930/3444, Loss: 0.05600772798061371\n",
      "Epoch: 3/8, Batch: 1940/3444, Loss: 0.021793698891997337\n",
      "Epoch: 3/8, Batch: 1950/3444, Loss: 0.02648126892745495\n",
      "Epoch: 3/8, Batch: 1960/3444, Loss: 0.050255756825208664\n",
      "Epoch: 3/8, Batch: 1970/3444, Loss: 0.025600319728255272\n",
      "Epoch: 3/8, Batch: 1980/3444, Loss: 0.01348061766475439\n",
      "Epoch: 3/8, Batch: 1990/3444, Loss: 0.019247135147452354\n",
      "Epoch: 3/8, Batch: 2000/3444, Loss: 0.02469564415514469\n",
      "Epoch: 3/8, Batch: 2010/3444, Loss: 0.016307910904288292\n",
      "Epoch: 3/8, Batch: 2020/3444, Loss: 0.021801043301820755\n",
      "Epoch: 3/8, Batch: 2030/3444, Loss: 0.025235580280423164\n",
      "Epoch: 3/8, Batch: 2040/3444, Loss: 0.03311877325177193\n",
      "Epoch: 3/8, Batch: 2050/3444, Loss: 0.020142048597335815\n",
      "Epoch: 3/8, Batch: 2060/3444, Loss: 0.028335198760032654\n",
      "Epoch: 3/8, Batch: 2070/3444, Loss: 0.03305531293153763\n",
      "Epoch: 3/8, Batch: 2080/3444, Loss: 0.011926466599106789\n",
      "Epoch: 3/8, Batch: 2090/3444, Loss: 0.04530468210577965\n",
      "Epoch: 3/8, Batch: 2100/3444, Loss: 0.05086875706911087\n",
      "Epoch: 3/8, Batch: 2110/3444, Loss: 0.04185693711042404\n",
      "Epoch: 3/8, Batch: 2120/3444, Loss: 0.016762644052505493\n",
      "Epoch: 3/8, Batch: 2130/3444, Loss: 0.03460032120347023\n",
      "Epoch: 3/8, Batch: 2140/3444, Loss: 0.01264472771435976\n",
      "Epoch: 3/8, Batch: 2150/3444, Loss: 0.06331541389226913\n",
      "Epoch: 3/8, Batch: 2160/3444, Loss: 0.02653534896671772\n",
      "Epoch: 3/8, Batch: 2170/3444, Loss: 0.010164671577513218\n",
      "Epoch: 3/8, Batch: 2180/3444, Loss: 0.01391376368701458\n",
      "Epoch: 3/8, Batch: 2190/3444, Loss: 0.04020650312304497\n",
      "Epoch: 3/8, Batch: 2200/3444, Loss: 0.033802226185798645\n",
      "Epoch: 3/8, Batch: 2210/3444, Loss: 0.030881887301802635\n",
      "Epoch: 3/8, Batch: 2220/3444, Loss: 0.07184842228889465\n",
      "Epoch: 3/8, Batch: 2230/3444, Loss: 0.022305401042103767\n",
      "Epoch: 3/8, Batch: 2240/3444, Loss: 0.025846384465694427\n",
      "Epoch: 3/8, Batch: 2250/3444, Loss: 0.014380967244505882\n",
      "Epoch: 3/8, Batch: 2260/3444, Loss: 0.03147479519248009\n",
      "Epoch: 3/8, Batch: 2270/3444, Loss: 0.014482841826975346\n",
      "Epoch: 3/8, Batch: 2280/3444, Loss: 0.012045211158692837\n",
      "Epoch: 3/8, Batch: 2290/3444, Loss: 0.013779784552752972\n",
      "Epoch: 3/8, Batch: 2300/3444, Loss: 0.04773368313908577\n",
      "Epoch: 3/8, Batch: 2310/3444, Loss: 0.03431792929768562\n",
      "Epoch: 3/8, Batch: 2320/3444, Loss: 0.03704611584544182\n",
      "Epoch: 3/8, Batch: 2330/3444, Loss: 0.05299890413880348\n",
      "Epoch: 3/8, Batch: 2340/3444, Loss: 0.03975028172135353\n",
      "Epoch: 3/8, Batch: 2350/3444, Loss: 0.045246027410030365\n",
      "Epoch: 3/8, Batch: 2360/3444, Loss: 0.026816602796316147\n",
      "Epoch: 3/8, Batch: 2370/3444, Loss: 0.014847726561129093\n",
      "Epoch: 3/8, Batch: 2380/3444, Loss: 0.016554275527596474\n",
      "Epoch: 3/8, Batch: 2390/3444, Loss: 0.0189871434122324\n",
      "Epoch: 3/8, Batch: 2400/3444, Loss: 0.04014341160655022\n",
      "Epoch: 3/8, Batch: 2410/3444, Loss: 0.01005652453750372\n",
      "Epoch: 3/8, Batch: 2420/3444, Loss: 0.030173612758517265\n",
      "Epoch: 3/8, Batch: 2430/3444, Loss: 0.05027945339679718\n",
      "Epoch: 3/8, Batch: 2440/3444, Loss: 0.09096325933933258\n",
      "Epoch: 3/8, Batch: 2450/3444, Loss: 0.01836208626627922\n",
      "Epoch: 3/8, Batch: 2460/3444, Loss: 0.0376136489212513\n",
      "Epoch: 3/8, Batch: 2470/3444, Loss: 0.042890019714832306\n",
      "Epoch: 3/8, Batch: 2480/3444, Loss: 0.032950833439826965\n",
      "Epoch: 3/8, Batch: 2490/3444, Loss: 0.028234660625457764\n",
      "Epoch: 3/8, Batch: 2500/3444, Loss: 0.05527473986148834\n",
      "Epoch: 3/8, Batch: 2510/3444, Loss: 0.012659135274589062\n",
      "Epoch: 3/8, Batch: 2520/3444, Loss: 0.029327725991606712\n",
      "Epoch: 3/8, Batch: 2530/3444, Loss: 0.013809998519718647\n",
      "Epoch: 3/8, Batch: 2540/3444, Loss: 0.04800588637590408\n",
      "Epoch: 3/8, Batch: 2550/3444, Loss: 0.05459420010447502\n",
      "Epoch: 3/8, Batch: 2560/3444, Loss: 0.033897094428539276\n",
      "Epoch: 3/8, Batch: 2570/3444, Loss: 0.012711076997220516\n",
      "Epoch: 3/8, Batch: 2580/3444, Loss: 0.03267408162355423\n",
      "Epoch: 3/8, Batch: 2590/3444, Loss: 0.013251041993498802\n",
      "Epoch: 3/8, Batch: 2600/3444, Loss: 0.023489104583859444\n",
      "Epoch: 3/8, Batch: 2610/3444, Loss: 0.02623393014073372\n",
      "Epoch: 3/8, Batch: 2620/3444, Loss: 0.050768207758665085\n",
      "Epoch: 3/8, Batch: 2630/3444, Loss: 0.03362412750720978\n",
      "Epoch: 3/8, Batch: 2640/3444, Loss: 0.03590637072920799\n",
      "Epoch: 3/8, Batch: 2650/3444, Loss: 0.015183642506599426\n",
      "Epoch: 3/8, Batch: 2660/3444, Loss: 0.017648063600063324\n",
      "Epoch: 3/8, Batch: 2670/3444, Loss: 0.013277142308652401\n",
      "Epoch: 3/8, Batch: 2680/3444, Loss: 0.028245827183127403\n",
      "Epoch: 3/8, Batch: 2690/3444, Loss: 0.11716783046722412\n",
      "Epoch: 3/8, Batch: 2700/3444, Loss: 0.016937723383307457\n",
      "Epoch: 3/8, Batch: 2710/3444, Loss: 0.022345613688230515\n",
      "Epoch: 3/8, Batch: 2720/3444, Loss: 0.038953717797994614\n",
      "Epoch: 3/8, Batch: 2730/3444, Loss: 0.035382747650146484\n",
      "Epoch: 3/8, Batch: 2740/3444, Loss: 0.016780447214841843\n",
      "Epoch: 3/8, Batch: 2750/3444, Loss: 0.011575107462704182\n",
      "Epoch: 3/8, Batch: 2760/3444, Loss: 0.04318608343601227\n",
      "Epoch: 3/8, Batch: 2770/3444, Loss: 0.034385453909635544\n",
      "Epoch: 3/8, Batch: 2780/3444, Loss: 0.02510134130716324\n",
      "Epoch: 3/8, Batch: 2790/3444, Loss: 0.04697061702609062\n",
      "Epoch: 3/8, Batch: 2800/3444, Loss: 0.028035344555974007\n",
      "Epoch: 3/8, Batch: 2810/3444, Loss: 0.024478711187839508\n",
      "Epoch: 3/8, Batch: 2820/3444, Loss: 0.03878006711602211\n",
      "Epoch: 3/8, Batch: 2830/3444, Loss: 0.015246434137225151\n",
      "Epoch: 3/8, Batch: 2840/3444, Loss: 0.05950060486793518\n",
      "Epoch: 3/8, Batch: 2850/3444, Loss: 0.031350649893283844\n",
      "Epoch: 3/8, Batch: 2860/3444, Loss: 0.0810481607913971\n",
      "Epoch: 3/8, Batch: 2870/3444, Loss: 0.039106227457523346\n",
      "Epoch: 3/8, Batch: 2880/3444, Loss: 0.07583855837583542\n",
      "Epoch: 3/8, Batch: 2890/3444, Loss: 0.027530329301953316\n",
      "Epoch: 3/8, Batch: 2900/3444, Loss: 0.024312511086463928\n",
      "Epoch: 3/8, Batch: 2910/3444, Loss: 0.024316756054759026\n",
      "Epoch: 3/8, Batch: 2920/3444, Loss: 0.014653313905000687\n",
      "Epoch: 3/8, Batch: 2930/3444, Loss: 0.04260214418172836\n",
      "Epoch: 3/8, Batch: 2940/3444, Loss: 0.0375867523252964\n",
      "Epoch: 3/8, Batch: 2950/3444, Loss: 0.061562348157167435\n",
      "Epoch: 3/8, Batch: 2960/3444, Loss: 0.021761715412139893\n",
      "Epoch: 3/8, Batch: 2970/3444, Loss: 0.043506938964128494\n",
      "Epoch: 3/8, Batch: 2980/3444, Loss: 0.019317150115966797\n",
      "Epoch: 3/8, Batch: 2990/3444, Loss: 0.03911512345075607\n",
      "Epoch: 3/8, Batch: 3000/3444, Loss: 0.014653803780674934\n",
      "Epoch: 3/8, Batch: 3010/3444, Loss: 0.039723314344882965\n",
      "Epoch: 3/8, Batch: 3020/3444, Loss: 0.04355386644601822\n",
      "Epoch: 3/8, Batch: 3030/3444, Loss: 0.013632196933031082\n",
      "Epoch: 3/8, Batch: 3040/3444, Loss: 0.013973434455692768\n",
      "Epoch: 3/8, Batch: 3050/3444, Loss: 0.01625153236091137\n",
      "Epoch: 3/8, Batch: 3060/3444, Loss: 0.022209497168660164\n",
      "Epoch: 3/8, Batch: 3070/3444, Loss: 0.014067582786083221\n",
      "Epoch: 3/8, Batch: 3080/3444, Loss: 0.012134226970374584\n",
      "Epoch: 3/8, Batch: 3090/3444, Loss: 0.04213927686214447\n",
      "Epoch: 3/8, Batch: 3100/3444, Loss: 0.052952297031879425\n",
      "Epoch: 3/8, Batch: 3110/3444, Loss: 0.03530105575919151\n",
      "Epoch: 3/8, Batch: 3120/3444, Loss: 0.04248737171292305\n",
      "Epoch: 3/8, Batch: 3130/3444, Loss: 0.042518384754657745\n",
      "Epoch: 3/8, Batch: 3140/3444, Loss: 0.025334302335977554\n",
      "Epoch: 3/8, Batch: 3150/3444, Loss: 0.044903527945280075\n",
      "Epoch: 3/8, Batch: 3160/3444, Loss: 0.015847953036427498\n",
      "Epoch: 3/8, Batch: 3170/3444, Loss: 0.14507469534873962\n",
      "Epoch: 3/8, Batch: 3180/3444, Loss: 0.03393936902284622\n",
      "Epoch: 3/8, Batch: 3190/3444, Loss: 0.014189787209033966\n",
      "Epoch: 3/8, Batch: 3200/3444, Loss: 0.020722275599837303\n",
      "Epoch: 3/8, Batch: 3210/3444, Loss: 0.014602967537939548\n",
      "Epoch: 3/8, Batch: 3220/3444, Loss: 0.019987206906080246\n",
      "Epoch: 3/8, Batch: 3230/3444, Loss: 0.028150122612714767\n",
      "Epoch: 3/8, Batch: 3240/3444, Loss: 0.13524162769317627\n",
      "Epoch: 3/8, Batch: 3250/3444, Loss: 0.02937684766948223\n",
      "Epoch: 3/8, Batch: 3260/3444, Loss: 0.16270779073238373\n",
      "Epoch: 3/8, Batch: 3270/3444, Loss: 0.036386407911777496\n",
      "Epoch: 3/8, Batch: 3280/3444, Loss: 0.03513151779770851\n",
      "Epoch: 3/8, Batch: 3290/3444, Loss: 0.03206782042980194\n",
      "Epoch: 3/8, Batch: 3300/3444, Loss: 0.027238355949521065\n",
      "Epoch: 3/8, Batch: 3310/3444, Loss: 0.01522643119096756\n",
      "Epoch: 3/8, Batch: 3320/3444, Loss: 0.043841514736413956\n",
      "Epoch: 3/8, Batch: 3330/3444, Loss: 0.04289195314049721\n",
      "Epoch: 3/8, Batch: 3340/3444, Loss: 0.013097815215587616\n",
      "Epoch: 3/8, Batch: 3350/3444, Loss: 0.05668118968605995\n",
      "Epoch: 3/8, Batch: 3360/3444, Loss: 0.048052918165922165\n",
      "Epoch: 3/8, Batch: 3370/3444, Loss: 0.05773749575018883\n",
      "Epoch: 3/8, Batch: 3380/3444, Loss: 0.023487720638513565\n",
      "Epoch: 3/8, Batch: 3390/3444, Loss: 0.02333138696849346\n",
      "Epoch: 3/8, Batch: 3400/3444, Loss: 0.019528167322278023\n",
      "Epoch: 3/8, Batch: 3410/3444, Loss: 0.017463654279708862\n",
      "Epoch: 3/8, Batch: 3420/3444, Loss: 0.03331250324845314\n",
      "Epoch: 3/8, Batch: 3430/3444, Loss: 0.03047131560742855\n",
      "Epoch: 3/8, Batch: 3440/3444, Loss: 0.02308284118771553\n",
      "Epoch 00004: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch: 3/8, Val Loss: 0.0652857701857677\n",
      "Epoch: 4/8, Batch: 10/3444, Loss: 0.04096011444926262\n",
      "Epoch: 4/8, Batch: 20/3444, Loss: 0.013701518066227436\n",
      "Epoch: 4/8, Batch: 30/3444, Loss: 0.01848548650741577\n",
      "Epoch: 4/8, Batch: 40/3444, Loss: 0.0227985717356205\n",
      "Epoch: 4/8, Batch: 50/3444, Loss: 0.013988523744046688\n",
      "Epoch: 4/8, Batch: 60/3444, Loss: 0.01958298124372959\n",
      "Epoch: 4/8, Batch: 70/3444, Loss: 0.01542229950428009\n",
      "Epoch: 4/8, Batch: 80/3444, Loss: 0.013539678417146206\n",
      "Epoch: 4/8, Batch: 90/3444, Loss: 0.00989209208637476\n",
      "Epoch: 4/8, Batch: 100/3444, Loss: 0.01826981082558632\n",
      "Epoch: 4/8, Batch: 110/3444, Loss: 0.010193511843681335\n",
      "Epoch: 4/8, Batch: 120/3444, Loss: 0.03257784992456436\n",
      "Epoch: 4/8, Batch: 130/3444, Loss: 0.027722453698515892\n",
      "Epoch: 4/8, Batch: 140/3444, Loss: 0.01039991993457079\n",
      "Epoch: 4/8, Batch: 150/3444, Loss: 0.01418356690555811\n",
      "Epoch: 4/8, Batch: 160/3444, Loss: 0.01739097572863102\n",
      "Epoch: 4/8, Batch: 170/3444, Loss: 0.01497466117143631\n",
      "Epoch: 4/8, Batch: 180/3444, Loss: 0.01628217287361622\n",
      "Epoch: 4/8, Batch: 190/3444, Loss: 0.022657811641693115\n",
      "Epoch: 4/8, Batch: 200/3444, Loss: 0.014687590301036835\n",
      "Epoch: 4/8, Batch: 210/3444, Loss: 0.018199829384684563\n",
      "Epoch: 4/8, Batch: 220/3444, Loss: 0.011388707906007767\n",
      "Epoch: 4/8, Batch: 230/3444, Loss: 0.018812352791428566\n",
      "Epoch: 4/8, Batch: 240/3444, Loss: 0.0229618102312088\n",
      "Epoch: 4/8, Batch: 250/3444, Loss: 0.018918834626674652\n",
      "Epoch: 4/8, Batch: 260/3444, Loss: 0.03631699085235596\n",
      "Epoch: 4/8, Batch: 270/3444, Loss: 0.013079291209578514\n",
      "Epoch: 4/8, Batch: 280/3444, Loss: 0.017679056152701378\n",
      "Epoch: 4/8, Batch: 290/3444, Loss: 0.030100008472800255\n",
      "Epoch: 4/8, Batch: 300/3444, Loss: 0.016148431226611137\n",
      "Epoch: 4/8, Batch: 310/3444, Loss: 0.02073153853416443\n",
      "Epoch: 4/8, Batch: 320/3444, Loss: 0.011671611107885838\n",
      "Epoch: 4/8, Batch: 330/3444, Loss: 0.009934230707585812\n",
      "Epoch: 4/8, Batch: 340/3444, Loss: 0.02356112189590931\n",
      "Epoch: 4/8, Batch: 350/3444, Loss: 0.019847987219691277\n",
      "Epoch: 4/8, Batch: 360/3444, Loss: 0.026521962136030197\n",
      "Epoch: 4/8, Batch: 370/3444, Loss: 0.022311853244900703\n",
      "Epoch: 4/8, Batch: 380/3444, Loss: 0.016323702409863472\n",
      "Epoch: 4/8, Batch: 390/3444, Loss: 0.004163956269621849\n",
      "Epoch: 4/8, Batch: 400/3444, Loss: 0.01692299172282219\n",
      "Epoch: 4/8, Batch: 410/3444, Loss: 0.011823573149740696\n",
      "Epoch: 4/8, Batch: 420/3444, Loss: 0.03638780489563942\n",
      "Epoch: 4/8, Batch: 430/3444, Loss: 0.01712573505938053\n",
      "Epoch: 4/8, Batch: 440/3444, Loss: 0.015571351163089275\n",
      "Epoch: 4/8, Batch: 450/3444, Loss: 0.04682676121592522\n",
      "Epoch: 4/8, Batch: 460/3444, Loss: 0.008401934057474136\n",
      "Epoch: 4/8, Batch: 470/3444, Loss: 0.01660042814910412\n",
      "Epoch: 4/8, Batch: 480/3444, Loss: 0.032301489263772964\n",
      "Epoch: 4/8, Batch: 490/3444, Loss: 0.014402317814528942\n",
      "Epoch: 4/8, Batch: 500/3444, Loss: 0.05039806291460991\n",
      "Epoch: 4/8, Batch: 510/3444, Loss: 0.04195360466837883\n",
      "Epoch: 4/8, Batch: 520/3444, Loss: 0.025017987936735153\n",
      "Epoch: 4/8, Batch: 530/3444, Loss: 0.006985831074416637\n",
      "Epoch: 4/8, Batch: 540/3444, Loss: 0.030678723007440567\n",
      "Epoch: 4/8, Batch: 550/3444, Loss: 0.021709440276026726\n",
      "Epoch: 4/8, Batch: 560/3444, Loss: 0.025282327085733414\n",
      "Epoch: 4/8, Batch: 570/3444, Loss: 0.012986776418983936\n",
      "Epoch: 4/8, Batch: 580/3444, Loss: 0.014701741747558117\n",
      "Epoch: 4/8, Batch: 590/3444, Loss: 0.024970296770334244\n",
      "Epoch: 4/8, Batch: 600/3444, Loss: 0.00591708580031991\n",
      "Epoch: 4/8, Batch: 610/3444, Loss: 0.023671763017773628\n",
      "Epoch: 4/8, Batch: 620/3444, Loss: 0.01924397237598896\n",
      "Epoch: 4/8, Batch: 630/3444, Loss: 0.029694607481360435\n",
      "Epoch: 4/8, Batch: 640/3444, Loss: 0.015843896195292473\n",
      "Epoch: 4/8, Batch: 650/3444, Loss: 0.007930620573461056\n",
      "Epoch: 4/8, Batch: 660/3444, Loss: 0.015249455347657204\n",
      "Epoch: 4/8, Batch: 670/3444, Loss: 0.019500402733683586\n",
      "Epoch: 4/8, Batch: 680/3444, Loss: 0.00977699551731348\n",
      "Epoch: 4/8, Batch: 690/3444, Loss: 0.01824985258281231\n",
      "Epoch: 4/8, Batch: 700/3444, Loss: 0.012302425689995289\n",
      "Epoch: 4/8, Batch: 710/3444, Loss: 0.012351985089480877\n",
      "Epoch: 4/8, Batch: 720/3444, Loss: 0.02450091950595379\n",
      "Epoch: 4/8, Batch: 730/3444, Loss: 0.0111448485404253\n",
      "Epoch: 4/8, Batch: 740/3444, Loss: 0.04058471694588661\n",
      "Epoch: 4/8, Batch: 750/3444, Loss: 0.012330855242908001\n",
      "Epoch: 4/8, Batch: 760/3444, Loss: 0.013021028600633144\n",
      "Epoch: 4/8, Batch: 770/3444, Loss: 0.028536055237054825\n",
      "Epoch: 4/8, Batch: 780/3444, Loss: 0.012799632735550404\n",
      "Epoch: 4/8, Batch: 790/3444, Loss: 0.01829177886247635\n",
      "Epoch: 4/8, Batch: 800/3444, Loss: 0.04683275893330574\n",
      "Epoch: 4/8, Batch: 810/3444, Loss: 0.00944718811661005\n",
      "Epoch: 4/8, Batch: 820/3444, Loss: 0.011862443760037422\n",
      "Epoch: 4/8, Batch: 830/3444, Loss: 0.019400451332330704\n",
      "Epoch: 4/8, Batch: 840/3444, Loss: 0.020650535821914673\n",
      "Epoch: 4/8, Batch: 850/3444, Loss: 0.008065150119364262\n",
      "Epoch: 4/8, Batch: 860/3444, Loss: 0.010835755616426468\n",
      "Epoch: 4/8, Batch: 870/3444, Loss: 0.011018182151019573\n",
      "Epoch: 4/8, Batch: 880/3444, Loss: 0.0189185943454504\n",
      "Epoch: 4/8, Batch: 890/3444, Loss: 0.04272780194878578\n",
      "Epoch: 4/8, Batch: 900/3444, Loss: 0.05319547280669212\n",
      "Epoch: 4/8, Batch: 910/3444, Loss: 0.02494869753718376\n",
      "Epoch: 4/8, Batch: 920/3444, Loss: 0.021230289712548256\n",
      "Epoch: 4/8, Batch: 930/3444, Loss: 0.019023867323994637\n",
      "Epoch: 4/8, Batch: 940/3444, Loss: 0.02747548371553421\n",
      "Epoch: 4/8, Batch: 950/3444, Loss: 0.015051932074129581\n",
      "Epoch: 4/8, Batch: 960/3444, Loss: 0.009333615191280842\n",
      "Epoch: 4/8, Batch: 970/3444, Loss: 0.010142887942492962\n",
      "Epoch: 4/8, Batch: 980/3444, Loss: 0.009464463219046593\n",
      "Epoch: 4/8, Batch: 990/3444, Loss: 0.02972126565873623\n",
      "Epoch: 4/8, Batch: 1000/3444, Loss: 0.008981317281723022\n",
      "Epoch: 4/8, Batch: 1010/3444, Loss: 0.010504716075956821\n",
      "Epoch: 4/8, Batch: 1020/3444, Loss: 0.01193761546164751\n",
      "Epoch: 4/8, Batch: 1030/3444, Loss: 0.006742061581462622\n",
      "Epoch: 4/8, Batch: 1040/3444, Loss: 0.006937049794942141\n",
      "Epoch: 4/8, Batch: 1050/3444, Loss: 0.028843384236097336\n",
      "Epoch: 4/8, Batch: 1060/3444, Loss: 0.006735766772180796\n",
      "Epoch: 4/8, Batch: 1070/3444, Loss: 0.006566063966602087\n",
      "Epoch: 4/8, Batch: 1080/3444, Loss: 0.013311794959008694\n",
      "Epoch: 4/8, Batch: 1090/3444, Loss: 0.025165077298879623\n",
      "Epoch: 4/8, Batch: 1100/3444, Loss: 0.01083084661513567\n",
      "Epoch: 4/8, Batch: 1110/3444, Loss: 0.02194996550679207\n",
      "Epoch: 4/8, Batch: 1120/3444, Loss: 0.02147800102829933\n",
      "Epoch: 4/8, Batch: 1130/3444, Loss: 0.016853267326951027\n",
      "Epoch: 4/8, Batch: 1140/3444, Loss: 0.01628720946609974\n",
      "Epoch: 4/8, Batch: 1150/3444, Loss: 0.01277950219810009\n",
      "Epoch: 4/8, Batch: 1160/3444, Loss: 0.022442413493990898\n",
      "Epoch: 4/8, Batch: 1170/3444, Loss: 0.016187164932489395\n",
      "Epoch: 4/8, Batch: 1180/3444, Loss: 0.03078143671154976\n",
      "Epoch: 4/8, Batch: 1190/3444, Loss: 0.007969992235302925\n",
      "Epoch: 4/8, Batch: 1200/3444, Loss: 0.008394382894039154\n",
      "Epoch: 4/8, Batch: 1210/3444, Loss: 0.00850583240389824\n",
      "Epoch: 4/8, Batch: 1220/3444, Loss: 0.0124824158847332\n",
      "Epoch: 4/8, Batch: 1230/3444, Loss: 0.021313292905688286\n",
      "Epoch: 4/8, Batch: 1240/3444, Loss: 0.0176481194794178\n",
      "Epoch: 4/8, Batch: 1250/3444, Loss: 0.02018154412508011\n",
      "Epoch: 4/8, Batch: 1260/3444, Loss: 0.024970294907689095\n",
      "Epoch: 4/8, Batch: 1270/3444, Loss: 0.021433955058455467\n",
      "Epoch: 4/8, Batch: 1280/3444, Loss: 0.02068939618766308\n",
      "Epoch: 4/8, Batch: 1290/3444, Loss: 0.019948285073041916\n",
      "Epoch: 4/8, Batch: 1300/3444, Loss: 0.027587562799453735\n",
      "Epoch: 4/8, Batch: 1310/3444, Loss: 0.02190716192126274\n",
      "Epoch: 4/8, Batch: 1320/3444, Loss: 0.011644279584288597\n",
      "Epoch: 4/8, Batch: 1330/3444, Loss: 0.017507383599877357\n",
      "Epoch: 4/8, Batch: 1340/3444, Loss: 0.018177561461925507\n",
      "Epoch: 4/8, Batch: 1350/3444, Loss: 0.012720772996544838\n",
      "Epoch: 4/8, Batch: 1360/3444, Loss: 0.007014248985797167\n",
      "Epoch: 4/8, Batch: 1370/3444, Loss: 0.014694182202219963\n",
      "Epoch: 4/8, Batch: 1380/3444, Loss: 0.01677631586790085\n",
      "Epoch: 4/8, Batch: 1390/3444, Loss: 0.010016978718340397\n",
      "Epoch: 4/8, Batch: 1400/3444, Loss: 0.01361309364438057\n",
      "Epoch: 4/8, Batch: 1410/3444, Loss: 0.00964373629540205\n",
      "Epoch: 4/8, Batch: 1420/3444, Loss: 0.011010278947651386\n",
      "Epoch: 4/8, Batch: 1430/3444, Loss: 0.015552288852632046\n",
      "Epoch: 4/8, Batch: 1440/3444, Loss: 0.012567853555083275\n",
      "Epoch: 4/8, Batch: 1450/3444, Loss: 0.023707469925284386\n",
      "Epoch: 4/8, Batch: 1460/3444, Loss: 0.012621533125638962\n",
      "Epoch: 4/8, Batch: 1470/3444, Loss: 0.020211094990372658\n",
      "Epoch: 4/8, Batch: 1480/3444, Loss: 0.0175587497651577\n",
      "Epoch: 4/8, Batch: 1490/3444, Loss: 0.031072253361344337\n",
      "Epoch: 4/8, Batch: 1500/3444, Loss: 0.0128599489107728\n",
      "Epoch: 4/8, Batch: 1510/3444, Loss: 0.0179495457559824\n",
      "Epoch: 4/8, Batch: 1520/3444, Loss: 0.011707257479429245\n",
      "Epoch: 4/8, Batch: 1530/3444, Loss: 0.021834220737218857\n",
      "Epoch: 4/8, Batch: 1540/3444, Loss: 0.017416764050722122\n",
      "Epoch: 4/8, Batch: 1550/3444, Loss: 0.01914040371775627\n",
      "Epoch: 4/8, Batch: 1560/3444, Loss: 0.018744833767414093\n",
      "Epoch: 4/8, Batch: 1570/3444, Loss: 0.020000804215669632\n",
      "Epoch: 4/8, Batch: 1580/3444, Loss: 0.042998600751161575\n",
      "Epoch: 4/8, Batch: 1590/3444, Loss: 0.010500702075660229\n",
      "Epoch: 4/8, Batch: 1600/3444, Loss: 0.016577089205384254\n",
      "Epoch: 4/8, Batch: 1610/3444, Loss: 0.007755578961223364\n",
      "Epoch: 4/8, Batch: 1620/3444, Loss: 0.009639771655201912\n",
      "Epoch: 4/8, Batch: 1630/3444, Loss: 0.024238457903265953\n",
      "Epoch: 4/8, Batch: 1640/3444, Loss: 0.01067543588578701\n",
      "Epoch: 4/8, Batch: 1650/3444, Loss: 0.008643784560263157\n",
      "Epoch: 4/8, Batch: 1660/3444, Loss: 0.009840899147093296\n",
      "Epoch: 4/8, Batch: 1670/3444, Loss: 0.011196410283446312\n",
      "Epoch: 4/8, Batch: 1680/3444, Loss: 0.026852557435631752\n",
      "Epoch: 4/8, Batch: 1690/3444, Loss: 0.02429535798728466\n",
      "Epoch: 4/8, Batch: 1700/3444, Loss: 0.016293874010443687\n",
      "Epoch: 4/8, Batch: 1710/3444, Loss: 0.008998560719192028\n",
      "Epoch: 4/8, Batch: 1720/3444, Loss: 0.019881773740053177\n",
      "Epoch: 4/8, Batch: 1730/3444, Loss: 0.009631761349737644\n",
      "Epoch: 4/8, Batch: 1740/3444, Loss: 0.01202800776809454\n",
      "Epoch: 4/8, Batch: 1750/3444, Loss: 0.009835656732320786\n",
      "Epoch: 4/8, Batch: 1760/3444, Loss: 0.0429852232336998\n",
      "Epoch: 4/8, Batch: 1770/3444, Loss: 0.006235243286937475\n",
      "Epoch: 4/8, Batch: 1780/3444, Loss: 0.012332449667155743\n",
      "Epoch: 4/8, Batch: 1790/3444, Loss: 0.009686552919447422\n",
      "Epoch: 4/8, Batch: 1800/3444, Loss: 0.011391783133149147\n",
      "Epoch: 4/8, Batch: 1810/3444, Loss: 0.010322102345526218\n",
      "Epoch: 4/8, Batch: 1820/3444, Loss: 0.009105355478823185\n",
      "Epoch: 4/8, Batch: 1830/3444, Loss: 0.009319433011114597\n",
      "Epoch: 4/8, Batch: 1840/3444, Loss: 0.03228909894824028\n",
      "Epoch: 4/8, Batch: 1850/3444, Loss: 0.03793453052639961\n",
      "Epoch: 4/8, Batch: 1860/3444, Loss: 0.010843002237379551\n",
      "Epoch: 4/8, Batch: 1870/3444, Loss: 0.013070649467408657\n",
      "Epoch: 4/8, Batch: 1880/3444, Loss: 0.01719513349235058\n",
      "Epoch: 4/8, Batch: 1890/3444, Loss: 0.027121931314468384\n",
      "Epoch: 4/8, Batch: 1900/3444, Loss: 0.010301394388079643\n",
      "Epoch: 4/8, Batch: 1910/3444, Loss: 0.02424321137368679\n",
      "Epoch: 4/8, Batch: 1920/3444, Loss: 0.009643645957112312\n",
      "Epoch: 4/8, Batch: 1930/3444, Loss: 0.015504107810556889\n",
      "Epoch: 4/8, Batch: 1940/3444, Loss: 0.004686703905463219\n",
      "Epoch: 4/8, Batch: 1950/3444, Loss: 0.014678619801998138\n",
      "Epoch: 4/8, Batch: 1960/3444, Loss: 0.01992187835276127\n",
      "Epoch: 4/8, Batch: 1970/3444, Loss: 0.026814298704266548\n",
      "Epoch: 4/8, Batch: 1980/3444, Loss: 0.019825274124741554\n",
      "Epoch: 4/8, Batch: 1990/3444, Loss: 0.011897766031324863\n",
      "Epoch: 4/8, Batch: 2000/3444, Loss: 0.013878744095563889\n",
      "Epoch: 4/8, Batch: 2010/3444, Loss: 0.007137899287045002\n",
      "Epoch: 4/8, Batch: 2020/3444, Loss: 0.019730225205421448\n",
      "Epoch: 4/8, Batch: 2030/3444, Loss: 0.00781187554821372\n",
      "Epoch: 4/8, Batch: 2040/3444, Loss: 0.006101525388658047\n",
      "Epoch: 4/8, Batch: 2050/3444, Loss: 0.01224715355783701\n",
      "Epoch: 4/8, Batch: 2060/3444, Loss: 0.03555186465382576\n",
      "Epoch: 4/8, Batch: 2070/3444, Loss: 0.0235489122569561\n",
      "Epoch: 4/8, Batch: 2080/3444, Loss: 0.024759992957115173\n",
      "Epoch: 4/8, Batch: 2090/3444, Loss: 0.023535368964076042\n",
      "Epoch: 4/8, Batch: 2100/3444, Loss: 0.012141396291553974\n",
      "Epoch: 4/8, Batch: 2110/3444, Loss: 0.013222633861005306\n",
      "Epoch: 4/8, Batch: 2120/3444, Loss: 0.01058436743915081\n",
      "Epoch: 4/8, Batch: 2130/3444, Loss: 0.013213999569416046\n",
      "Epoch: 4/8, Batch: 2140/3444, Loss: 0.02047862857580185\n",
      "Epoch: 4/8, Batch: 2150/3444, Loss: 0.025036565959453583\n",
      "Epoch: 4/8, Batch: 2160/3444, Loss: 0.010794241912662983\n",
      "Epoch: 4/8, Batch: 2170/3444, Loss: 0.01552408654242754\n",
      "Epoch: 4/8, Batch: 2180/3444, Loss: 0.02787722833454609\n",
      "Epoch: 4/8, Batch: 2190/3444, Loss: 0.02255033329129219\n",
      "Epoch: 4/8, Batch: 2200/3444, Loss: 0.011916045099496841\n",
      "Epoch: 4/8, Batch: 2210/3444, Loss: 0.010488197207450867\n",
      "Epoch: 4/8, Batch: 2220/3444, Loss: 0.026093965396285057\n",
      "Epoch: 4/8, Batch: 2230/3444, Loss: 0.009024372324347496\n",
      "Epoch: 4/8, Batch: 2240/3444, Loss: 0.016261788085103035\n",
      "Epoch: 4/8, Batch: 2250/3444, Loss: 0.02371967025101185\n",
      "Epoch: 4/8, Batch: 2260/3444, Loss: 0.010752703063189983\n",
      "Epoch: 4/8, Batch: 2270/3444, Loss: 0.010998775251209736\n",
      "Epoch: 4/8, Batch: 2280/3444, Loss: 0.008152441121637821\n",
      "Epoch: 4/8, Batch: 2290/3444, Loss: 0.010618836618959904\n",
      "Epoch: 4/8, Batch: 2300/3444, Loss: 0.04772059619426727\n",
      "Epoch: 4/8, Batch: 2310/3444, Loss: 0.011885999701917171\n",
      "Epoch: 4/8, Batch: 2320/3444, Loss: 0.011504829861223698\n",
      "Epoch: 4/8, Batch: 2330/3444, Loss: 0.018795553594827652\n",
      "Epoch: 4/8, Batch: 2340/3444, Loss: 0.01342624332755804\n",
      "Epoch: 4/8, Batch: 2350/3444, Loss: 0.010072333738207817\n",
      "Epoch: 4/8, Batch: 2360/3444, Loss: 0.012159026227891445\n",
      "Epoch: 4/8, Batch: 2370/3444, Loss: 0.01708100363612175\n",
      "Epoch: 4/8, Batch: 2380/3444, Loss: 0.01860436424612999\n",
      "Epoch: 4/8, Batch: 2390/3444, Loss: 0.02617604471743107\n",
      "Epoch: 4/8, Batch: 2400/3444, Loss: 0.02910613641142845\n",
      "Epoch: 4/8, Batch: 2410/3444, Loss: 0.016220130026340485\n",
      "Epoch: 4/8, Batch: 2420/3444, Loss: 0.013519755564630032\n",
      "Epoch: 4/8, Batch: 2430/3444, Loss: 0.015085157006978989\n",
      "Epoch: 4/8, Batch: 2440/3444, Loss: 0.050459492951631546\n",
      "Epoch: 4/8, Batch: 2450/3444, Loss: 0.020894262939691544\n",
      "Epoch: 4/8, Batch: 2460/3444, Loss: 0.006229982245713472\n",
      "Epoch: 4/8, Batch: 2470/3444, Loss: 0.023961437866091728\n",
      "Epoch: 4/8, Batch: 2480/3444, Loss: 0.01924818381667137\n",
      "Epoch: 4/8, Batch: 2490/3444, Loss: 0.02319931797683239\n",
      "Epoch: 4/8, Batch: 2500/3444, Loss: 0.016892092302441597\n",
      "Epoch: 4/8, Batch: 2510/3444, Loss: 0.01714448258280754\n",
      "Epoch: 4/8, Batch: 2520/3444, Loss: 0.011591171845793724\n",
      "Epoch: 4/8, Batch: 2530/3444, Loss: 0.02206745557487011\n",
      "Epoch: 4/8, Batch: 2540/3444, Loss: 0.015604527667164803\n",
      "Epoch: 4/8, Batch: 2550/3444, Loss: 0.011083903722465038\n",
      "Epoch: 4/8, Batch: 2560/3444, Loss: 0.009179194457828999\n",
      "Epoch: 4/8, Batch: 2570/3444, Loss: 0.008974393829703331\n",
      "Epoch: 4/8, Batch: 2580/3444, Loss: 0.028988203033804893\n",
      "Epoch: 4/8, Batch: 2590/3444, Loss: 0.03718280792236328\n",
      "Epoch: 4/8, Batch: 2600/3444, Loss: 0.014800447039306164\n",
      "Epoch: 4/8, Batch: 2610/3444, Loss: 0.012910377234220505\n",
      "Epoch: 4/8, Batch: 2620/3444, Loss: 0.0033386857248842716\n",
      "Epoch: 4/8, Batch: 2630/3444, Loss: 0.022313330322504044\n",
      "Epoch: 4/8, Batch: 2640/3444, Loss: 0.014469707384705544\n",
      "Epoch: 4/8, Batch: 2650/3444, Loss: 0.010423731058835983\n",
      "Epoch: 4/8, Batch: 2660/3444, Loss: 0.015236606821417809\n",
      "Epoch: 4/8, Batch: 2670/3444, Loss: 0.03635665029287338\n",
      "Epoch: 4/8, Batch: 2680/3444, Loss: 0.018102848902344704\n",
      "Epoch: 4/8, Batch: 2690/3444, Loss: 0.018907852470874786\n",
      "Epoch: 4/8, Batch: 2700/3444, Loss: 0.013820206746459007\n",
      "Epoch: 4/8, Batch: 2710/3444, Loss: 0.0161538515239954\n",
      "Epoch: 4/8, Batch: 2720/3444, Loss: 0.0050770617090165615\n",
      "Epoch: 4/8, Batch: 2730/3444, Loss: 0.007122888695448637\n",
      "Epoch: 4/8, Batch: 2740/3444, Loss: 0.009813736192882061\n",
      "Epoch: 4/8, Batch: 2750/3444, Loss: 0.012730223126709461\n",
      "Epoch: 4/8, Batch: 2760/3444, Loss: 0.0092069823294878\n",
      "Epoch: 4/8, Batch: 2770/3444, Loss: 0.010710425674915314\n",
      "Epoch: 4/8, Batch: 2780/3444, Loss: 0.015714216977357864\n",
      "Epoch: 4/8, Batch: 2790/3444, Loss: 0.020441342145204544\n",
      "Epoch: 4/8, Batch: 2800/3444, Loss: 0.02130007930099964\n",
      "Epoch: 4/8, Batch: 2810/3444, Loss: 0.04901140555739403\n",
      "Epoch: 4/8, Batch: 2820/3444, Loss: 0.011434938758611679\n",
      "Epoch: 4/8, Batch: 2830/3444, Loss: 0.025121871381998062\n",
      "Epoch: 4/8, Batch: 2840/3444, Loss: 0.01395372673869133\n",
      "Epoch: 4/8, Batch: 2850/3444, Loss: 0.013328053057193756\n",
      "Epoch: 4/8, Batch: 2860/3444, Loss: 0.02461749128997326\n",
      "Epoch: 4/8, Batch: 2870/3444, Loss: 0.0276267621666193\n",
      "Epoch: 4/8, Batch: 2880/3444, Loss: 0.011937347240746021\n",
      "Epoch: 4/8, Batch: 2890/3444, Loss: 0.010532625950872898\n",
      "Epoch: 4/8, Batch: 2900/3444, Loss: 0.009791023097932339\n",
      "Epoch: 4/8, Batch: 2910/3444, Loss: 0.008599766530096531\n",
      "Epoch: 4/8, Batch: 2920/3444, Loss: 0.021201996132731438\n",
      "Epoch: 4/8, Batch: 2930/3444, Loss: 0.011676507070660591\n",
      "Epoch: 4/8, Batch: 2940/3444, Loss: 0.015758488327264786\n",
      "Epoch: 4/8, Batch: 2950/3444, Loss: 0.0225013867020607\n",
      "Epoch: 4/8, Batch: 2960/3444, Loss: 0.024427680298686028\n",
      "Epoch: 4/8, Batch: 2970/3444, Loss: 0.02252366952598095\n",
      "Epoch: 4/8, Batch: 2980/3444, Loss: 0.013422702439129353\n",
      "Epoch: 4/8, Batch: 2990/3444, Loss: 0.011302486062049866\n",
      "Epoch: 4/8, Batch: 3000/3444, Loss: 0.010943764820694923\n",
      "Epoch: 4/8, Batch: 3010/3444, Loss: 0.014458713121712208\n",
      "Epoch: 4/8, Batch: 3020/3444, Loss: 0.008587619289755821\n",
      "Epoch: 4/8, Batch: 3030/3444, Loss: 0.02228696458041668\n",
      "Epoch: 4/8, Batch: 3040/3444, Loss: 0.006282716989517212\n",
      "Epoch: 4/8, Batch: 3050/3444, Loss: 0.011304767802357674\n",
      "Epoch: 4/8, Batch: 3060/3444, Loss: 0.029733631759881973\n",
      "Epoch: 4/8, Batch: 3070/3444, Loss: 0.009890081360936165\n",
      "Epoch: 4/8, Batch: 3080/3444, Loss: 0.007811781484633684\n",
      "Epoch: 4/8, Batch: 3090/3444, Loss: 0.03159589320421219\n",
      "Epoch: 4/8, Batch: 3100/3444, Loss: 0.015630943700671196\n",
      "Epoch: 4/8, Batch: 3110/3444, Loss: 0.014287613332271576\n",
      "Epoch: 4/8, Batch: 3120/3444, Loss: 0.0287956390529871\n",
      "Epoch: 4/8, Batch: 3130/3444, Loss: 0.01478760689496994\n",
      "Epoch: 4/8, Batch: 3140/3444, Loss: 0.009847455658018589\n",
      "Epoch: 4/8, Batch: 3150/3444, Loss: 0.013767455704510212\n",
      "Epoch: 4/8, Batch: 3160/3444, Loss: 0.010217436589300632\n",
      "Epoch: 4/8, Batch: 3170/3444, Loss: 0.018884707242250443\n",
      "Epoch: 4/8, Batch: 3180/3444, Loss: 0.026348726823925972\n",
      "Epoch: 4/8, Batch: 3190/3444, Loss: 0.010488799773156643\n",
      "Epoch: 4/8, Batch: 3200/3444, Loss: 0.014226137660443783\n",
      "Epoch: 4/8, Batch: 3210/3444, Loss: 0.014479330740869045\n",
      "Epoch: 4/8, Batch: 3220/3444, Loss: 0.018852852284908295\n",
      "Epoch: 4/8, Batch: 3230/3444, Loss: 0.022023985162377357\n",
      "Epoch: 4/8, Batch: 3240/3444, Loss: 0.015771692618727684\n",
      "Epoch: 4/8, Batch: 3250/3444, Loss: 0.010488174855709076\n",
      "Epoch: 4/8, Batch: 3260/3444, Loss: 0.03978610411286354\n",
      "Epoch: 4/8, Batch: 3270/3444, Loss: 0.03913667052984238\n",
      "Epoch: 4/8, Batch: 3280/3444, Loss: 0.009734831750392914\n",
      "Epoch: 4/8, Batch: 3290/3444, Loss: 0.011344064958393574\n",
      "Epoch: 4/8, Batch: 3300/3444, Loss: 0.014357618056237698\n",
      "Epoch: 4/8, Batch: 3310/3444, Loss: 0.01705935224890709\n",
      "Epoch: 4/8, Batch: 3320/3444, Loss: 0.014337129890918732\n",
      "Epoch: 4/8, Batch: 3330/3444, Loss: 0.017308717593550682\n",
      "Epoch: 4/8, Batch: 3340/3444, Loss: 0.011158392764627934\n",
      "Epoch: 4/8, Batch: 3350/3444, Loss: 0.03703194111585617\n",
      "Epoch: 4/8, Batch: 3360/3444, Loss: 0.011962692253291607\n",
      "Epoch: 4/8, Batch: 3370/3444, Loss: 0.0074242097325623035\n",
      "Epoch: 4/8, Batch: 3380/3444, Loss: 0.018519636243581772\n",
      "Epoch: 4/8, Batch: 3390/3444, Loss: 0.0062266322784125805\n",
      "Epoch: 4/8, Batch: 3400/3444, Loss: 0.016336748376488686\n",
      "Epoch: 4/8, Batch: 3410/3444, Loss: 0.01773999258875847\n",
      "Epoch: 4/8, Batch: 3420/3444, Loss: 0.006464366335421801\n",
      "Epoch: 4/8, Batch: 3430/3444, Loss: 0.005622670520097017\n",
      "Epoch: 4/8, Batch: 3440/3444, Loss: 0.01693032495677471\n",
      "Epoch: 4/8, Val Loss: 0.023796925515382098\n",
      "Epoch: 5/8, Batch: 10/3444, Loss: 0.013318659737706184\n",
      "Epoch: 5/8, Batch: 20/3444, Loss: 0.013539689593017101\n",
      "Epoch: 5/8, Batch: 30/3444, Loss: 0.019458161666989326\n",
      "Epoch: 5/8, Batch: 40/3444, Loss: 0.01063531357795\n",
      "Epoch: 5/8, Batch: 50/3444, Loss: 0.016476597636938095\n",
      "Epoch: 5/8, Batch: 60/3444, Loss: 0.004954922944307327\n",
      "Epoch: 5/8, Batch: 70/3444, Loss: 0.0205267034471035\n",
      "Epoch: 5/8, Batch: 80/3444, Loss: 0.011236158199608326\n",
      "Epoch: 5/8, Batch: 90/3444, Loss: 0.009839304722845554\n",
      "Epoch: 5/8, Batch: 100/3444, Loss: 0.01941351592540741\n",
      "Epoch: 5/8, Batch: 110/3444, Loss: 0.010400751605629921\n",
      "Epoch: 5/8, Batch: 120/3444, Loss: 0.012697411701083183\n",
      "Epoch: 5/8, Batch: 130/3444, Loss: 0.013336939737200737\n",
      "Epoch: 5/8, Batch: 140/3444, Loss: 0.03101087175309658\n",
      "Epoch: 5/8, Batch: 150/3444, Loss: 0.019700543954968452\n",
      "Epoch: 5/8, Batch: 160/3444, Loss: 0.011912456713616848\n",
      "Epoch: 5/8, Batch: 170/3444, Loss: 0.020586354658007622\n",
      "Epoch: 5/8, Batch: 180/3444, Loss: 0.033229514956474304\n",
      "Epoch: 5/8, Batch: 190/3444, Loss: 0.01713276468217373\n",
      "Epoch: 5/8, Batch: 200/3444, Loss: 0.010407700203359127\n",
      "Epoch: 5/8, Batch: 210/3444, Loss: 0.04174092039465904\n",
      "Epoch: 5/8, Batch: 220/3444, Loss: 0.010911686345934868\n",
      "Epoch: 5/8, Batch: 230/3444, Loss: 0.029922382906079292\n",
      "Epoch: 5/8, Batch: 240/3444, Loss: 0.016009973362088203\n",
      "Epoch: 5/8, Batch: 250/3444, Loss: 0.012406792491674423\n",
      "Epoch: 5/8, Batch: 260/3444, Loss: 0.0045317369513213634\n",
      "Epoch: 5/8, Batch: 270/3444, Loss: 0.03918395936489105\n",
      "Epoch: 5/8, Batch: 280/3444, Loss: 0.01140421349555254\n",
      "Epoch: 5/8, Batch: 290/3444, Loss: 0.0073435259982943535\n",
      "Epoch: 5/8, Batch: 300/3444, Loss: 0.019573139026761055\n",
      "Epoch: 5/8, Batch: 310/3444, Loss: 0.03055746667087078\n",
      "Epoch: 5/8, Batch: 320/3444, Loss: 0.017898885533213615\n",
      "Epoch: 5/8, Batch: 330/3444, Loss: 0.01103189866989851\n",
      "Epoch: 5/8, Batch: 340/3444, Loss: 0.02062174305319786\n",
      "Epoch: 5/8, Batch: 350/3444, Loss: 0.011172615922987461\n",
      "Epoch: 5/8, Batch: 360/3444, Loss: 0.015722401440143585\n",
      "Epoch: 5/8, Batch: 370/3444, Loss: 0.009288117289543152\n",
      "Epoch: 5/8, Batch: 380/3444, Loss: 0.016946125775575638\n",
      "Epoch: 5/8, Batch: 390/3444, Loss: 0.03485460206866264\n",
      "Epoch: 5/8, Batch: 400/3444, Loss: 0.013950087130069733\n",
      "Epoch: 5/8, Batch: 410/3444, Loss: 0.010877751745283604\n",
      "Epoch: 5/8, Batch: 420/3444, Loss: 0.004326191730797291\n",
      "Epoch: 5/8, Batch: 430/3444, Loss: 0.010503053665161133\n",
      "Epoch: 5/8, Batch: 440/3444, Loss: 0.008882223628461361\n",
      "Epoch: 5/8, Batch: 450/3444, Loss: 0.022791201248764992\n",
      "Epoch: 5/8, Batch: 460/3444, Loss: 0.026846466585993767\n",
      "Epoch: 5/8, Batch: 470/3444, Loss: 0.014861046336591244\n",
      "Epoch: 5/8, Batch: 480/3444, Loss: 0.014372477307915688\n",
      "Epoch: 5/8, Batch: 490/3444, Loss: 0.008330736309289932\n",
      "Epoch: 5/8, Batch: 500/3444, Loss: 0.01676216721534729\n",
      "Epoch: 5/8, Batch: 510/3444, Loss: 0.01726994849741459\n",
      "Epoch: 5/8, Batch: 520/3444, Loss: 0.012865028344094753\n",
      "Epoch: 5/8, Batch: 530/3444, Loss: 0.009005216881632805\n",
      "Epoch: 5/8, Batch: 540/3444, Loss: 0.01754830777645111\n",
      "Epoch: 5/8, Batch: 550/3444, Loss: 0.010254534892737865\n",
      "Epoch: 5/8, Batch: 560/3444, Loss: 0.007152920588850975\n",
      "Epoch: 5/8, Batch: 570/3444, Loss: 0.009394269436597824\n",
      "Epoch: 5/8, Batch: 580/3444, Loss: 0.017970971763134003\n",
      "Epoch: 5/8, Batch: 590/3444, Loss: 0.026703884825110435\n",
      "Epoch: 5/8, Batch: 600/3444, Loss: 0.010096932761371136\n",
      "Epoch: 5/8, Batch: 610/3444, Loss: 0.05630047619342804\n",
      "Epoch: 5/8, Batch: 620/3444, Loss: 0.009268179535865784\n",
      "Epoch: 5/8, Batch: 630/3444, Loss: 0.01649920828640461\n",
      "Epoch: 5/8, Batch: 640/3444, Loss: 0.00705742510035634\n",
      "Epoch: 5/8, Batch: 650/3444, Loss: 0.014928122982382774\n",
      "Epoch: 5/8, Batch: 660/3444, Loss: 0.01581949181854725\n",
      "Epoch: 5/8, Batch: 670/3444, Loss: 0.018189305439591408\n",
      "Epoch: 5/8, Batch: 680/3444, Loss: 0.02415822073817253\n",
      "Epoch: 5/8, Batch: 690/3444, Loss: 0.018617019057273865\n",
      "Epoch: 5/8, Batch: 700/3444, Loss: 0.010014799423515797\n",
      "Epoch: 5/8, Batch: 710/3444, Loss: 0.009592205286026001\n",
      "Epoch: 5/8, Batch: 720/3444, Loss: 0.030300432816147804\n",
      "Epoch: 5/8, Batch: 730/3444, Loss: 0.020481564104557037\n",
      "Epoch: 5/8, Batch: 740/3444, Loss: 0.010659401305019855\n",
      "Epoch: 5/8, Batch: 750/3444, Loss: 0.012872531078755856\n",
      "Epoch: 5/8, Batch: 760/3444, Loss: 0.03405888378620148\n",
      "Epoch: 5/8, Batch: 770/3444, Loss: 0.02611185610294342\n",
      "Epoch: 5/8, Batch: 780/3444, Loss: 0.013038583099842072\n",
      "Epoch: 5/8, Batch: 790/3444, Loss: 0.05233147367835045\n",
      "Epoch: 5/8, Batch: 800/3444, Loss: 0.010111362673342228\n",
      "Epoch: 5/8, Batch: 810/3444, Loss: 0.01590385101735592\n",
      "Epoch: 5/8, Batch: 820/3444, Loss: 0.06771614402532578\n",
      "Epoch: 5/8, Batch: 830/3444, Loss: 0.016460679471492767\n",
      "Epoch: 5/8, Batch: 840/3444, Loss: 0.005732537712901831\n",
      "Epoch: 5/8, Batch: 850/3444, Loss: 0.025839248672127724\n",
      "Epoch: 5/8, Batch: 860/3444, Loss: 0.006573934573680162\n",
      "Epoch: 5/8, Batch: 870/3444, Loss: 0.03673548623919487\n",
      "Epoch: 5/8, Batch: 880/3444, Loss: 0.014050859957933426\n",
      "Epoch: 5/8, Batch: 890/3444, Loss: 0.02788274735212326\n",
      "Epoch: 5/8, Batch: 900/3444, Loss: 0.010161735117435455\n",
      "Epoch: 5/8, Batch: 910/3444, Loss: 0.006036261562258005\n",
      "Epoch: 5/8, Batch: 920/3444, Loss: 0.008961094543337822\n",
      "Epoch: 5/8, Batch: 930/3444, Loss: 0.011486474424600601\n",
      "Epoch: 5/8, Batch: 940/3444, Loss: 0.05111606791615486\n",
      "Epoch: 5/8, Batch: 950/3444, Loss: 0.024262337014079094\n",
      "Epoch: 5/8, Batch: 960/3444, Loss: 0.011310797184705734\n",
      "Epoch: 5/8, Batch: 970/3444, Loss: 0.0072782812640070915\n",
      "Epoch: 5/8, Batch: 980/3444, Loss: 0.007577592507004738\n",
      "Epoch: 5/8, Batch: 990/3444, Loss: 0.007934854365885258\n",
      "Epoch: 5/8, Batch: 1000/3444, Loss: 0.03970446437597275\n",
      "Epoch: 5/8, Batch: 1010/3444, Loss: 0.010679719038307667\n",
      "Epoch: 5/8, Batch: 1020/3444, Loss: 0.00427546352148056\n",
      "Epoch: 5/8, Batch: 1030/3444, Loss: 0.004030401352792978\n",
      "Epoch: 5/8, Batch: 1040/3444, Loss: 0.011611662805080414\n",
      "Epoch: 5/8, Batch: 1050/3444, Loss: 0.013579198159277439\n",
      "Epoch: 5/8, Batch: 1060/3444, Loss: 0.007806433830410242\n",
      "Epoch: 5/8, Batch: 1070/3444, Loss: 0.005228744819760323\n",
      "Epoch: 5/8, Batch: 1080/3444, Loss: 0.02071325294673443\n",
      "Epoch: 5/8, Batch: 1090/3444, Loss: 0.007924458011984825\n",
      "Epoch: 5/8, Batch: 1100/3444, Loss: 0.011773060075938702\n",
      "Epoch: 5/8, Batch: 1110/3444, Loss: 0.020764797925949097\n",
      "Epoch: 5/8, Batch: 1120/3444, Loss: 0.012337691150605679\n",
      "Epoch: 5/8, Batch: 1130/3444, Loss: 0.012495120987296104\n",
      "Epoch: 5/8, Batch: 1140/3444, Loss: 0.013736327178776264\n",
      "Epoch: 5/8, Batch: 1150/3444, Loss: 0.005689012818038464\n",
      "Epoch: 5/8, Batch: 1160/3444, Loss: 0.00978230033069849\n",
      "Epoch: 5/8, Batch: 1170/3444, Loss: 0.012696071527898312\n",
      "Epoch: 5/8, Batch: 1180/3444, Loss: 0.019473034888505936\n",
      "Epoch: 5/8, Batch: 1190/3444, Loss: 0.02301006019115448\n",
      "Epoch: 5/8, Batch: 1200/3444, Loss: 0.0070242867805063725\n",
      "Epoch: 5/8, Batch: 1210/3444, Loss: 0.008488155901432037\n",
      "Epoch: 5/8, Batch: 1220/3444, Loss: 0.019430793821811676\n",
      "Epoch: 5/8, Batch: 1230/3444, Loss: 0.019070783630013466\n",
      "Epoch: 5/8, Batch: 1240/3444, Loss: 0.018349003046751022\n",
      "Epoch: 5/8, Batch: 1250/3444, Loss: 0.015259143896400928\n",
      "Epoch: 5/8, Batch: 1260/3444, Loss: 0.0037334486842155457\n",
      "Epoch: 5/8, Batch: 1270/3444, Loss: 0.016183748841285706\n",
      "Epoch: 5/8, Batch: 1280/3444, Loss: 0.022313501685857773\n",
      "Epoch: 5/8, Batch: 1290/3444, Loss: 0.016526855528354645\n",
      "Epoch: 5/8, Batch: 1300/3444, Loss: 0.009500658139586449\n",
      "Epoch: 5/8, Batch: 1310/3444, Loss: 0.006396958604454994\n",
      "Epoch: 5/8, Batch: 1320/3444, Loss: 0.016423586755990982\n",
      "Epoch: 5/8, Batch: 1330/3444, Loss: 0.025384007021784782\n",
      "Epoch: 5/8, Batch: 1340/3444, Loss: 0.0316191092133522\n",
      "Epoch: 5/8, Batch: 1350/3444, Loss: 0.0046151368878781796\n",
      "Epoch: 5/8, Batch: 1360/3444, Loss: 0.010169759392738342\n",
      "Epoch: 5/8, Batch: 1370/3444, Loss: 0.00775108253583312\n",
      "Epoch: 5/8, Batch: 1380/3444, Loss: 0.025999177247285843\n",
      "Epoch: 5/8, Batch: 1390/3444, Loss: 0.013845380395650864\n",
      "Epoch: 5/8, Batch: 1400/3444, Loss: 0.00637697521597147\n",
      "Epoch: 5/8, Batch: 1410/3444, Loss: 0.024585364386439323\n",
      "Epoch: 5/8, Batch: 1420/3444, Loss: 0.008917189203202724\n",
      "Epoch: 5/8, Batch: 1430/3444, Loss: 0.01778908260166645\n",
      "Epoch: 5/8, Batch: 1440/3444, Loss: 0.005938561167567968\n",
      "Epoch: 5/8, Batch: 1450/3444, Loss: 0.007496618200093508\n",
      "Epoch: 5/8, Batch: 1460/3444, Loss: 0.016787782311439514\n",
      "Epoch: 5/8, Batch: 1470/3444, Loss: 0.0128122977912426\n",
      "Epoch: 5/8, Batch: 1480/3444, Loss: 0.01949395425617695\n",
      "Epoch: 5/8, Batch: 1490/3444, Loss: 0.025422830134630203\n",
      "Epoch: 5/8, Batch: 1500/3444, Loss: 0.024260040372610092\n",
      "Epoch: 5/8, Batch: 1510/3444, Loss: 0.013587553054094315\n",
      "Epoch: 5/8, Batch: 1520/3444, Loss: 0.010019428096711636\n",
      "Epoch: 5/8, Batch: 1530/3444, Loss: 0.010171550326049328\n",
      "Epoch: 5/8, Batch: 1540/3444, Loss: 0.01742302067577839\n",
      "Epoch: 5/8, Batch: 1550/3444, Loss: 0.02106413245201111\n",
      "Epoch: 5/8, Batch: 1560/3444, Loss: 0.008010882884263992\n",
      "Epoch: 5/8, Batch: 1570/3444, Loss: 0.00485195592045784\n",
      "Epoch: 5/8, Batch: 1580/3444, Loss: 0.0463092103600502\n",
      "Epoch: 5/8, Batch: 1590/3444, Loss: 0.02478761039674282\n",
      "Epoch: 5/8, Batch: 1600/3444, Loss: 0.024904709309339523\n",
      "Epoch: 5/8, Batch: 1610/3444, Loss: 0.014886143617331982\n",
      "Epoch: 5/8, Batch: 1620/3444, Loss: 0.021849997341632843\n",
      "Epoch: 5/8, Batch: 1630/3444, Loss: 0.00696543836966157\n",
      "Epoch: 5/8, Batch: 1640/3444, Loss: 0.009650666266679764\n",
      "Epoch: 5/8, Batch: 1650/3444, Loss: 0.01849067211151123\n",
      "Epoch: 5/8, Batch: 1660/3444, Loss: 0.011138761416077614\n",
      "Epoch: 5/8, Batch: 1670/3444, Loss: 0.011720170266926289\n",
      "Epoch: 5/8, Batch: 1680/3444, Loss: 0.013015365228056908\n",
      "Epoch: 5/8, Batch: 1690/3444, Loss: 0.007569851819425821\n",
      "Epoch: 5/8, Batch: 1700/3444, Loss: 0.00745044881477952\n",
      "Epoch: 5/8, Batch: 1710/3444, Loss: 0.02576436661183834\n",
      "Epoch: 5/8, Batch: 1720/3444, Loss: 0.019370514899492264\n",
      "Epoch: 5/8, Batch: 1730/3444, Loss: 0.007376766297966242\n",
      "Epoch: 5/8, Batch: 1740/3444, Loss: 0.01307071652263403\n",
      "Epoch: 5/8, Batch: 1750/3444, Loss: 0.02051660604774952\n",
      "Epoch: 5/8, Batch: 1760/3444, Loss: 0.009650127962231636\n",
      "Epoch: 5/8, Batch: 1770/3444, Loss: 0.014279714785516262\n",
      "Epoch: 5/8, Batch: 1780/3444, Loss: 0.022601893171668053\n",
      "Epoch: 5/8, Batch: 1790/3444, Loss: 0.016262123361229897\n",
      "Epoch: 5/8, Batch: 1800/3444, Loss: 0.015207445248961449\n",
      "Epoch: 5/8, Batch: 1810/3444, Loss: 0.0069712004624307156\n",
      "Epoch: 5/8, Batch: 1820/3444, Loss: 0.007851666770875454\n",
      "Epoch: 5/8, Batch: 1830/3444, Loss: 0.008096514269709587\n",
      "Epoch: 5/8, Batch: 1840/3444, Loss: 0.011080913245677948\n",
      "Epoch: 5/8, Batch: 1850/3444, Loss: 0.006495789624750614\n",
      "Epoch: 5/8, Batch: 1860/3444, Loss: 0.01565859280526638\n",
      "Epoch: 5/8, Batch: 1870/3444, Loss: 0.011661726050078869\n",
      "Epoch: 5/8, Batch: 1880/3444, Loss: 0.02192174270749092\n",
      "Epoch: 5/8, Batch: 1890/3444, Loss: 0.017977165058255196\n",
      "Epoch: 5/8, Batch: 1900/3444, Loss: 0.01227402500808239\n",
      "Epoch: 5/8, Batch: 1910/3444, Loss: 0.037167105823755264\n",
      "Epoch: 5/8, Batch: 1920/3444, Loss: 0.021532582119107246\n",
      "Epoch: 5/8, Batch: 1930/3444, Loss: 0.01416333019733429\n",
      "Epoch: 5/8, Batch: 1940/3444, Loss: 0.009183957241475582\n",
      "Epoch: 5/8, Batch: 1950/3444, Loss: 0.02825445868074894\n",
      "Epoch: 5/8, Batch: 1960/3444, Loss: 0.009701026603579521\n",
      "Epoch: 5/8, Batch: 1970/3444, Loss: 0.005689636804163456\n",
      "Epoch: 5/8, Batch: 1980/3444, Loss: 0.0071523841470479965\n",
      "Epoch: 5/8, Batch: 1990/3444, Loss: 0.0169850904494524\n",
      "Epoch: 5/8, Batch: 2000/3444, Loss: 0.018398651853203773\n",
      "Epoch: 5/8, Batch: 2010/3444, Loss: 0.007040824741125107\n",
      "Epoch: 5/8, Batch: 2020/3444, Loss: 0.009912043809890747\n",
      "Epoch: 5/8, Batch: 2030/3444, Loss: 0.00931490771472454\n",
      "Epoch: 5/8, Batch: 2040/3444, Loss: 0.032723695039749146\n",
      "Epoch: 5/8, Batch: 2050/3444, Loss: 0.026380717754364014\n",
      "Epoch: 5/8, Batch: 2060/3444, Loss: 0.010805469006299973\n",
      "Epoch: 5/8, Batch: 2070/3444, Loss: 0.0200276430696249\n",
      "Epoch: 5/8, Batch: 2080/3444, Loss: 0.019008157774806023\n",
      "Epoch: 5/8, Batch: 2090/3444, Loss: 0.023392878472805023\n",
      "Epoch: 5/8, Batch: 2100/3444, Loss: 0.010383250191807747\n",
      "Epoch: 5/8, Batch: 2110/3444, Loss: 0.022240271791815758\n",
      "Epoch: 5/8, Batch: 2120/3444, Loss: 0.009344499558210373\n",
      "Epoch: 5/8, Batch: 2130/3444, Loss: 0.00851397030055523\n",
      "Epoch: 5/8, Batch: 2140/3444, Loss: 0.006283661350607872\n",
      "Epoch: 5/8, Batch: 2150/3444, Loss: 0.02122180536389351\n",
      "Epoch: 5/8, Batch: 2160/3444, Loss: 0.02824808843433857\n",
      "Epoch: 5/8, Batch: 2170/3444, Loss: 0.014424130320549011\n",
      "Epoch: 5/8, Batch: 2180/3444, Loss: 0.011794280260801315\n",
      "Epoch: 5/8, Batch: 2190/3444, Loss: 0.024852154776453972\n",
      "Epoch: 5/8, Batch: 2200/3444, Loss: 0.0050486731342971325\n",
      "Epoch: 5/8, Batch: 2210/3444, Loss: 0.035318195819854736\n",
      "Epoch: 5/8, Batch: 2220/3444, Loss: 0.016006268560886383\n",
      "Epoch: 5/8, Batch: 2230/3444, Loss: 0.007662714459002018\n",
      "Epoch: 5/8, Batch: 2240/3444, Loss: 0.01903955452144146\n",
      "Epoch: 5/8, Batch: 2250/3444, Loss: 0.021929703652858734\n",
      "Epoch: 5/8, Batch: 2260/3444, Loss: 0.014434250071644783\n",
      "Epoch: 5/8, Batch: 2270/3444, Loss: 0.006666906643658876\n",
      "Epoch: 5/8, Batch: 2280/3444, Loss: 0.011625254526734352\n",
      "Epoch: 5/8, Batch: 2290/3444, Loss: 0.015897778794169426\n",
      "Epoch: 5/8, Batch: 2300/3444, Loss: 0.01554846577346325\n",
      "Epoch: 5/8, Batch: 2310/3444, Loss: 0.010096335783600807\n",
      "Epoch: 5/8, Batch: 2320/3444, Loss: 0.013162474147975445\n",
      "Epoch: 5/8, Batch: 2330/3444, Loss: 0.02431507594883442\n",
      "Epoch: 5/8, Batch: 2340/3444, Loss: 0.01033166702836752\n",
      "Epoch: 5/8, Batch: 2350/3444, Loss: 0.025505254045128822\n",
      "Epoch: 5/8, Batch: 2360/3444, Loss: 0.015552455559372902\n",
      "Epoch: 5/8, Batch: 2370/3444, Loss: 0.010607519187033176\n",
      "Epoch: 5/8, Batch: 2380/3444, Loss: 0.0191622544080019\n",
      "Epoch: 5/8, Batch: 2390/3444, Loss: 0.01309549156576395\n",
      "Epoch: 5/8, Batch: 2400/3444, Loss: 0.004984687548130751\n",
      "Epoch: 5/8, Batch: 2410/3444, Loss: 0.00870656967163086\n",
      "Epoch: 5/8, Batch: 2420/3444, Loss: 0.01265746634453535\n",
      "Epoch: 5/8, Batch: 2430/3444, Loss: 0.008179396390914917\n",
      "Epoch: 5/8, Batch: 2440/3444, Loss: 0.004150640219449997\n",
      "Epoch: 5/8, Batch: 2450/3444, Loss: 0.012075608596205711\n",
      "Epoch: 5/8, Batch: 2460/3444, Loss: 0.024817828088998795\n",
      "Epoch: 5/8, Batch: 2470/3444, Loss: 0.013784365728497505\n",
      "Epoch: 5/8, Batch: 2480/3444, Loss: 0.027809282764792442\n",
      "Epoch: 5/8, Batch: 2490/3444, Loss: 0.012636007741093636\n",
      "Epoch: 5/8, Batch: 2500/3444, Loss: 0.004071615636348724\n",
      "Epoch: 5/8, Batch: 2510/3444, Loss: 0.019507402554154396\n",
      "Epoch: 5/8, Batch: 2520/3444, Loss: 0.10886285454034805\n",
      "Epoch: 5/8, Batch: 2530/3444, Loss: 0.04519996792078018\n",
      "Epoch: 5/8, Batch: 2540/3444, Loss: 0.008881228044629097\n",
      "Epoch: 5/8, Batch: 2550/3444, Loss: 0.006166033446788788\n",
      "Epoch: 5/8, Batch: 2560/3444, Loss: 0.009598057717084885\n",
      "Epoch: 5/8, Batch: 2570/3444, Loss: 0.012527845799922943\n",
      "Epoch: 5/8, Batch: 2580/3444, Loss: 0.004405517131090164\n",
      "Epoch: 5/8, Batch: 2590/3444, Loss: 0.02638518251478672\n",
      "Epoch: 5/8, Batch: 2600/3444, Loss: 0.018052754923701286\n",
      "Epoch: 5/8, Batch: 2610/3444, Loss: 0.010719042271375656\n",
      "Epoch: 5/8, Batch: 2620/3444, Loss: 0.01683172583580017\n",
      "Epoch: 5/8, Batch: 2630/3444, Loss: 0.019279707223176956\n",
      "Epoch: 5/8, Batch: 2640/3444, Loss: 0.017962541431188583\n",
      "Epoch: 5/8, Batch: 2650/3444, Loss: 0.01072580274194479\n",
      "Epoch: 5/8, Batch: 2660/3444, Loss: 0.05266827717423439\n",
      "Epoch: 5/8, Batch: 2670/3444, Loss: 0.009450133889913559\n",
      "Epoch: 5/8, Batch: 2680/3444, Loss: 0.01762763410806656\n",
      "Epoch: 5/8, Batch: 2690/3444, Loss: 0.020744306966662407\n",
      "Epoch: 5/8, Batch: 2700/3444, Loss: 0.02933034859597683\n",
      "Epoch: 5/8, Batch: 2710/3444, Loss: 0.017983876168727875\n",
      "Epoch: 5/8, Batch: 2720/3444, Loss: 0.00981967244297266\n",
      "Epoch: 5/8, Batch: 2730/3444, Loss: 0.011904692277312279\n",
      "Epoch: 5/8, Batch: 2740/3444, Loss: 0.011527090333402157\n",
      "Epoch: 5/8, Batch: 2750/3444, Loss: 0.008573442697525024\n",
      "Epoch: 5/8, Batch: 2760/3444, Loss: 0.011407907120883465\n",
      "Epoch: 5/8, Batch: 2770/3444, Loss: 0.03395805507898331\n",
      "Epoch: 5/8, Batch: 2780/3444, Loss: 0.008112156763672829\n",
      "Epoch: 5/8, Batch: 2790/3444, Loss: 0.009071818552911282\n",
      "Epoch: 5/8, Batch: 2800/3444, Loss: 0.02570904977619648\n",
      "Epoch: 5/8, Batch: 2810/3444, Loss: 0.023736583068966866\n",
      "Epoch: 5/8, Batch: 2820/3444, Loss: 0.01293159555643797\n",
      "Epoch: 5/8, Batch: 2830/3444, Loss: 0.0067784469574689865\n",
      "Epoch: 5/8, Batch: 2840/3444, Loss: 0.00810920912772417\n",
      "Epoch: 5/8, Batch: 2850/3444, Loss: 0.0067068743519485\n",
      "Epoch: 5/8, Batch: 2860/3444, Loss: 0.007620918098837137\n",
      "Epoch: 5/8, Batch: 2870/3444, Loss: 0.02983204834163189\n",
      "Epoch: 5/8, Batch: 2880/3444, Loss: 0.008191920816898346\n",
      "Epoch: 5/8, Batch: 2890/3444, Loss: 0.018042759969830513\n",
      "Epoch: 5/8, Batch: 2900/3444, Loss: 0.010006027296185493\n",
      "Epoch: 5/8, Batch: 2910/3444, Loss: 0.008622962981462479\n",
      "Epoch: 5/8, Batch: 2920/3444, Loss: 0.011081513948738575\n",
      "Epoch: 5/8, Batch: 2930/3444, Loss: 0.006858734879642725\n",
      "Epoch: 5/8, Batch: 2940/3444, Loss: 0.021598907187581062\n",
      "Epoch: 5/8, Batch: 2950/3444, Loss: 0.040992703288793564\n",
      "Epoch: 5/8, Batch: 2960/3444, Loss: 0.007970497943460941\n",
      "Epoch: 5/8, Batch: 2970/3444, Loss: 0.03170933946967125\n",
      "Epoch: 5/8, Batch: 2980/3444, Loss: 0.006849103607237339\n",
      "Epoch: 5/8, Batch: 2990/3444, Loss: 0.015637123957276344\n",
      "Epoch: 5/8, Batch: 3000/3444, Loss: 0.012482380494475365\n",
      "Epoch: 5/8, Batch: 3010/3444, Loss: 0.012120090425014496\n",
      "Epoch: 5/8, Batch: 3020/3444, Loss: 0.03862075135111809\n",
      "Epoch: 5/8, Batch: 3030/3444, Loss: 0.015196148306131363\n",
      "Epoch: 5/8, Batch: 3040/3444, Loss: 0.019417736679315567\n",
      "Epoch: 5/8, Batch: 3050/3444, Loss: 0.014992988668382168\n",
      "Epoch: 5/8, Batch: 3060/3444, Loss: 0.013234592974185944\n",
      "Epoch: 5/8, Batch: 3070/3444, Loss: 0.03265383094549179\n",
      "Epoch: 5/8, Batch: 3080/3444, Loss: 0.03057580068707466\n",
      "Epoch: 5/8, Batch: 3090/3444, Loss: 0.02330072410404682\n",
      "Epoch: 5/8, Batch: 3100/3444, Loss: 0.012712863273918629\n",
      "Epoch: 5/8, Batch: 3110/3444, Loss: 0.028336521238088608\n",
      "Epoch: 5/8, Batch: 3120/3444, Loss: 0.011538096703588963\n",
      "Epoch: 5/8, Batch: 3130/3444, Loss: 0.01615113392472267\n",
      "Epoch: 5/8, Batch: 3140/3444, Loss: 0.01618681661784649\n",
      "Epoch: 5/8, Batch: 3150/3444, Loss: 0.011107292957603931\n",
      "Epoch: 5/8, Batch: 3160/3444, Loss: 0.019620131701231003\n",
      "Epoch: 5/8, Batch: 3170/3444, Loss: 0.01746288873255253\n",
      "Epoch: 5/8, Batch: 3180/3444, Loss: 0.023250538855791092\n",
      "Epoch: 5/8, Batch: 3190/3444, Loss: 0.025803375989198685\n",
      "Epoch: 5/8, Batch: 3200/3444, Loss: 0.005514041520655155\n",
      "Epoch: 5/8, Batch: 3210/3444, Loss: 0.007788824383169413\n",
      "Epoch: 5/8, Batch: 3220/3444, Loss: 0.038470156490802765\n",
      "Epoch: 5/8, Batch: 3230/3444, Loss: 0.017968405038118362\n",
      "Epoch: 5/8, Batch: 3240/3444, Loss: 0.012454349547624588\n",
      "Epoch: 5/8, Batch: 3250/3444, Loss: 0.01199503056704998\n",
      "Epoch: 5/8, Batch: 3260/3444, Loss: 0.01025653537362814\n",
      "Epoch: 5/8, Batch: 3270/3444, Loss: 0.030389467254281044\n",
      "Epoch: 5/8, Batch: 3280/3444, Loss: 0.02274801768362522\n",
      "Epoch: 5/8, Batch: 3290/3444, Loss: 0.02188807725906372\n",
      "Epoch: 5/8, Batch: 3300/3444, Loss: 0.012052907608449459\n",
      "Epoch: 5/8, Batch: 3310/3444, Loss: 0.04472649097442627\n",
      "Epoch: 5/8, Batch: 3320/3444, Loss: 0.005885033402591944\n",
      "Epoch: 5/8, Batch: 3330/3444, Loss: 0.015531711280345917\n",
      "Epoch: 5/8, Batch: 3340/3444, Loss: 0.007014556787908077\n",
      "Epoch: 5/8, Batch: 3350/3444, Loss: 0.018340175971388817\n",
      "Epoch: 5/8, Batch: 3360/3444, Loss: 0.014580002054572105\n",
      "Epoch: 5/8, Batch: 3370/3444, Loss: 0.012799802236258984\n",
      "Epoch: 5/8, Batch: 3380/3444, Loss: 0.036341678351163864\n",
      "Epoch: 5/8, Batch: 3390/3444, Loss: 0.03916320577263832\n",
      "Epoch: 5/8, Batch: 3400/3444, Loss: 0.019435681402683258\n",
      "Epoch: 5/8, Batch: 3410/3444, Loss: 0.007649356499314308\n",
      "Epoch: 5/8, Batch: 3420/3444, Loss: 0.007050900254398584\n",
      "Epoch: 5/8, Batch: 3430/3444, Loss: 0.011566863395273685\n",
      "Epoch: 5/8, Batch: 3440/3444, Loss: 0.013520326465368271\n",
      "Epoch: 5/8, Val Loss: 0.03016573493990887\n",
      "Epoch: 6/8, Batch: 10/3444, Loss: 0.02252843603491783\n",
      "Epoch: 6/8, Batch: 20/3444, Loss: 0.03433854132890701\n",
      "Epoch: 6/8, Batch: 30/3444, Loss: 0.008395436219871044\n",
      "Epoch: 6/8, Batch: 40/3444, Loss: 0.006654149852693081\n",
      "Epoch: 6/8, Batch: 50/3444, Loss: 0.012534108012914658\n",
      "Epoch: 6/8, Batch: 60/3444, Loss: 0.013618909753859043\n",
      "Epoch: 6/8, Batch: 70/3444, Loss: 0.011234388686716557\n",
      "Epoch: 6/8, Batch: 80/3444, Loss: 0.0070302486419677734\n",
      "Epoch: 6/8, Batch: 90/3444, Loss: 0.013240319676697254\n",
      "Epoch: 6/8, Batch: 100/3444, Loss: 0.007706714328378439\n",
      "Epoch: 6/8, Batch: 110/3444, Loss: 0.008576028048992157\n",
      "Epoch: 6/8, Batch: 120/3444, Loss: 0.011249199509620667\n",
      "Epoch: 6/8, Batch: 130/3444, Loss: 0.01004895381629467\n",
      "Epoch: 6/8, Batch: 140/3444, Loss: 0.008659753017127514\n",
      "Epoch: 6/8, Batch: 150/3444, Loss: 0.017295217141509056\n",
      "Epoch: 6/8, Batch: 160/3444, Loss: 0.014412349089980125\n",
      "Epoch: 6/8, Batch: 170/3444, Loss: 0.016405610367655754\n",
      "Epoch: 6/8, Batch: 180/3444, Loss: 0.02567860297858715\n",
      "Epoch: 6/8, Batch: 190/3444, Loss: 0.009705462493002415\n",
      "Epoch: 6/8, Batch: 200/3444, Loss: 0.015877852216362953\n",
      "Epoch: 6/8, Batch: 210/3444, Loss: 0.015273476019501686\n",
      "Epoch: 6/8, Batch: 220/3444, Loss: 0.01787959784269333\n",
      "Epoch: 6/8, Batch: 230/3444, Loss: 0.02513877861201763\n",
      "Epoch: 6/8, Batch: 240/3444, Loss: 0.021440288051962852\n",
      "Epoch: 6/8, Batch: 250/3444, Loss: 0.009610067121684551\n",
      "Epoch: 6/8, Batch: 260/3444, Loss: 0.023660697042942047\n",
      "Epoch: 6/8, Batch: 270/3444, Loss: 0.0076452516950666904\n",
      "Epoch: 6/8, Batch: 280/3444, Loss: 0.009465235285460949\n",
      "Epoch: 6/8, Batch: 290/3444, Loss: 0.005978841334581375\n",
      "Epoch: 6/8, Batch: 300/3444, Loss: 0.012802891433238983\n",
      "Epoch: 6/8, Batch: 310/3444, Loss: 0.00788928847759962\n",
      "Epoch: 6/8, Batch: 320/3444, Loss: 0.0032987503800541162\n",
      "Epoch: 6/8, Batch: 330/3444, Loss: 0.008287004195153713\n",
      "Epoch: 6/8, Batch: 340/3444, Loss: 0.023119401186704636\n",
      "Epoch: 6/8, Batch: 350/3444, Loss: 0.014093227684497833\n",
      "Epoch: 6/8, Batch: 360/3444, Loss: 0.016216648742556572\n",
      "Epoch: 6/8, Batch: 370/3444, Loss: 0.022389667108654976\n",
      "Epoch: 6/8, Batch: 380/3444, Loss: 0.004671374335885048\n",
      "Epoch: 6/8, Batch: 390/3444, Loss: 0.010502604767680168\n",
      "Epoch: 6/8, Batch: 400/3444, Loss: 0.007342267781496048\n",
      "Epoch: 6/8, Batch: 410/3444, Loss: 0.009729797020554543\n",
      "Epoch: 6/8, Batch: 420/3444, Loss: 0.02061663754284382\n",
      "Epoch: 6/8, Batch: 430/3444, Loss: 0.014878079295158386\n",
      "Epoch: 6/8, Batch: 440/3444, Loss: 0.014766105450689793\n",
      "Epoch: 6/8, Batch: 450/3444, Loss: 0.012506232596933842\n",
      "Epoch: 6/8, Batch: 460/3444, Loss: 0.004555793013423681\n",
      "Epoch: 6/8, Batch: 470/3444, Loss: 0.003908497281372547\n",
      "Epoch: 6/8, Batch: 480/3444, Loss: 0.015545961447060108\n",
      "Epoch: 6/8, Batch: 490/3444, Loss: 0.0210808627307415\n",
      "Epoch: 6/8, Batch: 500/3444, Loss: 0.008232885971665382\n",
      "Epoch: 6/8, Batch: 510/3444, Loss: 0.014043981209397316\n",
      "Epoch: 6/8, Batch: 520/3444, Loss: 0.010765905492007732\n",
      "Epoch: 6/8, Batch: 530/3444, Loss: 0.018201403319835663\n",
      "Epoch: 6/8, Batch: 540/3444, Loss: 0.008767753839492798\n",
      "Epoch: 6/8, Batch: 550/3444, Loss: 0.025880863890051842\n",
      "Epoch: 6/8, Batch: 560/3444, Loss: 0.007837356999516487\n",
      "Epoch: 6/8, Batch: 570/3444, Loss: 0.015402423217892647\n",
      "Epoch: 6/8, Batch: 580/3444, Loss: 0.016978008672595024\n",
      "Epoch: 6/8, Batch: 590/3444, Loss: 0.020110374316573143\n",
      "Epoch: 6/8, Batch: 600/3444, Loss: 0.008091785944998264\n",
      "Epoch: 6/8, Batch: 610/3444, Loss: 0.027845939621329308\n",
      "Epoch: 6/8, Batch: 620/3444, Loss: 0.010591438040137291\n",
      "Epoch: 6/8, Batch: 630/3444, Loss: 0.009513786062598228\n",
      "Epoch: 6/8, Batch: 640/3444, Loss: 0.008459306322038174\n",
      "Epoch: 6/8, Batch: 650/3444, Loss: 0.01650204323232174\n",
      "Epoch: 6/8, Batch: 660/3444, Loss: 0.00786922313272953\n",
      "Epoch: 6/8, Batch: 670/3444, Loss: 0.023687880486249924\n",
      "Epoch: 6/8, Batch: 680/3444, Loss: 0.009646981954574585\n",
      "Epoch: 6/8, Batch: 690/3444, Loss: 0.01332548726350069\n",
      "Epoch: 6/8, Batch: 700/3444, Loss: 0.027462631464004517\n",
      "Epoch: 6/8, Batch: 710/3444, Loss: 0.009565367363393307\n",
      "Epoch: 6/8, Batch: 720/3444, Loss: 0.008688787929713726\n",
      "Epoch: 6/8, Batch: 730/3444, Loss: 0.008763348683714867\n",
      "Epoch: 6/8, Batch: 740/3444, Loss: 0.017404694110155106\n",
      "Epoch: 6/8, Batch: 750/3444, Loss: 0.0073439329862594604\n",
      "Epoch: 6/8, Batch: 760/3444, Loss: 0.00901508517563343\n",
      "Epoch: 6/8, Batch: 770/3444, Loss: 0.015879027545452118\n",
      "Epoch: 6/8, Batch: 780/3444, Loss: 0.01612992212176323\n",
      "Epoch: 6/8, Batch: 790/3444, Loss: 0.014454113319516182\n",
      "Epoch: 6/8, Batch: 800/3444, Loss: 0.015722114592790604\n",
      "Epoch: 6/8, Batch: 810/3444, Loss: 0.018702981993556023\n",
      "Epoch: 6/8, Batch: 820/3444, Loss: 0.01212181057780981\n",
      "Epoch: 6/8, Batch: 830/3444, Loss: 0.013288707472383976\n",
      "Epoch: 6/8, Batch: 840/3444, Loss: 0.03571281209588051\n",
      "Epoch: 6/8, Batch: 850/3444, Loss: 0.011003918945789337\n",
      "Epoch: 6/8, Batch: 860/3444, Loss: 0.015162693336606026\n",
      "Epoch: 6/8, Batch: 870/3444, Loss: 0.014365221373736858\n",
      "Epoch: 6/8, Batch: 880/3444, Loss: 0.018948843702673912\n",
      "Epoch: 6/8, Batch: 890/3444, Loss: 0.023305188864469528\n",
      "Epoch: 6/8, Batch: 900/3444, Loss: 0.025293434038758278\n",
      "Epoch: 6/8, Batch: 910/3444, Loss: 0.015000291168689728\n",
      "Epoch: 6/8, Batch: 920/3444, Loss: 0.008607481606304646\n",
      "Epoch: 6/8, Batch: 930/3444, Loss: 0.020449472591280937\n",
      "Epoch: 6/8, Batch: 940/3444, Loss: 0.008273436687886715\n",
      "Epoch: 6/8, Batch: 950/3444, Loss: 0.03467552363872528\n",
      "Epoch: 6/8, Batch: 960/3444, Loss: 0.017114050686359406\n",
      "Epoch: 6/8, Batch: 970/3444, Loss: 0.014462041668593884\n",
      "Epoch: 6/8, Batch: 980/3444, Loss: 0.028673509135842323\n",
      "Epoch: 6/8, Batch: 990/3444, Loss: 0.015390019863843918\n",
      "Epoch: 6/8, Batch: 1000/3444, Loss: 0.010934890247881413\n",
      "Epoch: 6/8, Batch: 1010/3444, Loss: 0.025092707946896553\n",
      "Epoch: 6/8, Batch: 1020/3444, Loss: 0.016969945281744003\n",
      "Epoch: 6/8, Batch: 1030/3444, Loss: 0.013248488306999207\n",
      "Epoch: 6/8, Batch: 1040/3444, Loss: 0.019297998398542404\n",
      "Epoch: 6/8, Batch: 1050/3444, Loss: 0.04484671726822853\n",
      "Epoch: 6/8, Batch: 1060/3444, Loss: 0.01265035755932331\n",
      "Epoch: 6/8, Batch: 1070/3444, Loss: 0.010067851282656193\n",
      "Epoch: 6/8, Batch: 1080/3444, Loss: 0.018235765397548676\n",
      "Epoch: 6/8, Batch: 1090/3444, Loss: 0.009535676799714565\n",
      "Epoch: 6/8, Batch: 1100/3444, Loss: 0.02190023474395275\n",
      "Epoch: 6/8, Batch: 1110/3444, Loss: 0.0278181079775095\n",
      "Epoch: 6/8, Batch: 1120/3444, Loss: 0.016838708892464638\n",
      "Epoch: 6/8, Batch: 1130/3444, Loss: 0.018485965207219124\n",
      "Epoch: 6/8, Batch: 1140/3444, Loss: 0.01639391854405403\n",
      "Epoch: 6/8, Batch: 1150/3444, Loss: 0.012234245426952839\n",
      "Epoch: 6/8, Batch: 1160/3444, Loss: 0.010570883750915527\n",
      "Epoch: 6/8, Batch: 1170/3444, Loss: 0.009102814830839634\n",
      "Epoch: 6/8, Batch: 1180/3444, Loss: 0.019547171890735626\n",
      "Epoch: 6/8, Batch: 1190/3444, Loss: 0.009316452778875828\n",
      "Epoch: 6/8, Batch: 1200/3444, Loss: 0.005755386780947447\n",
      "Epoch: 6/8, Batch: 1210/3444, Loss: 0.021748626604676247\n",
      "Epoch: 6/8, Batch: 1220/3444, Loss: 0.007705305237323046\n",
      "Epoch: 6/8, Batch: 1230/3444, Loss: 0.011148198507726192\n",
      "Epoch: 6/8, Batch: 1240/3444, Loss: 0.04271985962986946\n",
      "Epoch: 6/8, Batch: 1250/3444, Loss: 0.018056921660900116\n",
      "Epoch: 6/8, Batch: 1260/3444, Loss: 0.005603729281574488\n",
      "Epoch: 6/8, Batch: 1270/3444, Loss: 0.006789183709770441\n",
      "Epoch: 6/8, Batch: 1280/3444, Loss: 0.006419702433049679\n",
      "Epoch: 6/8, Batch: 1290/3444, Loss: 0.005520715843886137\n",
      "Epoch: 6/8, Batch: 1300/3444, Loss: 0.02202463150024414\n",
      "Epoch: 6/8, Batch: 1310/3444, Loss: 0.01987195573747158\n",
      "Epoch: 6/8, Batch: 1320/3444, Loss: 0.008405791595578194\n",
      "Epoch: 6/8, Batch: 1330/3444, Loss: 0.008424529805779457\n",
      "Epoch: 6/8, Batch: 1340/3444, Loss: 0.01871279813349247\n",
      "Epoch: 6/8, Batch: 1350/3444, Loss: 0.008683711290359497\n",
      "Epoch: 6/8, Batch: 1360/3444, Loss: 0.008724650368094444\n",
      "Epoch: 6/8, Batch: 1370/3444, Loss: 0.022927386686205864\n",
      "Epoch: 6/8, Batch: 1380/3444, Loss: 0.007422889117151499\n",
      "Epoch: 6/8, Batch: 1390/3444, Loss: 0.01698591746389866\n",
      "Epoch: 6/8, Batch: 1400/3444, Loss: 0.030958550050854683\n",
      "Epoch: 6/8, Batch: 1410/3444, Loss: 0.013436838984489441\n",
      "Epoch: 6/8, Batch: 1420/3444, Loss: 0.005382463335990906\n",
      "Epoch: 6/8, Batch: 1430/3444, Loss: 0.011414498090744019\n",
      "Epoch: 6/8, Batch: 1440/3444, Loss: 0.008552742190659046\n",
      "Epoch: 6/8, Batch: 1450/3444, Loss: 0.01129191741347313\n",
      "Epoch: 6/8, Batch: 1460/3444, Loss: 0.01559311244636774\n",
      "Epoch: 6/8, Batch: 1470/3444, Loss: 0.012283164076507092\n",
      "Epoch: 6/8, Batch: 1480/3444, Loss: 0.03199220448732376\n",
      "Epoch: 6/8, Batch: 1490/3444, Loss: 0.005073626060038805\n",
      "Epoch: 6/8, Batch: 1500/3444, Loss: 0.014717578887939453\n",
      "Epoch: 6/8, Batch: 1510/3444, Loss: 0.026380738243460655\n",
      "Epoch: 6/8, Batch: 1520/3444, Loss: 0.014647380448877811\n",
      "Epoch: 6/8, Batch: 1530/3444, Loss: 0.01321423426270485\n",
      "Epoch: 6/8, Batch: 1540/3444, Loss: 0.011774937622249126\n",
      "Epoch: 6/8, Batch: 1550/3444, Loss: 0.016099121421575546\n",
      "Epoch: 6/8, Batch: 1560/3444, Loss: 0.02177884802222252\n",
      "Epoch: 6/8, Batch: 1570/3444, Loss: 0.022606071084737778\n",
      "Epoch: 6/8, Batch: 1580/3444, Loss: 0.0077390847727656364\n",
      "Epoch: 6/8, Batch: 1590/3444, Loss: 0.01806103065609932\n",
      "Epoch: 6/8, Batch: 1600/3444, Loss: 0.021641291677951813\n",
      "Epoch: 6/8, Batch: 1610/3444, Loss: 0.01352511253207922\n",
      "Epoch: 6/8, Batch: 1620/3444, Loss: 0.011031021364033222\n",
      "Epoch: 6/8, Batch: 1630/3444, Loss: 0.017906442284584045\n",
      "Epoch: 6/8, Batch: 1640/3444, Loss: 0.016388459131121635\n",
      "Epoch: 6/8, Batch: 1650/3444, Loss: 0.028682047501206398\n",
      "Epoch: 6/8, Batch: 1660/3444, Loss: 0.007733581122010946\n",
      "Epoch: 6/8, Batch: 1670/3444, Loss: 0.006661289371550083\n",
      "Epoch: 6/8, Batch: 1680/3444, Loss: 0.01410156860947609\n",
      "Epoch: 6/8, Batch: 1690/3444, Loss: 0.0242327768355608\n",
      "Epoch: 6/8, Batch: 1700/3444, Loss: 0.01624024100601673\n",
      "Epoch: 6/8, Batch: 1710/3444, Loss: 0.012226263992488384\n",
      "Epoch: 6/8, Batch: 1720/3444, Loss: 0.008424133062362671\n",
      "Epoch: 6/8, Batch: 1730/3444, Loss: 0.006224568001925945\n",
      "Epoch: 6/8, Batch: 1740/3444, Loss: 0.007817259058356285\n",
      "Epoch: 6/8, Batch: 1750/3444, Loss: 0.01911819539964199\n",
      "Epoch: 6/8, Batch: 1760/3444, Loss: 0.009194545447826385\n",
      "Epoch: 6/8, Batch: 1770/3444, Loss: 0.01691479980945587\n",
      "Epoch: 6/8, Batch: 1780/3444, Loss: 0.005373852793127298\n",
      "Epoch: 6/8, Batch: 1790/3444, Loss: 0.02903631143271923\n",
      "Epoch: 6/8, Batch: 1800/3444, Loss: 0.0069140181876719\n",
      "Epoch: 6/8, Batch: 1810/3444, Loss: 0.007461202330887318\n",
      "Epoch: 6/8, Batch: 1820/3444, Loss: 0.004927307367324829\n",
      "Epoch: 6/8, Batch: 1830/3444, Loss: 0.014678061939775944\n",
      "Epoch: 6/8, Batch: 1840/3444, Loss: 0.021767931059002876\n",
      "Epoch: 6/8, Batch: 1850/3444, Loss: 0.01573803834617138\n",
      "Epoch: 6/8, Batch: 1860/3444, Loss: 0.00979202426970005\n",
      "Epoch: 6/8, Batch: 1870/3444, Loss: 0.024517225101590157\n",
      "Epoch: 6/8, Batch: 1880/3444, Loss: 0.051194511353969574\n",
      "Epoch: 6/8, Batch: 1890/3444, Loss: 0.014808517880737782\n",
      "Epoch: 6/8, Batch: 1900/3444, Loss: 0.013088938780128956\n",
      "Epoch: 6/8, Batch: 1910/3444, Loss: 0.018270228058099747\n",
      "Epoch: 6/8, Batch: 1920/3444, Loss: 0.0158640518784523\n",
      "Epoch: 6/8, Batch: 1930/3444, Loss: 0.009739434346556664\n",
      "Epoch: 6/8, Batch: 1940/3444, Loss: 0.006273466628044844\n",
      "Epoch: 6/8, Batch: 1950/3444, Loss: 0.028212986886501312\n",
      "Epoch: 6/8, Batch: 1960/3444, Loss: 0.02125082165002823\n",
      "Epoch: 6/8, Batch: 1970/3444, Loss: 0.01021872740238905\n",
      "Epoch: 6/8, Batch: 1980/3444, Loss: 0.007343362085521221\n",
      "Epoch: 6/8, Batch: 1990/3444, Loss: 0.0061507271602749825\n",
      "Epoch: 6/8, Batch: 2000/3444, Loss: 0.01562713272869587\n",
      "Epoch: 6/8, Batch: 2010/3444, Loss: 0.01736588589847088\n",
      "Epoch: 6/8, Batch: 2020/3444, Loss: 0.013024522922933102\n",
      "Epoch: 6/8, Batch: 2030/3444, Loss: 0.007821362465620041\n",
      "Epoch: 6/8, Batch: 2040/3444, Loss: 0.010738647542893887\n",
      "Epoch: 6/8, Batch: 2050/3444, Loss: 0.021287458017468452\n",
      "Epoch: 6/8, Batch: 2060/3444, Loss: 0.01740136556327343\n",
      "Epoch: 6/8, Batch: 2070/3444, Loss: 0.03994651138782501\n",
      "Epoch: 6/8, Batch: 2080/3444, Loss: 0.015952948480844498\n",
      "Epoch: 6/8, Batch: 2090/3444, Loss: 0.008058677427470684\n",
      "Epoch: 6/8, Batch: 2100/3444, Loss: 0.007605721242725849\n",
      "Epoch: 6/8, Batch: 2110/3444, Loss: 0.012924237176775932\n",
      "Epoch: 6/8, Batch: 2120/3444, Loss: 0.01716509833931923\n",
      "Epoch: 6/8, Batch: 2130/3444, Loss: 0.008449623361229897\n",
      "Epoch: 6/8, Batch: 2140/3444, Loss: 0.024709368124604225\n",
      "Epoch: 6/8, Batch: 2150/3444, Loss: 0.014587556943297386\n",
      "Epoch: 6/8, Batch: 2160/3444, Loss: 0.007000450510531664\n",
      "Epoch: 6/8, Batch: 2170/3444, Loss: 0.03697550296783447\n",
      "Epoch: 6/8, Batch: 2180/3444, Loss: 0.014241560362279415\n",
      "Epoch: 6/8, Batch: 2190/3444, Loss: 0.025515897199511528\n",
      "Epoch: 6/8, Batch: 2200/3444, Loss: 0.010586533695459366\n",
      "Epoch: 6/8, Batch: 2210/3444, Loss: 0.006252485793083906\n",
      "Epoch: 6/8, Batch: 2220/3444, Loss: 0.01511658076196909\n",
      "Epoch: 6/8, Batch: 2230/3444, Loss: 0.007159036118537188\n",
      "Epoch: 6/8, Batch: 2240/3444, Loss: 0.014062028378248215\n",
      "Epoch: 6/8, Batch: 2250/3444, Loss: 0.0031134786549955606\n",
      "Epoch: 6/8, Batch: 2260/3444, Loss: 0.009456348605453968\n",
      "Epoch: 6/8, Batch: 2270/3444, Loss: 0.010466700419783592\n",
      "Epoch: 6/8, Batch: 2280/3444, Loss: 0.009885032661259174\n",
      "Epoch: 6/8, Batch: 2290/3444, Loss: 0.012778181582689285\n",
      "Epoch: 6/8, Batch: 2300/3444, Loss: 0.032300032675266266\n",
      "Epoch: 6/8, Batch: 2310/3444, Loss: 0.01060450728982687\n",
      "Epoch: 6/8, Batch: 2320/3444, Loss: 0.019471563398838043\n",
      "Epoch: 6/8, Batch: 2330/3444, Loss: 0.01894199289381504\n",
      "Epoch: 6/8, Batch: 2340/3444, Loss: 0.01534334197640419\n",
      "Epoch: 6/8, Batch: 2350/3444, Loss: 0.011054480448365211\n",
      "Epoch: 6/8, Batch: 2360/3444, Loss: 0.018046654760837555\n",
      "Epoch: 6/8, Batch: 2370/3444, Loss: 0.017794910818338394\n",
      "Epoch: 6/8, Batch: 2380/3444, Loss: 0.0126719418913126\n",
      "Epoch: 6/8, Batch: 2390/3444, Loss: 0.01289015170186758\n",
      "Epoch: 6/8, Batch: 2400/3444, Loss: 0.016578204929828644\n",
      "Epoch: 6/8, Batch: 2410/3444, Loss: 0.012644869275391102\n",
      "Epoch: 6/8, Batch: 2420/3444, Loss: 0.009143918752670288\n",
      "Epoch: 6/8, Batch: 2430/3444, Loss: 0.015680866315960884\n",
      "Epoch: 6/8, Batch: 2440/3444, Loss: 0.013303475454449654\n",
      "Epoch: 6/8, Batch: 2450/3444, Loss: 0.01315613929182291\n",
      "Epoch: 6/8, Batch: 2460/3444, Loss: 0.02995670959353447\n",
      "Epoch: 6/8, Batch: 2470/3444, Loss: 0.016267776489257812\n",
      "Epoch: 6/8, Batch: 2480/3444, Loss: 0.00909668579697609\n",
      "Epoch: 6/8, Batch: 2490/3444, Loss: 0.007665134500712156\n",
      "Epoch: 6/8, Batch: 2500/3444, Loss: 0.009518210776150227\n",
      "Epoch: 6/8, Batch: 2510/3444, Loss: 0.012614752165973186\n",
      "Epoch: 6/8, Batch: 2520/3444, Loss: 0.015161716379225254\n",
      "Epoch: 6/8, Batch: 2530/3444, Loss: 0.026715993881225586\n",
      "Epoch: 6/8, Batch: 2540/3444, Loss: 0.005777363665401936\n",
      "Epoch: 6/8, Batch: 2550/3444, Loss: 0.01265275664627552\n",
      "Epoch: 6/8, Batch: 2560/3444, Loss: 0.006340174004435539\n",
      "Epoch: 6/8, Batch: 2570/3444, Loss: 0.012408135458827019\n",
      "Epoch: 6/8, Batch: 2580/3444, Loss: 0.0335722453892231\n",
      "Epoch: 6/8, Batch: 2590/3444, Loss: 0.011458467692136765\n",
      "Epoch: 6/8, Batch: 2600/3444, Loss: 0.023165665566921234\n",
      "Epoch: 6/8, Batch: 2610/3444, Loss: 0.020422399044036865\n",
      "Epoch: 6/8, Batch: 2620/3444, Loss: 0.01751299761235714\n",
      "Epoch: 6/8, Batch: 2630/3444, Loss: 0.008287987671792507\n",
      "Epoch: 6/8, Batch: 2640/3444, Loss: 0.013460671529173851\n",
      "Epoch: 6/8, Batch: 2650/3444, Loss: 0.009921630844473839\n",
      "Epoch: 6/8, Batch: 2660/3444, Loss: 0.004885361064225435\n",
      "Epoch: 6/8, Batch: 2670/3444, Loss: 0.006573945749551058\n",
      "Epoch: 6/8, Batch: 2680/3444, Loss: 0.007276533637195826\n",
      "Epoch: 6/8, Batch: 2690/3444, Loss: 0.012741529382765293\n",
      "Epoch: 6/8, Batch: 2700/3444, Loss: 0.018189921975135803\n",
      "Epoch: 6/8, Batch: 2710/3444, Loss: 0.006386727560311556\n",
      "Epoch: 6/8, Batch: 2720/3444, Loss: 0.004264939110726118\n",
      "Epoch: 6/8, Batch: 2730/3444, Loss: 0.007499976083636284\n",
      "Epoch: 6/8, Batch: 2740/3444, Loss: 0.011781353503465652\n",
      "Epoch: 6/8, Batch: 2750/3444, Loss: 0.020116202533245087\n",
      "Epoch: 6/8, Batch: 2760/3444, Loss: 0.007203987333923578\n",
      "Epoch: 6/8, Batch: 2770/3444, Loss: 0.025711048394441605\n",
      "Epoch: 6/8, Batch: 2780/3444, Loss: 0.021359382197260857\n",
      "Epoch: 6/8, Batch: 2790/3444, Loss: 0.022775862365961075\n",
      "Epoch: 6/8, Batch: 2800/3444, Loss: 0.026024799793958664\n",
      "Epoch: 6/8, Batch: 2810/3444, Loss: 0.007480365224182606\n",
      "Epoch: 6/8, Batch: 2820/3444, Loss: 0.006656510755419731\n",
      "Epoch: 6/8, Batch: 2830/3444, Loss: 0.016804592683911324\n",
      "Epoch: 6/8, Batch: 2840/3444, Loss: 0.011055444367229939\n",
      "Epoch: 6/8, Batch: 2850/3444, Loss: 0.006826988887041807\n",
      "Epoch: 6/8, Batch: 2860/3444, Loss: 0.03964841365814209\n",
      "Epoch: 6/8, Batch: 2870/3444, Loss: 0.011822662316262722\n",
      "Epoch: 6/8, Batch: 2880/3444, Loss: 0.008184663020074368\n",
      "Epoch: 6/8, Batch: 2890/3444, Loss: 0.03801028057932854\n",
      "Epoch: 6/8, Batch: 2900/3444, Loss: 0.016637852415442467\n",
      "Epoch: 6/8, Batch: 2910/3444, Loss: 0.004622046370059252\n",
      "Epoch: 6/8, Batch: 2920/3444, Loss: 0.01426481083035469\n",
      "Epoch: 6/8, Batch: 2930/3444, Loss: 0.009257066994905472\n",
      "Epoch: 6/8, Batch: 2940/3444, Loss: 0.02089385688304901\n",
      "Epoch: 6/8, Batch: 2950/3444, Loss: 0.007279063109308481\n",
      "Epoch: 6/8, Batch: 2960/3444, Loss: 0.00982027966529131\n",
      "Epoch: 6/8, Batch: 2970/3444, Loss: 0.009028199128806591\n",
      "Epoch: 6/8, Batch: 2980/3444, Loss: 0.01928965002298355\n",
      "Epoch: 6/8, Batch: 2990/3444, Loss: 0.00646569486707449\n",
      "Epoch: 6/8, Batch: 3000/3444, Loss: 0.021466653794050217\n",
      "Epoch: 6/8, Batch: 3010/3444, Loss: 0.01404051948338747\n",
      "Epoch: 6/8, Batch: 3020/3444, Loss: 0.011882588267326355\n",
      "Epoch: 6/8, Batch: 3030/3444, Loss: 0.023682786151766777\n",
      "Epoch: 6/8, Batch: 3040/3444, Loss: 0.020026305690407753\n",
      "Epoch: 6/8, Batch: 3050/3444, Loss: 0.01964099518954754\n",
      "Epoch: 6/8, Batch: 3060/3444, Loss: 0.008420373313128948\n",
      "Epoch: 6/8, Batch: 3070/3444, Loss: 0.00850678887218237\n",
      "Epoch: 6/8, Batch: 3080/3444, Loss: 0.01385470200330019\n",
      "Epoch: 6/8, Batch: 3090/3444, Loss: 0.00955011323094368\n",
      "Epoch: 6/8, Batch: 3100/3444, Loss: 0.02624497190117836\n",
      "Epoch: 6/8, Batch: 3110/3444, Loss: 0.010913586243987083\n",
      "Epoch: 6/8, Batch: 3120/3444, Loss: 0.02400331385433674\n",
      "Epoch: 6/8, Batch: 3130/3444, Loss: 0.006902687717229128\n",
      "Epoch: 6/8, Batch: 3140/3444, Loss: 0.019482653588056564\n",
      "Epoch: 6/8, Batch: 3150/3444, Loss: 0.015770168974995613\n",
      "Epoch: 6/8, Batch: 3160/3444, Loss: 0.04243659973144531\n",
      "Epoch: 6/8, Batch: 3170/3444, Loss: 0.041673850268125534\n",
      "Epoch: 6/8, Batch: 3180/3444, Loss: 0.013761636801064014\n",
      "Epoch: 6/8, Batch: 3190/3444, Loss: 0.012754860334098339\n",
      "Epoch: 6/8, Batch: 3200/3444, Loss: 0.029701970517635345\n",
      "Epoch: 6/8, Batch: 3210/3444, Loss: 0.012533353641629219\n",
      "Epoch: 6/8, Batch: 3220/3444, Loss: 0.00955052301287651\n",
      "Epoch: 6/8, Batch: 3230/3444, Loss: 0.018666131421923637\n",
      "Epoch: 6/8, Batch: 3240/3444, Loss: 0.01507201325148344\n",
      "Epoch: 6/8, Batch: 3250/3444, Loss: 0.008084988221526146\n",
      "Epoch: 6/8, Batch: 3260/3444, Loss: 0.010867827571928501\n",
      "Epoch: 6/8, Batch: 3270/3444, Loss: 0.01315562054514885\n",
      "Epoch: 6/8, Batch: 3280/3444, Loss: 0.013152421452105045\n",
      "Epoch: 6/8, Batch: 3290/3444, Loss: 0.019732244312763214\n",
      "Epoch: 6/8, Batch: 3300/3444, Loss: 0.02348221279680729\n",
      "Epoch: 6/8, Batch: 3310/3444, Loss: 0.007551949936896563\n",
      "Epoch: 6/8, Batch: 3320/3444, Loss: 0.008664645254611969\n",
      "Epoch: 6/8, Batch: 3330/3444, Loss: 0.009000154212117195\n",
      "Epoch: 6/8, Batch: 3340/3444, Loss: 0.010271850042045116\n",
      "Epoch: 6/8, Batch: 3350/3444, Loss: 0.011639424599707127\n",
      "Epoch: 6/8, Batch: 3360/3444, Loss: 0.006286397576332092\n",
      "Epoch: 6/8, Batch: 3370/3444, Loss: 0.03928553685545921\n",
      "Epoch: 6/8, Batch: 3380/3444, Loss: 0.010438867844641209\n",
      "Epoch: 6/8, Batch: 3390/3444, Loss: 0.04068922623991966\n",
      "Epoch: 6/8, Batch: 3400/3444, Loss: 0.009991761296987534\n",
      "Epoch: 6/8, Batch: 3410/3444, Loss: 0.010465437546372414\n",
      "Epoch: 6/8, Batch: 3420/3444, Loss: 0.007662785239517689\n",
      "Epoch: 6/8, Batch: 3430/3444, Loss: 0.01401558518409729\n",
      "Epoch: 6/8, Batch: 3440/3444, Loss: 0.04047945886850357\n",
      "Epoch 00007: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch: 6/8, Val Loss: 0.0321950312470206\n",
      "Epoch: 7/8, Batch: 10/3444, Loss: 0.019348198547959328\n",
      "Epoch: 7/8, Batch: 20/3444, Loss: 0.011709715239703655\n",
      "Epoch: 7/8, Batch: 30/3444, Loss: 0.01552492007613182\n",
      "Epoch: 7/8, Batch: 40/3444, Loss: 0.026246327906847\n",
      "Epoch: 7/8, Batch: 50/3444, Loss: 0.01051823329180479\n",
      "Epoch: 7/8, Batch: 60/3444, Loss: 0.01020573079586029\n",
      "Epoch: 7/8, Batch: 70/3444, Loss: 0.01624887064099312\n",
      "Epoch: 7/8, Batch: 80/3444, Loss: 0.009872658178210258\n",
      "Epoch: 7/8, Batch: 90/3444, Loss: 0.019536586478352547\n",
      "Epoch: 7/8, Batch: 100/3444, Loss: 0.010166225954890251\n",
      "Epoch: 7/8, Batch: 110/3444, Loss: 0.039888300001621246\n",
      "Epoch: 7/8, Batch: 120/3444, Loss: 0.008310478180646896\n",
      "Epoch: 7/8, Batch: 130/3444, Loss: 0.006189504638314247\n",
      "Epoch: 7/8, Batch: 140/3444, Loss: 0.010578576475381851\n",
      "Epoch: 7/8, Batch: 150/3444, Loss: 0.006205042824149132\n",
      "Epoch: 7/8, Batch: 160/3444, Loss: 0.013162406161427498\n",
      "Epoch: 7/8, Batch: 170/3444, Loss: 0.015707997605204582\n",
      "Epoch: 7/8, Batch: 180/3444, Loss: 0.0070625836960971355\n",
      "Epoch: 7/8, Batch: 190/3444, Loss: 0.0051917582750320435\n",
      "Epoch: 7/8, Batch: 200/3444, Loss: 0.01282713282853365\n",
      "Epoch: 7/8, Batch: 210/3444, Loss: 0.01340256817638874\n",
      "Epoch: 7/8, Batch: 220/3444, Loss: 0.011651587672531605\n",
      "Epoch: 7/8, Batch: 230/3444, Loss: 0.005749124567955732\n",
      "Epoch: 7/8, Batch: 240/3444, Loss: 0.008690213784575462\n",
      "Epoch: 7/8, Batch: 250/3444, Loss: 0.011125648394227028\n",
      "Epoch: 7/8, Batch: 260/3444, Loss: 0.008151662535965443\n",
      "Epoch: 7/8, Batch: 270/3444, Loss: 0.023981088772416115\n",
      "Epoch: 7/8, Batch: 280/3444, Loss: 0.0069789765402674675\n",
      "Epoch: 7/8, Batch: 290/3444, Loss: 0.004592678043991327\n",
      "Epoch: 7/8, Batch: 300/3444, Loss: 0.011524774134159088\n",
      "Epoch: 7/8, Batch: 310/3444, Loss: 0.005518773570656776\n",
      "Epoch: 7/8, Batch: 320/3444, Loss: 0.010764040052890778\n",
      "Epoch: 7/8, Batch: 330/3444, Loss: 0.01046864502131939\n",
      "Epoch: 7/8, Batch: 340/3444, Loss: 0.01344089861959219\n",
      "Epoch: 7/8, Batch: 350/3444, Loss: 0.004906552843749523\n",
      "Epoch: 7/8, Batch: 360/3444, Loss: 0.013159576803445816\n",
      "Epoch: 7/8, Batch: 370/3444, Loss: 0.008352058008313179\n",
      "Epoch: 7/8, Batch: 380/3444, Loss: 0.026704562827944756\n",
      "Epoch: 7/8, Batch: 390/3444, Loss: 0.03591374680399895\n",
      "Epoch: 7/8, Batch: 400/3444, Loss: 0.018509989604353905\n",
      "Epoch: 7/8, Batch: 410/3444, Loss: 0.018193088471889496\n",
      "Epoch: 7/8, Batch: 420/3444, Loss: 0.008211541920900345\n",
      "Epoch: 7/8, Batch: 430/3444, Loss: 0.024159656837582588\n",
      "Epoch: 7/8, Batch: 440/3444, Loss: 0.005284282378852367\n",
      "Epoch: 7/8, Batch: 450/3444, Loss: 0.0185239240527153\n",
      "Epoch: 7/8, Batch: 460/3444, Loss: 0.005638523027300835\n",
      "Epoch: 7/8, Batch: 470/3444, Loss: 0.010451769456267357\n",
      "Epoch: 7/8, Batch: 480/3444, Loss: 0.007821351289749146\n",
      "Epoch: 7/8, Batch: 490/3444, Loss: 0.005829512607306242\n",
      "Epoch: 7/8, Batch: 500/3444, Loss: 0.017514219507575035\n",
      "Epoch: 7/8, Batch: 510/3444, Loss: 0.008524741977453232\n",
      "Epoch: 7/8, Batch: 520/3444, Loss: 0.00466569047421217\n",
      "Epoch: 7/8, Batch: 530/3444, Loss: 0.006633876822888851\n",
      "Epoch: 7/8, Batch: 540/3444, Loss: 0.008878413587808609\n",
      "Epoch: 7/8, Batch: 550/3444, Loss: 0.009652648121118546\n",
      "Epoch: 7/8, Batch: 560/3444, Loss: 0.025754589587450027\n",
      "Epoch: 7/8, Batch: 570/3444, Loss: 0.021036256104707718\n",
      "Epoch: 7/8, Batch: 580/3444, Loss: 0.009393086656928062\n",
      "Epoch: 7/8, Batch: 590/3444, Loss: 0.006598384585231543\n",
      "Epoch: 7/8, Batch: 600/3444, Loss: 0.005420937202870846\n",
      "Epoch: 7/8, Batch: 610/3444, Loss: 0.027887700125575066\n",
      "Epoch: 7/8, Batch: 620/3444, Loss: 0.01188609004020691\n",
      "Epoch: 7/8, Batch: 630/3444, Loss: 0.02171059139072895\n",
      "Epoch: 7/8, Batch: 640/3444, Loss: 0.006327858194708824\n",
      "Epoch: 7/8, Batch: 650/3444, Loss: 0.006883063353598118\n",
      "Epoch: 7/8, Batch: 660/3444, Loss: 0.009875720366835594\n",
      "Epoch: 7/8, Batch: 670/3444, Loss: 0.02552628517150879\n",
      "Epoch: 7/8, Batch: 680/3444, Loss: 0.006277810782194138\n",
      "Epoch: 7/8, Batch: 690/3444, Loss: 0.015008838847279549\n",
      "Epoch: 7/8, Batch: 700/3444, Loss: 0.016599884256720543\n",
      "Epoch: 7/8, Batch: 710/3444, Loss: 0.017305124551057816\n",
      "Epoch: 7/8, Batch: 720/3444, Loss: 0.021999632939696312\n",
      "Epoch: 7/8, Batch: 730/3444, Loss: 0.02568516694009304\n",
      "Epoch: 7/8, Batch: 740/3444, Loss: 0.009547298774123192\n",
      "Epoch: 7/8, Batch: 750/3444, Loss: 0.011564357206225395\n",
      "Epoch: 7/8, Batch: 760/3444, Loss: 0.04123274236917496\n",
      "Epoch: 7/8, Batch: 770/3444, Loss: 0.00674086669459939\n",
      "Epoch: 7/8, Batch: 780/3444, Loss: 0.006164028309285641\n",
      "Epoch: 7/8, Batch: 790/3444, Loss: 0.016192199662327766\n",
      "Epoch: 7/8, Batch: 800/3444, Loss: 0.004891342017799616\n",
      "Epoch: 7/8, Batch: 810/3444, Loss: 0.009332919493317604\n",
      "Epoch: 7/8, Batch: 820/3444, Loss: 0.011500585824251175\n",
      "Epoch: 7/8, Batch: 830/3444, Loss: 0.07732488960027695\n",
      "Epoch: 7/8, Batch: 840/3444, Loss: 0.01234353892505169\n",
      "Epoch: 7/8, Batch: 850/3444, Loss: 0.02317800559103489\n",
      "Epoch: 7/8, Batch: 860/3444, Loss: 0.018737761303782463\n",
      "Epoch: 7/8, Batch: 870/3444, Loss: 0.007550614885985851\n",
      "Epoch: 7/8, Batch: 880/3444, Loss: 0.009337005205452442\n",
      "Epoch: 7/8, Batch: 890/3444, Loss: 0.008847522549331188\n",
      "Epoch: 7/8, Batch: 900/3444, Loss: 0.0067332047037780285\n",
      "Epoch: 7/8, Batch: 910/3444, Loss: 0.0042937868274748325\n",
      "Epoch: 7/8, Batch: 920/3444, Loss: 0.007103150710463524\n",
      "Epoch: 7/8, Batch: 930/3444, Loss: 0.012125391513109207\n",
      "Epoch: 7/8, Batch: 940/3444, Loss: 0.014500376768410206\n",
      "Epoch: 7/8, Batch: 950/3444, Loss: 0.006960285361856222\n",
      "Epoch: 7/8, Batch: 960/3444, Loss: 0.005583079066127539\n",
      "Epoch: 7/8, Batch: 970/3444, Loss: 0.015766337513923645\n",
      "Epoch: 7/8, Batch: 980/3444, Loss: 0.012488779611885548\n",
      "Epoch: 7/8, Batch: 990/3444, Loss: 0.008579817600548267\n",
      "Epoch: 7/8, Batch: 1000/3444, Loss: 0.007319689728319645\n",
      "Epoch: 7/8, Batch: 1010/3444, Loss: 0.00828688032925129\n",
      "Epoch: 7/8, Batch: 1020/3444, Loss: 0.021338408812880516\n",
      "Epoch: 7/8, Batch: 1030/3444, Loss: 0.010554836131632328\n",
      "Epoch: 7/8, Batch: 1040/3444, Loss: 0.018307844176888466\n",
      "Epoch: 7/8, Batch: 1050/3444, Loss: 0.003185956971719861\n",
      "Epoch: 7/8, Batch: 1060/3444, Loss: 0.021251657977700233\n",
      "Epoch: 7/8, Batch: 1070/3444, Loss: 0.027608588337898254\n",
      "Epoch: 7/8, Batch: 1080/3444, Loss: 0.006742772180587053\n",
      "Epoch: 7/8, Batch: 1090/3444, Loss: 0.015839029103517532\n",
      "Epoch: 7/8, Batch: 1100/3444, Loss: 0.006754968781024218\n",
      "Epoch: 7/8, Batch: 1110/3444, Loss: 0.015147660858929157\n",
      "Epoch: 7/8, Batch: 1120/3444, Loss: 0.007123121526092291\n",
      "Epoch: 7/8, Batch: 1130/3444, Loss: 0.028857367113232613\n",
      "Epoch: 7/8, Batch: 1140/3444, Loss: 0.025296228006482124\n",
      "Epoch: 7/8, Batch: 1150/3444, Loss: 0.014616455882787704\n",
      "Epoch: 7/8, Batch: 1160/3444, Loss: 0.023470252752304077\n",
      "Epoch: 7/8, Batch: 1170/3444, Loss: 0.028009485453367233\n",
      "Epoch: 7/8, Batch: 1180/3444, Loss: 0.01110091619193554\n",
      "Epoch: 7/8, Batch: 1190/3444, Loss: 0.00725598493590951\n",
      "Epoch: 7/8, Batch: 1200/3444, Loss: 0.007230342831462622\n",
      "Epoch: 7/8, Batch: 1210/3444, Loss: 0.00663860933855176\n",
      "Epoch: 7/8, Batch: 1220/3444, Loss: 0.021944798529148102\n",
      "Epoch: 7/8, Batch: 1230/3444, Loss: 0.013288983143866062\n",
      "Epoch: 7/8, Batch: 1240/3444, Loss: 0.017343349754810333\n",
      "Epoch: 7/8, Batch: 1250/3444, Loss: 0.007588732056319714\n",
      "Epoch: 7/8, Batch: 1260/3444, Loss: 0.007542927283793688\n",
      "Epoch: 7/8, Batch: 1270/3444, Loss: 0.015148014761507511\n",
      "Epoch: 7/8, Batch: 1280/3444, Loss: 0.004422434139996767\n",
      "Epoch: 7/8, Batch: 1290/3444, Loss: 0.007178978994488716\n",
      "Epoch: 7/8, Batch: 1300/3444, Loss: 0.015022613108158112\n",
      "Epoch: 7/8, Batch: 1310/3444, Loss: 0.017536427825689316\n",
      "Epoch: 7/8, Batch: 1320/3444, Loss: 0.013706553727388382\n",
      "Epoch: 7/8, Batch: 1330/3444, Loss: 0.010856443084776402\n",
      "Epoch: 7/8, Batch: 1340/3444, Loss: 0.007833180017769337\n",
      "Epoch: 7/8, Batch: 1350/3444, Loss: 0.039508234709501266\n",
      "Epoch: 7/8, Batch: 1360/3444, Loss: 0.008788783103227615\n",
      "Epoch: 7/8, Batch: 1370/3444, Loss: 0.013832168653607368\n",
      "Epoch: 7/8, Batch: 1380/3444, Loss: 0.013933154754340649\n",
      "Epoch: 7/8, Batch: 1390/3444, Loss: 0.008695913478732109\n",
      "Epoch: 7/8, Batch: 1400/3444, Loss: 0.004569411277770996\n",
      "Epoch: 7/8, Batch: 1410/3444, Loss: 0.007116420194506645\n",
      "Epoch: 7/8, Batch: 1420/3444, Loss: 0.006225490476936102\n",
      "Epoch: 7/8, Batch: 1430/3444, Loss: 0.006710002198815346\n",
      "Epoch: 7/8, Batch: 1440/3444, Loss: 0.02395525947213173\n",
      "Epoch: 7/8, Batch: 1450/3444, Loss: 0.016287051141262054\n",
      "Epoch: 7/8, Batch: 1460/3444, Loss: 0.006693401373922825\n",
      "Epoch: 7/8, Batch: 1470/3444, Loss: 0.023340938612818718\n",
      "Epoch: 7/8, Batch: 1480/3444, Loss: 0.010132803581655025\n",
      "Epoch: 7/8, Batch: 1490/3444, Loss: 0.015650494024157524\n",
      "Epoch: 7/8, Batch: 1500/3444, Loss: 0.017386984080076218\n",
      "Epoch: 7/8, Batch: 1510/3444, Loss: 0.013882438652217388\n",
      "Epoch: 7/8, Batch: 1520/3444, Loss: 0.007088783662766218\n",
      "Epoch: 7/8, Batch: 1530/3444, Loss: 0.00793445110321045\n",
      "Epoch: 7/8, Batch: 1540/3444, Loss: 0.021788321435451508\n",
      "Epoch: 7/8, Batch: 1550/3444, Loss: 0.014996983110904694\n",
      "Epoch: 7/8, Batch: 1560/3444, Loss: 0.01917031966149807\n",
      "Epoch: 7/8, Batch: 1570/3444, Loss: 0.015081056393682957\n",
      "Epoch: 7/8, Batch: 1580/3444, Loss: 0.02552717551589012\n",
      "Epoch: 7/8, Batch: 1590/3444, Loss: 0.006716765463352203\n",
      "Epoch: 7/8, Batch: 1600/3444, Loss: 0.006407580804079771\n",
      "Epoch: 7/8, Batch: 1610/3444, Loss: 0.007223618216812611\n",
      "Epoch: 7/8, Batch: 1620/3444, Loss: 0.012706592679023743\n",
      "Epoch: 7/8, Batch: 1630/3444, Loss: 0.01583564653992653\n",
      "Epoch: 7/8, Batch: 1640/3444, Loss: 0.006852702237665653\n",
      "Epoch: 7/8, Batch: 1650/3444, Loss: 0.005574143957346678\n",
      "Epoch: 7/8, Batch: 1660/3444, Loss: 0.012317966669797897\n",
      "Epoch: 7/8, Batch: 1670/3444, Loss: 0.009397982619702816\n",
      "Epoch: 7/8, Batch: 1680/3444, Loss: 0.006811268161982298\n",
      "Epoch: 7/8, Batch: 1690/3444, Loss: 0.017491044476628304\n",
      "Epoch: 7/8, Batch: 1700/3444, Loss: 0.007989459671080112\n",
      "Epoch: 7/8, Batch: 1710/3444, Loss: 0.006152980495244265\n",
      "Epoch: 7/8, Batch: 1720/3444, Loss: 0.016910279169678688\n",
      "Epoch: 7/8, Batch: 1730/3444, Loss: 0.007517656311392784\n",
      "Epoch: 7/8, Batch: 1740/3444, Loss: 0.036373745650053024\n",
      "Epoch: 7/8, Batch: 1750/3444, Loss: 0.016678670421242714\n",
      "Epoch: 7/8, Batch: 1760/3444, Loss: 0.008839272893965244\n",
      "Epoch: 7/8, Batch: 1770/3444, Loss: 0.003989051561802626\n",
      "Epoch: 7/8, Batch: 1780/3444, Loss: 0.004840159323066473\n",
      "Epoch: 7/8, Batch: 1790/3444, Loss: 0.006047634873539209\n",
      "Epoch: 7/8, Batch: 1800/3444, Loss: 0.01753237284719944\n",
      "Epoch: 7/8, Batch: 1810/3444, Loss: 0.00841591414064169\n",
      "Epoch: 7/8, Batch: 1820/3444, Loss: 0.0046142833307385445\n",
      "Epoch: 7/8, Batch: 1830/3444, Loss: 0.011134402826428413\n",
      "Epoch: 7/8, Batch: 1840/3444, Loss: 0.021957486867904663\n",
      "Epoch: 7/8, Batch: 1850/3444, Loss: 0.004565137438476086\n",
      "Epoch: 7/8, Batch: 1860/3444, Loss: 0.007393742445856333\n",
      "Epoch: 7/8, Batch: 1870/3444, Loss: 0.007564302999526262\n",
      "Epoch: 7/8, Batch: 1880/3444, Loss: 0.005335301626473665\n",
      "Epoch: 7/8, Batch: 1890/3444, Loss: 0.0090518519282341\n",
      "Epoch: 7/8, Batch: 1900/3444, Loss: 0.009581295773386955\n",
      "Epoch: 7/8, Batch: 1910/3444, Loss: 0.007786703296005726\n",
      "Epoch: 7/8, Batch: 1920/3444, Loss: 0.018503576517105103\n",
      "Epoch: 7/8, Batch: 1930/3444, Loss: 0.01326136477291584\n",
      "Epoch: 7/8, Batch: 1940/3444, Loss: 0.005030051339417696\n",
      "Epoch: 7/8, Batch: 1950/3444, Loss: 0.016265271231532097\n",
      "Epoch: 7/8, Batch: 1960/3444, Loss: 0.023667369037866592\n",
      "Epoch: 7/8, Batch: 1970/3444, Loss: 0.01532596256583929\n",
      "Epoch: 7/8, Batch: 1980/3444, Loss: 0.018390456214547157\n",
      "Epoch: 7/8, Batch: 1990/3444, Loss: 0.01023787260055542\n",
      "Epoch: 7/8, Batch: 2000/3444, Loss: 0.003958993591368198\n",
      "Epoch: 7/8, Batch: 2010/3444, Loss: 0.006961956154555082\n",
      "Epoch: 7/8, Batch: 2020/3444, Loss: 0.013289588503539562\n",
      "Epoch: 7/8, Batch: 2030/3444, Loss: 0.0036268688272684813\n",
      "Epoch: 7/8, Batch: 2040/3444, Loss: 0.01431978028267622\n",
      "Epoch: 7/8, Batch: 2050/3444, Loss: 0.0035716695711016655\n",
      "Epoch: 7/8, Batch: 2060/3444, Loss: 0.0063247354701161385\n",
      "Epoch: 7/8, Batch: 2070/3444, Loss: 0.013262339867651463\n",
      "Epoch: 7/8, Batch: 2080/3444, Loss: 0.008673373609781265\n",
      "Epoch: 7/8, Batch: 2090/3444, Loss: 0.009077259339392185\n",
      "Epoch: 7/8, Batch: 2100/3444, Loss: 0.004209308419376612\n",
      "Epoch: 7/8, Batch: 2110/3444, Loss: 0.01035300362855196\n",
      "Epoch: 7/8, Batch: 2120/3444, Loss: 0.009427878074347973\n",
      "Epoch: 7/8, Batch: 2130/3444, Loss: 0.0060645537450909615\n",
      "Epoch: 7/8, Batch: 2140/3444, Loss: 0.005086589604616165\n",
      "Epoch: 7/8, Batch: 2150/3444, Loss: 0.010961678810417652\n",
      "Epoch: 7/8, Batch: 2160/3444, Loss: 0.0078023988753557205\n",
      "Epoch: 7/8, Batch: 2170/3444, Loss: 0.011813833378255367\n",
      "Epoch: 7/8, Batch: 2180/3444, Loss: 0.0060524847358465195\n",
      "Epoch: 7/8, Batch: 2190/3444, Loss: 0.006374537479132414\n",
      "Epoch: 7/8, Batch: 2200/3444, Loss: 0.01240187045186758\n",
      "Epoch: 7/8, Batch: 2210/3444, Loss: 0.016198530793190002\n",
      "Epoch: 7/8, Batch: 2220/3444, Loss: 0.016393836587667465\n",
      "Epoch: 7/8, Batch: 2230/3444, Loss: 0.02619018964469433\n",
      "Epoch: 7/8, Batch: 2240/3444, Loss: 0.018216611817479134\n",
      "Epoch: 7/8, Batch: 2250/3444, Loss: 0.0027230314444750547\n",
      "Epoch: 7/8, Batch: 2260/3444, Loss: 0.008143777027726173\n",
      "Epoch: 7/8, Batch: 2270/3444, Loss: 0.008022540248930454\n",
      "Epoch: 7/8, Batch: 2280/3444, Loss: 0.010959488339722157\n",
      "Epoch: 7/8, Batch: 2290/3444, Loss: 0.021869583055377007\n",
      "Epoch: 7/8, Batch: 2300/3444, Loss: 0.009883630089461803\n",
      "Epoch: 7/8, Batch: 2310/3444, Loss: 0.0024440204724669456\n",
      "Epoch: 7/8, Batch: 2320/3444, Loss: 0.009314603172242641\n",
      "Epoch: 7/8, Batch: 2330/3444, Loss: 0.007844728417694569\n",
      "Epoch: 7/8, Batch: 2340/3444, Loss: 0.008535842411220074\n",
      "Epoch: 7/8, Batch: 2350/3444, Loss: 0.006612039636820555\n",
      "Epoch: 7/8, Batch: 2360/3444, Loss: 0.01777847297489643\n",
      "Epoch: 7/8, Batch: 2370/3444, Loss: 0.010028524324297905\n",
      "Epoch: 7/8, Batch: 2380/3444, Loss: 0.0038317448925226927\n",
      "Epoch: 7/8, Batch: 2390/3444, Loss: 0.00824328325688839\n",
      "Epoch: 7/8, Batch: 2400/3444, Loss: 0.008073480799794197\n",
      "Epoch: 7/8, Batch: 2410/3444, Loss: 0.006991160102188587\n",
      "Epoch: 7/8, Batch: 2420/3444, Loss: 0.014655007049441338\n",
      "Epoch: 7/8, Batch: 2430/3444, Loss: 0.0032763569615781307\n",
      "Epoch: 7/8, Batch: 2440/3444, Loss: 0.005316357593983412\n",
      "Epoch: 7/8, Batch: 2450/3444, Loss: 0.018674422055482864\n",
      "Epoch: 7/8, Batch: 2460/3444, Loss: 0.03314788267016411\n",
      "Epoch: 7/8, Batch: 2470/3444, Loss: 0.005240749567747116\n",
      "Epoch: 7/8, Batch: 2480/3444, Loss: 0.012083313427865505\n",
      "Epoch: 7/8, Batch: 2490/3444, Loss: 0.008928176946938038\n",
      "Epoch: 7/8, Batch: 2500/3444, Loss: 0.01765896938741207\n",
      "Epoch: 7/8, Batch: 2510/3444, Loss: 0.005754208657890558\n",
      "Epoch: 7/8, Batch: 2520/3444, Loss: 0.009532556869089603\n",
      "Epoch: 7/8, Batch: 2530/3444, Loss: 0.007417995948344469\n",
      "Epoch: 7/8, Batch: 2540/3444, Loss: 0.01492863055318594\n",
      "Epoch: 7/8, Batch: 2550/3444, Loss: 0.011793937534093857\n",
      "Epoch: 7/8, Batch: 2560/3444, Loss: 0.024141576141119003\n",
      "Epoch: 7/8, Batch: 2570/3444, Loss: 0.015071090310811996\n",
      "Epoch: 7/8, Batch: 2580/3444, Loss: 0.012649294920265675\n",
      "Epoch: 7/8, Batch: 2590/3444, Loss: 0.018169527873396873\n",
      "Epoch: 7/8, Batch: 2600/3444, Loss: 0.010563213378190994\n",
      "Epoch: 7/8, Batch: 2610/3444, Loss: 0.007488573435693979\n",
      "Epoch: 7/8, Batch: 2620/3444, Loss: 0.0049683344550430775\n",
      "Epoch: 7/8, Batch: 2630/3444, Loss: 0.0058201756328344345\n",
      "Epoch: 7/8, Batch: 2640/3444, Loss: 0.00928746722638607\n",
      "Epoch: 7/8, Batch: 2650/3444, Loss: 0.02074313722550869\n",
      "Epoch: 7/8, Batch: 2660/3444, Loss: 0.007543119601905346\n",
      "Epoch: 7/8, Batch: 2670/3444, Loss: 0.009023219347000122\n",
      "Epoch: 7/8, Batch: 2680/3444, Loss: 0.007923951372504234\n",
      "Epoch: 7/8, Batch: 2690/3444, Loss: 0.006938216742128134\n",
      "Epoch: 7/8, Batch: 2700/3444, Loss: 0.00954001396894455\n",
      "Epoch: 7/8, Batch: 2710/3444, Loss: 0.0050371671095490456\n",
      "Epoch: 7/8, Batch: 2720/3444, Loss: 0.01043727807700634\n",
      "Epoch: 7/8, Batch: 2730/3444, Loss: 0.005091426894068718\n",
      "Epoch: 7/8, Batch: 2740/3444, Loss: 0.01897302456200123\n",
      "Epoch: 7/8, Batch: 2750/3444, Loss: 0.02042543888092041\n",
      "Epoch: 7/8, Batch: 2760/3444, Loss: 0.004899219609797001\n",
      "Epoch: 7/8, Batch: 2770/3444, Loss: 0.011424343101680279\n",
      "Epoch: 7/8, Batch: 2780/3444, Loss: 0.007009030785411596\n",
      "Epoch: 7/8, Batch: 2790/3444, Loss: 0.002315779682248831\n",
      "Epoch: 7/8, Batch: 2800/3444, Loss: 0.012019280344247818\n",
      "Epoch: 7/8, Batch: 2810/3444, Loss: 0.0047193546779453754\n",
      "Epoch: 7/8, Batch: 2820/3444, Loss: 0.011203985661268234\n",
      "Epoch: 7/8, Batch: 2830/3444, Loss: 0.010292485356330872\n",
      "Epoch: 7/8, Batch: 2840/3444, Loss: 0.005485144909471273\n",
      "Epoch: 7/8, Batch: 2850/3444, Loss: 0.004636755213141441\n",
      "Epoch: 7/8, Batch: 2860/3444, Loss: 0.013150526210665703\n",
      "Epoch: 7/8, Batch: 2870/3444, Loss: 0.008634543046355247\n",
      "Epoch: 7/8, Batch: 2880/3444, Loss: 0.021662097424268723\n",
      "Epoch: 7/8, Batch: 2890/3444, Loss: 0.00640768650919199\n",
      "Epoch: 7/8, Batch: 2900/3444, Loss: 0.00671008974313736\n",
      "Epoch: 7/8, Batch: 2910/3444, Loss: 0.005448534153401852\n",
      "Epoch: 7/8, Batch: 2920/3444, Loss: 0.008590408600866795\n",
      "Epoch: 7/8, Batch: 2930/3444, Loss: 0.010457716882228851\n",
      "Epoch: 7/8, Batch: 2940/3444, Loss: 0.0068160113878548145\n",
      "Epoch: 7/8, Batch: 2950/3444, Loss: 0.012650687247514725\n",
      "Epoch: 7/8, Batch: 2960/3444, Loss: 0.009317156858742237\n",
      "Epoch: 7/8, Batch: 2970/3444, Loss: 0.006080614402890205\n",
      "Epoch: 7/8, Batch: 2980/3444, Loss: 0.009590310044586658\n",
      "Epoch: 7/8, Batch: 2990/3444, Loss: 0.008848298341035843\n",
      "Epoch: 7/8, Batch: 3000/3444, Loss: 0.014972922392189503\n",
      "Epoch: 7/8, Batch: 3010/3444, Loss: 0.010626721195876598\n",
      "Epoch: 7/8, Batch: 3020/3444, Loss: 0.0036120375152677298\n",
      "Epoch: 7/8, Batch: 3030/3444, Loss: 0.009021410718560219\n",
      "Epoch: 7/8, Batch: 3040/3444, Loss: 0.006916572339832783\n",
      "Epoch: 7/8, Batch: 3050/3444, Loss: 0.012766167521476746\n",
      "Epoch: 7/8, Batch: 3060/3444, Loss: 0.0032527176663279533\n",
      "Epoch: 7/8, Batch: 3070/3444, Loss: 0.004197938833385706\n",
      "Epoch: 7/8, Batch: 3080/3444, Loss: 0.0052452594973146915\n",
      "Epoch: 7/8, Batch: 3090/3444, Loss: 0.0037378747947514057\n",
      "Epoch: 7/8, Batch: 3100/3444, Loss: 0.004013704136013985\n",
      "Epoch: 7/8, Batch: 3110/3444, Loss: 0.008740276098251343\n",
      "Epoch: 7/8, Batch: 3120/3444, Loss: 0.015040071681141853\n",
      "Epoch: 7/8, Batch: 3130/3444, Loss: 0.013524352572858334\n",
      "Epoch: 7/8, Batch: 3140/3444, Loss: 0.03176402300596237\n",
      "Epoch: 7/8, Batch: 3150/3444, Loss: 0.009324715472757816\n",
      "Epoch: 7/8, Batch: 3160/3444, Loss: 0.0068897963501513\n",
      "Epoch: 7/8, Batch: 3170/3444, Loss: 0.008301565423607826\n",
      "Epoch: 7/8, Batch: 3180/3444, Loss: 0.01573333889245987\n",
      "Epoch: 7/8, Batch: 3190/3444, Loss: 0.009424402378499508\n",
      "Epoch: 7/8, Batch: 3200/3444, Loss: 0.012706826440989971\n",
      "Epoch: 7/8, Batch: 3210/3444, Loss: 0.015930771827697754\n",
      "Epoch: 7/8, Batch: 3220/3444, Loss: 0.01263936422765255\n",
      "Epoch: 7/8, Batch: 3230/3444, Loss: 0.007401107810437679\n",
      "Epoch: 7/8, Batch: 3240/3444, Loss: 0.0042293923906981945\n",
      "Epoch: 7/8, Batch: 3250/3444, Loss: 0.02968018874526024\n",
      "Epoch: 7/8, Batch: 3260/3444, Loss: 0.02438235841691494\n",
      "Epoch: 7/8, Batch: 3270/3444, Loss: 0.011939211748540401\n",
      "Epoch: 7/8, Batch: 3280/3444, Loss: 0.020594310015439987\n",
      "Epoch: 7/8, Batch: 3290/3444, Loss: 0.0036808005534112453\n",
      "Epoch: 7/8, Batch: 3300/3444, Loss: 0.0033882458228617907\n",
      "Epoch: 7/8, Batch: 3310/3444, Loss: 0.030212244018912315\n",
      "Epoch: 7/8, Batch: 3320/3444, Loss: 0.012821720913052559\n",
      "Epoch: 7/8, Batch: 3330/3444, Loss: 0.02106918767094612\n",
      "Epoch: 7/8, Batch: 3340/3444, Loss: 0.007751771714538336\n",
      "Epoch: 7/8, Batch: 3350/3444, Loss: 0.027434568852186203\n",
      "Epoch: 7/8, Batch: 3360/3444, Loss: 0.011438680812716484\n",
      "Epoch: 7/8, Batch: 3370/3444, Loss: 0.0111582325771451\n",
      "Epoch: 7/8, Batch: 3380/3444, Loss: 0.02935798652470112\n",
      "Epoch: 7/8, Batch: 3390/3444, Loss: 0.015196547843515873\n",
      "Epoch: 7/8, Batch: 3400/3444, Loss: 0.0235389806330204\n",
      "Epoch: 7/8, Batch: 3410/3444, Loss: 0.00833495706319809\n",
      "Epoch: 7/8, Batch: 3420/3444, Loss: 0.006767929531633854\n",
      "Epoch: 7/8, Batch: 3430/3444, Loss: 0.005268190521746874\n",
      "Epoch: 7/8, Batch: 3440/3444, Loss: 0.017046717926859856\n",
      "Epoch: 7/8, Val Loss: 0.021997721561281557\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lstm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\Load_Argo2_Public.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000011?line=38'>39</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep(val_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000011?line=39'>40</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m, Val Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000011?line=41'>42</a>\u001b[0m torch\u001b[39m.\u001b[39msave(lstm\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mmodels/palo_alto_model.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lstm' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "learning_rate = 0.001\n",
    "\n",
    "mlp = MLP()\n",
    "mlp.to(device)\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.25, verbose=True)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mlp.train(True)\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        inp, label = sample_batch\n",
    "        \n",
    "        inp = inp.to(device)\n",
    "        label = label.to(device)\n",
    "        output = mlp(inp.float())\n",
    "        # Reshape output from [batch_size, 120] to [batch_size, 60, 60]\n",
    "        output = output.view(output.size(0), -1, 2)\n",
    "\n",
    "        loss = loss_fn(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % 10 == 0 and i_batch != 0:\n",
    "            print(f\"Epoch: {epoch}/{num_epochs}, Batch: {i_batch}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    mlp.train(False)\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batch in enumerate(val_loader):\n",
    "            inp, label = sample_batch\n",
    "            inp, label = inp.to(device), label.to(device)\n",
    "            output = mlp(inp.float())\n",
    "            output = output.view(output.size(0), -1, 2)\n",
    "            loss = loss_fn(output, label)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch: {epoch}/{num_epochs}, Val Loss: {val_loss}\")\n",
    "            \n",
    "torch.save(mlp.state_dict(), \"models/palo_alto_model.pt\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae6203",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "de312802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(2, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (tanh): Tanh()\n",
       "  (linear2): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = LSTM()\n",
    "lstm.load_state_dict(torch.load(\"models/austin_model.pt\", map_location=device))\n",
    "lstm.to(device)\n",
    "lstm.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac0370",
   "metadata": {},
   "source": [
    "## Sample Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a571d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x, model, steps=60):\n",
    "    h = None\n",
    "    c = None\n",
    "    arr = []\n",
    "    for i in range(steps):\n",
    "        output, h, c = model(x, h, c)\n",
    "        # if i == 0:\n",
    "        #     print(x[0][40:-1], output[0][40:-1])\n",
    "        elem = output[:, -1, :]\n",
    "        x = elem.unsqueeze(1)\n",
    "        elem = elem.squeeze().detach().cpu().numpy()\n",
    "        arr.append(elem)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9c395",
   "metadata": {},
   "source": [
    "## Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "15b2920a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5767,  0.2304],\n",
      "        [-0.5774,  0.2307],\n",
      "        [-0.5780,  0.2309],\n",
      "        [-0.5786,  0.2311],\n",
      "        [-0.5792,  0.2314],\n",
      "        [-0.5798,  0.2316],\n",
      "        [-0.5804,  0.2319],\n",
      "        [-0.5810,  0.2321],\n",
      "        [-0.5816,  0.2324]]) tensor([[-0.5731,  0.2248],\n",
      "        [-0.5738,  0.2251],\n",
      "        [-0.5744,  0.2253],\n",
      "        [-0.5750,  0.2255],\n",
      "        [-0.5756,  0.2258],\n",
      "        [-0.5762,  0.2260],\n",
      "        [-0.5769,  0.2263],\n",
      "        [-0.5775,  0.2265],\n",
      "        [-0.5781,  0.2268]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.8301,  0.7677],\n",
      "        [-0.8300,  0.7672],\n",
      "        [-0.8298,  0.7668],\n",
      "        [-0.8296,  0.7663],\n",
      "        [-0.8294,  0.7659],\n",
      "        [-0.8292,  0.7655],\n",
      "        [-0.8289,  0.7651],\n",
      "        [-0.8287,  0.7647],\n",
      "        [-0.8284,  0.7643]]) tensor([[-0.8437,  0.7678],\n",
      "        [-0.8435,  0.7673],\n",
      "        [-0.8433,  0.7668],\n",
      "        [-0.8432,  0.7663],\n",
      "        [-0.8430,  0.7658],\n",
      "        [-0.8427,  0.7654],\n",
      "        [-0.8425,  0.7650],\n",
      "        [-0.8422,  0.7646],\n",
      "        [-0.8420,  0.7642]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257]]) tensor([[-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.6576,  2.3997],\n",
      "        [-0.6575,  2.3998],\n",
      "        [-0.6575,  2.3999],\n",
      "        [-0.6575,  2.4000],\n",
      "        [-0.6575,  2.4001],\n",
      "        [-0.6575,  2.4003],\n",
      "        [-0.6575,  2.4004],\n",
      "        [-0.6575,  2.4005],\n",
      "        [-0.6575,  2.4007]]) tensor([[-0.7435,  2.6390],\n",
      "        [-0.7435,  2.6392],\n",
      "        [-0.7435,  2.6393],\n",
      "        [-0.7434,  2.6395],\n",
      "        [-0.7434,  2.6397],\n",
      "        [-0.7434,  2.6399],\n",
      "        [-0.7434,  2.6400],\n",
      "        [-0.7434,  2.6402],\n",
      "        [-0.7434,  2.6404]], grad_fn=<SliceBackward0>)\n",
      "tensor([[0.3002, 0.1719],\n",
      "        [0.3010, 0.1716],\n",
      "        [0.3017, 0.1712],\n",
      "        [0.3024, 0.1708],\n",
      "        [0.3032, 0.1704],\n",
      "        [0.3039, 0.1700],\n",
      "        [0.3047, 0.1697],\n",
      "        [0.3055, 0.1692],\n",
      "        [0.3063, 0.1688]]) tensor([[0.2964, 0.1674],\n",
      "        [0.2971, 0.1670],\n",
      "        [0.2979, 0.1667],\n",
      "        [0.2986, 0.1663],\n",
      "        [0.2993, 0.1659],\n",
      "        [0.3000, 0.1655],\n",
      "        [0.3008, 0.1651],\n",
      "        [0.3016, 0.1647],\n",
      "        [0.3024, 0.1643]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.9210, -1.1868],\n",
      "        [ 0.9216, -1.1871],\n",
      "        [ 0.9223, -1.1874],\n",
      "        [ 0.9229, -1.1876],\n",
      "        [ 0.9236, -1.1881],\n",
      "        [ 0.9243, -1.1884],\n",
      "        [ 0.9249, -1.1887],\n",
      "        [ 0.9256, -1.1891],\n",
      "        [ 0.9262, -1.1894]]) tensor([[ 0.9512, -1.2350],\n",
      "        [ 0.9520, -1.2353],\n",
      "        [ 0.9527, -1.2356],\n",
      "        [ 0.9533, -1.2359],\n",
      "        [ 0.9541, -1.2363],\n",
      "        [ 0.9549, -1.2367],\n",
      "        [ 0.9556, -1.2371],\n",
      "        [ 0.9563, -1.2375],\n",
      "        [ 0.9570, -1.2378]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 2.1759, -1.1351],\n",
      "        [ 2.1755, -1.1361],\n",
      "        [ 2.1751, -1.1371],\n",
      "        [ 2.1747, -1.1380],\n",
      "        [ 2.1743, -1.1389],\n",
      "        [ 2.1740, -1.1396],\n",
      "        [ 2.1736, -1.1405],\n",
      "        [ 2.1732, -1.1415],\n",
      "        [ 2.1727, -1.1425]]) tensor([[ 2.4154, -1.2999],\n",
      "        [ 2.4147, -1.3008],\n",
      "        [ 2.4142, -1.3017],\n",
      "        [ 2.4136, -1.3025],\n",
      "        [ 2.4131, -1.3033],\n",
      "        [ 2.4126, -1.3040],\n",
      "        [ 2.4122, -1.3048],\n",
      "        [ 2.4118, -1.3056],\n",
      "        [ 2.4114, -1.3065]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.5309, -0.9934],\n",
      "        [ 0.5302, -0.9931],\n",
      "        [ 0.5295, -0.9927],\n",
      "        [ 0.5288, -0.9924],\n",
      "        [ 0.5281, -0.9920],\n",
      "        [ 0.5274, -0.9917],\n",
      "        [ 0.5267, -0.9913],\n",
      "        [ 0.5259, -0.9910],\n",
      "        [ 0.5252, -0.9906]]) tensor([[ 0.5369, -1.0204],\n",
      "        [ 0.5361, -1.0201],\n",
      "        [ 0.5354, -1.0197],\n",
      "        [ 0.5347, -1.0193],\n",
      "        [ 0.5339, -1.0189],\n",
      "        [ 0.5332, -1.0185],\n",
      "        [ 0.5325, -1.0182],\n",
      "        [ 0.5317, -1.0178],\n",
      "        [ 0.5310, -1.0174]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.7803, -0.0776],\n",
      "        [-0.7803, -0.0778],\n",
      "        [-0.7804, -0.0780],\n",
      "        [-0.7805, -0.0783],\n",
      "        [-0.7805, -0.0785],\n",
      "        [-0.7806, -0.0788],\n",
      "        [-0.7807, -0.0791],\n",
      "        [-0.7807, -0.0793],\n",
      "        [-0.7808, -0.0796]]) tensor([[-0.7809, -0.0798],\n",
      "        [-0.7809, -0.0800],\n",
      "        [-0.7810, -0.0802],\n",
      "        [-0.7810, -0.0804],\n",
      "        [-0.7811, -0.0807],\n",
      "        [-0.7812, -0.0809],\n",
      "        [-0.7812, -0.0812],\n",
      "        [-0.7813, -0.0815],\n",
      "        [-0.7814, -0.0818]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.9303,  1.0487],\n",
      "        [-0.9304,  1.0488],\n",
      "        [-0.9305,  1.0488],\n",
      "        [-0.9306,  1.0489],\n",
      "        [-0.9307,  1.0490],\n",
      "        [-0.9308,  1.0491],\n",
      "        [-0.9309,  1.0491],\n",
      "        [-0.9310,  1.0492],\n",
      "        [-0.9311,  1.0493]]) tensor([[-0.9571,  1.0620],\n",
      "        [-0.9572,  1.0621],\n",
      "        [-0.9573,  1.0622],\n",
      "        [-0.9574,  1.0622],\n",
      "        [-0.9575,  1.0623],\n",
      "        [-0.9576,  1.0624],\n",
      "        [-0.9577,  1.0625],\n",
      "        [-0.9578,  1.0626],\n",
      "        [-0.9580,  1.0627]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.4405,  0.1986],\n",
      "        [-0.4403,  0.1995],\n",
      "        [-0.4401,  0.2004],\n",
      "        [-0.4399,  0.2013],\n",
      "        [-0.4396,  0.2022],\n",
      "        [-0.4394,  0.2030],\n",
      "        [-0.4392,  0.2039],\n",
      "        [-0.4390,  0.2047],\n",
      "        [-0.4388,  0.2056]]) tensor([[-0.4355,  0.1923],\n",
      "        [-0.4353,  0.1932],\n",
      "        [-0.4350,  0.1941],\n",
      "        [-0.4348,  0.1950],\n",
      "        [-0.4346,  0.1959],\n",
      "        [-0.4344,  0.1967],\n",
      "        [-0.4342,  0.1975],\n",
      "        [-0.4339,  0.1984],\n",
      "        [-0.4337,  0.1992]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 1.3955, -1.4146],\n",
      "        [ 1.3956, -1.4147],\n",
      "        [ 1.3957, -1.4148],\n",
      "        [ 1.3959, -1.4148],\n",
      "        [ 1.3960, -1.4149],\n",
      "        [ 1.3961, -1.4149],\n",
      "        [ 1.3961, -1.4150],\n",
      "        [ 1.3962, -1.4150],\n",
      "        [ 1.3963, -1.4150]]) tensor([[ 1.4981, -1.5026],\n",
      "        [ 1.4983, -1.5027],\n",
      "        [ 1.4984, -1.5027],\n",
      "        [ 1.4986, -1.5028],\n",
      "        [ 1.4987, -1.5029],\n",
      "        [ 1.4988, -1.5029],\n",
      "        [ 1.4989, -1.5030],\n",
      "        [ 1.4990, -1.5031],\n",
      "        [ 1.4991, -1.5031]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.7437,  2.7918],\n",
      "        [-0.7433,  2.7905],\n",
      "        [-0.7429,  2.7892],\n",
      "        [-0.7426,  2.7879],\n",
      "        [-0.7422,  2.7866],\n",
      "        [-0.7417,  2.7853],\n",
      "        [-0.7412,  2.7840],\n",
      "        [-0.7407,  2.7828],\n",
      "        [-0.7401,  2.7816]]) tensor([[-0.8537,  3.1593],\n",
      "        [-0.8533,  3.1577],\n",
      "        [-0.8529,  3.1561],\n",
      "        [-0.8524,  3.1544],\n",
      "        [-0.8520,  3.1527],\n",
      "        [-0.8515,  3.1510],\n",
      "        [-0.8511,  3.1494],\n",
      "        [-0.8505,  3.1478],\n",
      "        [-0.8500,  3.1462]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.7191,  0.0761],\n",
      "        [-0.7191,  0.0761],\n",
      "        [-0.7191,  0.0761],\n",
      "        [-0.7191,  0.0761],\n",
      "        [-0.7191,  0.0761],\n",
      "        [-0.7192,  0.0761],\n",
      "        [-0.7192,  0.0761],\n",
      "        [-0.7192,  0.0761],\n",
      "        [-0.7192,  0.0761]]) tensor([[-0.7182,  0.0728],\n",
      "        [-0.7182,  0.0728],\n",
      "        [-0.7182,  0.0728],\n",
      "        [-0.7182,  0.0728],\n",
      "        [-0.7182,  0.0728],\n",
      "        [-0.7182,  0.0728],\n",
      "        [-0.7183,  0.0728],\n",
      "        [-0.7183,  0.0728],\n",
      "        [-0.7183,  0.0728]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 1.8780, -0.9495],\n",
      "        [ 1.8777, -0.9505],\n",
      "        [ 1.8774, -0.9515],\n",
      "        [ 1.8771, -0.9525],\n",
      "        [ 1.8767, -0.9535],\n",
      "        [ 1.8764, -0.9545],\n",
      "        [ 1.8761, -0.9555],\n",
      "        [ 1.8758, -0.9566],\n",
      "        [ 1.8755, -0.9576]]) tensor([[ 2.0249, -1.0523],\n",
      "        [ 2.0246, -1.0533],\n",
      "        [ 2.0243, -1.0543],\n",
      "        [ 2.0239, -1.0553],\n",
      "        [ 2.0236, -1.0562],\n",
      "        [ 2.0233, -1.0572],\n",
      "        [ 2.0230, -1.0582],\n",
      "        [ 2.0227, -1.0592],\n",
      "        [ 2.0225, -1.0603]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 1.3886, -1.4113],\n",
      "        [ 1.3890, -1.4115],\n",
      "        [ 1.3894, -1.4117],\n",
      "        [ 1.3898, -1.4118],\n",
      "        [ 1.3902, -1.4120],\n",
      "        [ 1.3905, -1.4122],\n",
      "        [ 1.3909, -1.4123],\n",
      "        [ 1.3913, -1.4125],\n",
      "        [ 1.3916, -1.4127]]) tensor([[ 1.4895, -1.4984],\n",
      "        [ 1.4900, -1.4986],\n",
      "        [ 1.4904, -1.4988],\n",
      "        [ 1.4909, -1.4990],\n",
      "        [ 1.4913, -1.4992],\n",
      "        [ 1.4918, -1.4994],\n",
      "        [ 1.4922, -1.4996],\n",
      "        [ 1.4927, -1.4998],\n",
      "        [ 1.4931, -1.5000]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.2122, -0.6324],\n",
      "        [-0.2122, -0.6324],\n",
      "        [-0.2122, -0.6324],\n",
      "        [-0.2122, -0.6324],\n",
      "        [-0.2122, -0.6325],\n",
      "        [-0.2122, -0.6325],\n",
      "        [-0.2122, -0.6325],\n",
      "        [-0.2122, -0.6324],\n",
      "        [-0.2122, -0.6324]]) tensor([[-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 1.6322, -0.7112],\n",
      "        [ 1.6322, -0.7113],\n",
      "        [ 1.6322, -0.7113],\n",
      "        [ 1.6322, -0.7114],\n",
      "        [ 1.6322, -0.7114],\n",
      "        [ 1.6321, -0.7114],\n",
      "        [ 1.6321, -0.7115],\n",
      "        [ 1.6321, -0.7115],\n",
      "        [ 1.6321, -0.7115]]) tensor([[ 1.7187, -0.7804],\n",
      "        [ 1.7187, -0.7804],\n",
      "        [ 1.7187, -0.7804],\n",
      "        [ 1.7187, -0.7805],\n",
      "        [ 1.7186, -0.7805],\n",
      "        [ 1.7186, -0.7805],\n",
      "        [ 1.7186, -0.7806],\n",
      "        [ 1.7186, -0.7806],\n",
      "        [ 1.7186, -0.7806]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 1.2708, -0.5980],\n",
      "        [ 1.2699, -0.5978],\n",
      "        [ 1.2690, -0.5976],\n",
      "        [ 1.2682, -0.5974],\n",
      "        [ 1.2673, -0.5972],\n",
      "        [ 1.2664, -0.5970],\n",
      "        [ 1.2655, -0.5968],\n",
      "        [ 1.2646, -0.5966],\n",
      "        [ 1.2637, -0.5964]]) tensor([[ 1.3098, -0.6384],\n",
      "        [ 1.3088, -0.6382],\n",
      "        [ 1.3079, -0.6379],\n",
      "        [ 1.3069, -0.6377],\n",
      "        [ 1.3059, -0.6374],\n",
      "        [ 1.3050, -0.6372],\n",
      "        [ 1.3040, -0.6369],\n",
      "        [ 1.3030, -0.6367],\n",
      "        [ 1.3020, -0.6364]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\Load_Argo2_Public.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000018?line=3'>4</a>\u001b[0m inp \u001b[39m=\u001b[39m (inp \u001b[39m-\u001b[39m austin_mean) \u001b[39m/\u001b[39m austin_std\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000018?line=4'>5</a>\u001b[0m inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000018?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m sample(inp, lstm)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000018?line=7'>8</a>\u001b[0m output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(output)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000018?line=8'>9</a>\u001b[0m output \u001b[39m=\u001b[39m output \u001b[39m*\u001b[39m austin_std \u001b[39m+\u001b[39m austin_mean     \n",
      "\u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\Load_Argo2_Public.ipynb Cell 16'\u001b[0m in \u001b[0;36msample\u001b[1;34m(x, model, steps)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000016?line=3'>4</a>\u001b[0m arr \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000016?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000016?line=5'>6</a>\u001b[0m     output, h, c \u001b[39m=\u001b[39m model(x, h, c)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000016?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000016?line=7'>8</a>\u001b[0m         \u001b[39mprint\u001b[39m(x[\u001b[39m0\u001b[39m][\u001b[39m40\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], output[\u001b[39m0\u001b[39m][\u001b[39m40\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\Load_Argo2_Public.ipynb Cell 10'\u001b[0m in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x, h, c)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, h\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, c\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m (h \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (c \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=17'>18</a>\u001b[0m         x, (h, c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h, c))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=18'>19</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=19'>20</a>\u001b[0m         x, (h, c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(x)\n",
      "File \u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=758'>759</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=759'>760</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=760'>761</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=761'>762</a>\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=762'>763</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=763'>764</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=764'>765</a>\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, inp in enumerate(test_loader):\n",
    "    original_inp = inp\n",
    "    inp = inp.to(device)\n",
    "    inp = (inp - austin_mean) / austin_std\n",
    "    inp = inp.float()\n",
    "    output = sample(inp, lstm)\n",
    "\n",
    "    output = np.array(output)\n",
    "    output = output * austin_std + austin_mean     \n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.scatter(original_inp[0][:,0], original_inp[0][:,1], c='b', s=20)\n",
    "    plt.scatter(output[:,0], output[:,1], c='r', s=20)\n",
    "    plt.savefig(f\"outputs/austin_prediction_{i}.png\")\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e29fbf03db329727dad78c9f26053fabfa9b2ead348090a9805a8744123bd7f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
