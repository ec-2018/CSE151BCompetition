{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "\n",
    "        trajectories = np.concatenate((inputs, outputs), axis=1)\n",
    "        trajectories = trajectories.reshape(-1, trajectories.shape[1], trajectories.shape[2])\n",
    "        trajectories = trajectories.astype(np.float32)\n",
    "\n",
    "        if normalized:\n",
    "            trajectories = (trajectories - trajectories.mean(axis=0)) / trajectories.std(axis=0)\n",
    "\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        for trajectory in trajectories:\n",
    "            for i in range(trajectory.shape[0] - 50):\n",
    "                inputs.append(trajectory[i:i+50])\n",
    "                outputs.append(trajectory[i+50])\n",
    "\n",
    "    inputs, outputs = np.asarray(inputs), np.asarray(outputs)\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize each dataset\n",
    "train_austin = ArgoverseDataset(city = \"austin\", split = \"train\")\n",
    "train_miami = ArgoverseDataset(city = \"miami\", split = \"train\")\n",
    "train_palo_alto = ArgoverseDataset(city = \"palo-alto\", split = \"train\")\n",
    "train_pittsburgh = ArgoverseDataset(city = \"pittsburgh\", split = \"train\")\n",
    "train_dearborn = ArgoverseDataset(city = \"dearborn\", split = \"train\")\n",
    "train_washington_dc = ArgoverseDataset(city = \"washington-dc\", split = \"train\")\n",
    "test_austin = ArgoverseDataset(city = \"austin\", split = \"test\")\n",
    "test_miami = ArgoverseDataset(city = \"miami\", split = \"test\")\n",
    "test_palo_alto = ArgoverseDataset(city = \"palo-alto\", split = \"test\")\n",
    "test_pittsburgh = ArgoverseDataset(city = \"pittsburgh\", split = \"test\")\n",
    "test_dearborn = ArgoverseDataset(city = \"dearborn\", split = \"test\")\n",
    "test_washington_dc = ArgoverseDataset(city = \"washington-dc\", split = \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 10  # batch size \n",
    "train_loader = DataLoader(train_palo_alto,batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7971, -0.7893],\n",
      "        [ 0.7971, -0.7893],\n",
      "        [ 0.7972, -0.7893],\n",
      "        [ 0.7972, -0.7894],\n",
      "        [ 0.7973, -0.7894],\n",
      "        [ 0.7974, -0.7894],\n",
      "        [ 0.7974, -0.7894],\n",
      "        [ 0.7975, -0.7895],\n",
      "        [ 0.7976, -0.7895],\n",
      "        [ 0.7976, -0.7895],\n",
      "        [ 0.7977, -0.7896],\n",
      "        [ 0.7977, -0.7896],\n",
      "        [ 0.7978, -0.7896],\n",
      "        [ 0.7978, -0.7896],\n",
      "        [ 0.7978, -0.7896],\n",
      "        [ 0.7979, -0.7896],\n",
      "        [ 0.7979, -0.7896],\n",
      "        [ 0.7979, -0.7896],\n",
      "        [ 0.7979, -0.7896],\n",
      "        [ 0.7979, -0.7896],\n",
      "        [ 0.7979, -0.7896],\n",
      "        [ 0.7979, -0.7896],\n",
      "        [ 0.7979, -0.7896],\n",
      "        [ 0.7980, -0.7896],\n",
      "        [ 0.7980, -0.7896],\n",
      "        [ 0.7981, -0.7897],\n",
      "        [ 0.7982, -0.7897],\n",
      "        [ 0.7983, -0.7897],\n",
      "        [ 0.7984, -0.7898],\n",
      "        [ 0.7985, -0.7899],\n",
      "        [ 0.7986, -0.7899],\n",
      "        [ 0.7988, -0.7900],\n",
      "        [ 0.7989, -0.7901],\n",
      "        [ 0.7991, -0.7902],\n",
      "        [ 0.7993, -0.7903],\n",
      "        [ 0.7995, -0.7905],\n",
      "        [ 0.7997, -0.7906],\n",
      "        [ 0.7999, -0.7907],\n",
      "        [ 0.8002, -0.7909],\n",
      "        [ 0.8004, -0.7910],\n",
      "        [ 0.8007, -0.7912],\n",
      "        [ 0.8010, -0.7913],\n",
      "        [ 0.8013, -0.7915],\n",
      "        [ 0.8016, -0.7917],\n",
      "        [ 0.8019, -0.7919],\n",
      "        [ 0.8022, -0.7921],\n",
      "        [ 0.8026, -0.7923],\n",
      "        [ 0.8030, -0.7926],\n",
      "        [ 0.8034, -0.7928],\n",
      "        [ 0.8038, -0.7931]]) tensor([ 0.8043, -0.7934])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x216 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "        axs[i].scatter(out[i,0], out[i,1], c='r')\n",
    "    \n",
    "\n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    if i_batch != 50: continue\n",
    "    inputs, outputs = sample_batch\n",
    "    show_sample_batch(sample_batch)\n",
    "    print(inputs[0], outputs[0])\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a53b5",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "56c6a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=2, hidden_size=512, num_layers=3, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(512, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(256, 2)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        if (h is not None) and (c is not None):\n",
    "            x, (h, c) = self.lstm(x, (h, c))\n",
    "        else:\n",
    "            x, (h, c) = self.lstm(x)\n",
    "        print(x.shape)\n",
    "        print(h.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        return x, h, c\n",
    "\n",
    "    def predict(self, x, h=None, c=None):\n",
    "        output, h, c = self.forward(x, h, c)\n",
    "        output = output[-1]\n",
    "        output = output.cpu().detach().numpy()\n",
    "            \n",
    "        return output, h, c\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf38ed7",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5e605e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "Epoch: 0/60, Batch: 0/71958, Loss: 1511228.5\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "Epoch: 0/60, Batch: 10/71958, Loss: 1024332.1875\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "Epoch: 0/60, Batch: 20/71958, Loss: 5177995.0\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "Epoch: 0/60, Batch: 30/71958, Loss: 1360072.125\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "Epoch: 0/60, Batch: 40/71958, Loss: 350503.5\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n",
      "torch.Size([10, 50, 512])\n",
      "torch.Size([3, 10, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ec201\\Desktop\\CSE151BCompetition\\Load_Argo2_Public.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000013?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000013?line=19'>20</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ec201/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000013?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000013?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000013?line=22'>23</a>\u001b[0m \u001b[39mif\u001b[39;00m i_batch \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ec201\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\ec201\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "lstm = LSTM()\n",
    "lstm.to(device)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lstm.train(True)\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        inp, label = sample_batch\n",
    "        \n",
    "        inp = inp.to(device)\n",
    "        output, h, c = lstm(inp.float())\n",
    "\n",
    "        output = output[:, -1, :]\n",
    "\n",
    "        loss = loss_fn(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}/{num_epochs}, Batch: {i_batch}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    lstm.train(False)\n",
    "    for i_batch, sample_batch in enumerate(test_loader):\n",
    "        inp, label = sample_batch\n",
    "        inp = inp.to(device)\n",
    "        output, h, c = lstm(inp)\n",
    "        loss = loss_fn(output, label)\n",
    "        print(f\"Epoch: {epoch}/{num_epochs}, Batch: {i_batch}/{len(test_loader)}, Loss: {loss.item()}\")\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dfaea13b2c70b10e61fc5114159871c988fdb8a8d57529e1e6239db333764196"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
