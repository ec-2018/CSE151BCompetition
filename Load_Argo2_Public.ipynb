{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "    inputs = pickle.load(open(f_in, \"rb\"))\n",
    "    inputs = np.asarray(inputs)\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)\n",
    "\n",
    "        trajectories = np.concatenate((inputs, outputs), axis=1)\n",
    "        trajectories = trajectories.reshape(-1, trajectories.shape[1], trajectories.shape[2])\n",
    "        trajectories = trajectories.astype(np.float32)\n",
    "\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        for trajectory in trajectories:\n",
    "            for i in range(trajectory.shape[0] - 50):\n",
    "                inputs.append(trajectory[i:i+50])\n",
    "                outputs.append(trajectory[i+50])\n",
    "\n",
    "    inputs, outputs = np.asarray(inputs), np.asarray(outputs)\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = (self.inputs[idx], self.outputs[idx])\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# intialize each dataset\n",
    "train_austin = ArgoverseDataset(city = \"austin\", split = \"train\")\n",
    "train_miami = ArgoverseDataset(city = \"miami\", split = \"train\")\n",
    "train_palo_alto = ArgoverseDataset(city = \"palo-alto\", split = \"train\")\n",
    "train_pittsburgh = ArgoverseDataset(city = \"pittsburgh\", split = \"train\")\n",
    "train_dearborn = ArgoverseDataset(city = \"dearborn\", split = \"train\")\n",
    "train_washington_dc = ArgoverseDataset(city = \"washington-dc\", split = \"train\")\n",
    "test_austin = ArgoverseDataset(city = \"austin\", split = \"test\")\n",
    "test_miami = ArgoverseDataset(city = \"miami\", split = \"test\")\n",
    "test_palo_alto = ArgoverseDataset(city = \"palo-alto\", split = \"test\")\n",
    "test_pittsburgh = ArgoverseDataset(city = \"pittsburgh\", split = \"test\")\n",
    "test_dearborn = ArgoverseDataset(city = \"dearborn\", split = \"test\")\n",
    "test_washington_dc = ArgoverseDataset(city = \"washington-dc\", split = \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 10  # batch size \n",
    "train_loader = DataLoader(train_palo_alto,batch_size=batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 50, 2]) torch.Size([10, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1YAAAC0CAYAAACXOL1/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZBUlEQVR4nO3dsa9kZ33H4XO9viZrJGsVLSLCEmyHkGUhYNtEIg2pLERBg5KSig4hkc4uEEpWiJ7eDUV0hav9A9IhrlDkYrsFca0UlgwRWlss9k2xWcX23pm5Z+YzM+fMPE8TsTcho++873ve3++8c87J5eXl5QAAAMDantv3BwAAAJg7hRUAAMCGFFYAAAAbUlgBAABsSGEFAACwoefH/C/fvn17uHPnzpY+ynF5+PChLEPybMmz8/Dhw2EYBnlGjM2WPFvy7MiyJc/Ww4cPh3ffffeZfx9VWN25c2f49a9/nX2oY3b37l1ZhuTZkmfn7t27wzAM8owYmy15tuTZkWVLnq2n1/ZPcxQQAABgQworAACADSmsAAAANqSw2oU33xyGO3eG4bnnnvzPN9/c9yeaL1m25NmSZ0eWLXm25NmRZUuerZF5jnp4BWt4881h+P73h+HRoyf/+Xe/e/Kfv/KV/X6uOVqUJeuRZ2tRnp///DD87d/u97PNjbHZkmdLnh1ZtuTZWiPPk8vLy8vr/vd7osga7tx58kV8yt1vfEOWYy3IcvjSl4a7t2/Lcyx5thbN9RdeGIZXX5XnGMZmS54teXZk2ZJna408HQXcorPzi+Gj3/1+3x/jYFz+fkGWi/6dhZaOTXmOtjTPv/xltx/mAJjrHXO9Jc+Wud6SZ2fdua6w2pKz84vhX//jv4Z3Xrq9749yEM7OL4Z3Xvrc1X/84hd3+2FmbuXYlOcoK/N84YXdfqCZM9c75npLni1zvSXPziZzXWG1JffuPxjef/zh8O//8C/Do+c/88k/vvjifj7UjN27/2D4t7//52eyfP/0M8Pwk5/s6VPN08qxKc9RVub58sv7+WAzZa53zPWWPFvmekuenU3musJqC87OL4aLP74/DMMw/OqVbw4//qcfDH946XPDR8PJ8IeXPjcMv/jFnj/hvDzN86osf/ytHwzD97637484KyvHpjyv7Vpz3YMrrs1cb5nrLXl2zPWWPFubzHVPBYw9vX34cb965ZvDr1755jAMw/DyrZvDf37vH4fh5z/fx8ebnU/n+fEsh+FJnlzf2fnFcDIMw9Mn1lw5NrkWc71lrrfM9ZY8O+Z6S56tTee6O1axp7cPr3Lz9Mbwo299ecefaN7k2bp3/8Fw1WNAT4ZBliMZmy15tsz1ljw75npLnq1N57rCKvb09uFVfvqdV4dvf83vLcaQZ+fjx9Y+7XIYZDmSsdmSZ8dcb8mzZa53lo3NYZDnWMVcV1iFnt4+vMrLt24a3CPJs3PVsbWPc1RgHGOzJc+Oud6SZ8tc71xnbMrz+qq5rrAKOSrQkmfHUYGWsdmSZ8dcb8mzZa53jM1WlafCKuKoQEueLUcFOsZmS54tc73jmFXLXG8Zm51yriusAo4KtOTZcvSiY2y25Nky1zuOWbXM9Za53qnnusIq4HZsS54tRy86xmZLni1zvWNstuTZMtc79dhUWAXcju04etFy9KJlbLbk2THXW8ZmS54dc731Tjw2FVYbcju24+hFy9GLlrnekmfHXG8Zmy15dsz13q0XT6/893XHpsJqQ27HdhwVaMmzZa635Nkx11vGZkueHXO9dXZ+Mfz5g78+8++nN07WzlJhtQG3Y1uOCrTk2THXW/JsmesdY7Mlz1Z9bO3Y3bv/YHj80bNl/2dfeH7tLBVWa3I7tuWoQEueHXO9Jc+Wud4xNlvy7NXH1o7ZsqL/T+8/Xvu/V2G1JrdjW44KtOTZMddb8myZ6x1jsyXP1jaOrR2rVUX/FzYo+hVWa3L0ouOoQEueLXO9Jc+Oud4yNjue8NvbxrG1Y7XNol9htQZHLzqOCrTk2TLXW/LsmOstY7PjCb+9bR1bO1bbLPoVVmtw9KLjqEBLni1zvSXPjrneMjY7xmZrm8fWjtG2mygKq5EcvWg5KtCSZ8dcb8mzZa53jM2WsdlSqLa23URRWI3g6EXL0YuWPDvmekueLXO9Y2y2jM2W36q1dtFEUViNoGvQcvSiJc+Oud6SZ8tc7xibLWOz47dqrV01URRWI+gadBy9aMmzZa635Nkx11vGZsfYbCn6W7vKU2F1TW5vdxy9aMmzZa635Nkx11vGZsfY7Cn6W7vKU2F1TW5vd3RhWvJsmesteXbM9Zax2TE2W4r+1i7zVFhdg9vbLV2Yljw75npLni1zvWNstozNlqK/tcs8FVYruL3d0oVpybNjrrfk2TLXO8Zmy9hsKfpbu85TYbWC29stXZiWPDvmekueLXO9Y2y2jM2Oor+1jzwVViu4vd3RhWnJs2Wut+TZMddbxmbH2Gwp+lv7yFNhtYTb2x1dmJY8W+Z6S54dc71lbHaMzZ6iv7OvlysrrJZwe7ujC9OSZ8tcb8mzY663jM2OsdlS9Hf2+XJlhdUCbm+3dGFa8uyY6y15tsz1jrHZMjZbiv7OPot+hdUV3N5u6cK05Nkx11vybJnrHWOzZWy2FP2tfRb9CqsruL3d0oXpnJ1fDD/85W/lGXnjrbfN9ZC1s2Xt7BibLWOzo+hv7bvoV1hdwe3tji5M5+ni++HlVZczeY51dn4xvPfo8cK/m+vjWTs71s6WsdkxNluK/ta+i36F1afsu9I9JLowrWWL7zDIc6x79x8s/Ju5Pp61s2PtbBmbHWOzp+jvTKHoV1h9yr4r3UPimFVr2eIrz3FWPYZVluM4otrSwW65rneMzZaivzOVol9h9TFTqHQPhWNWrWWL742TE3mOsGrxvXXzVJYjOKLa08HuuK63jM2OhlRrKkW/wur/TKXSPRSOWbWWdVx/9t2vynOEVYvv66+9suNPNG+OqLZ0sDuu6y1js6Mh1ZtK0f/8Tv6/TNzTrsGiAe729jiOWbV0XFtTWXwPhSOqLcfWOlPpYB8KY7OjIdV6WvRfNT53XfQf/R2rVV2DYbDZGsMxq5aOa0vHteWIaksTpaWJ0jE2WxpSrSkV/UdfWF2na2DBuD7HrFo6rq0pLb6HwBHVjiZKSxOlY2y2NKR6Uyr6j76w0jVo6RC25NnRcW3Js+Upqi1NlI4GX0tDqrWqibJrR11Y6Rq0dAhb8uzouLbk2fIU1Zaiv6XB1zI2W1Nrohx1YaVr0Jra4J47eXZ0XFvybHmKakfR39Lga03t7srcTbGJcrSF1RS/jDmTZ0ueLR3Xljw7nqLacqSypcHXkmdnqk2UoyyspvplzJU8W/Js6bi25NnxFNWWI5UtDb6WPFtTPTlxlIXVVL+MuZJnS54tHcKWPDueotpypLKjwdeSZ2+qJyeOsrCa6pcxV/JsybOjQ9iSZ8tc7zhS2dLga8mzNeWTE0dXWE35y5gjebbk2dEhbMmzZa53HKnsKfpb8mxN+eTE0RVWU/4y5kienbPzi+GHv/ytPCN+xN7ScW1ZOzuOVLYU/S15tqZ+cuKoCqupfxlzI8/O047rh5dXbbXkOZYfsfd0XDvWzpax2VL0t+TZmcPJiaMprObwZcyJPFvLOq7DIM+x/Ii9pePasXa2jM2eor+jidKaw0mUoymsHGNpybO1rOMqz3H8iL3liGrL2tlyN6DlBbYdTZTWXE6iHE1h5ahAS56dZReyGycn8hzBj9hbjqj2rJ0tdwM6migtTZTWXE6iHEVh5ahAS56tZR3Xn333q/IcwY/YW46otqydLXdXOpooPU2UzpxOohxFYeWoQEdHq+X8dcuFrOWIasfa2XNt72iitDRROnM7iXLwhZWNa0dHq+X8dcuFrOWIasfa2XNtb2mitBT9nbmdRDnowsrGtaWj1XL+uuVC1nJEtWPtbLm2tzRRWor+1txOohx0YWXj2tLRas1tsZgyF7KWPFvWzpZre0sTpaPob83xJMpBF1Y2rh0drdYcF4upciFrybNl7ey5tnc0UVpzeM/SnMzxJMrBFlY2ri0drdYcF4up0r1uybNl7Wy5tnc0UVpzec/SXMy16D/YwsrGtTPXwT1V8mzpXrfk2THXW56s2NJEac3lPUtzMOei/yALKxezzpwH9xTJs6V73ZJnx1xvebJiTxOlM6f3LM3BnIv+gyusXMxacx7cUyTPljvTLXl2zPWWJyu2NFE6c3vP0hzMueg/uMLKxayzqgMz9cE9NfJsuTPdkmfLXG95smJLE6Uzt/csTd3ci/6DKqxsXDvXufMny+uTZ8ud6ZY8W3PfGEyNJyu2NFFa9p2dQ/gd5cEUVjauLXf+WvJsybMlz5a7AS1PVuxoorQ0UTqH8jvKgymsbAxaOjAtebbk2ZJnx92Aljxb3rPU0kTpHMrvKA+msLIx6OjAtOTZkmdLnh13A1rybHnPUkvR3zqU31EeRGFlY9DSgWnJs3MI56+nRJ4tJyda8mx5z1JH0d86pN9RHkRhZePa0YFpybNzKOevp0KePScnWvLseM9SS9HfOqTfUc6+sLJx7ejAtOTZOpTz11Mhz5aTEy15drxnqafo7xzaPn7WhZWNa0sHpiXP1qGcv54KeXYcqew5idLxnqWWor9ziPv4WRdWNq4tHZiWPDuHdP56CuTZcaSyd2gd7H1zLepoorQOcR8/68LKYtHRgWnJs3VI56+nQJ4dRypbh9jB3ifXoo4mSu8Q9/GzLawsFi3HLjo6Wi3d65Y8W45Utg6xg71Pru0dTZTWoe7jZ1tYWSw6NlodHa2W7nVLni1HKlurnlwnz3Fc21uaKK1D3cfPsrCyWHRstFo6Wi3d65Y8W45Udq5zLZLn9bm2tzRRWoe8j59dYWWxaNlotXS0WrrXLXl2DnljsA9vvPW2a1HItb2lidI59H387Aori0XLRqujo9U61PPX+yLPzqFvDHbt7PxieO/R44V/t3aO59re0URpHfo+fnaFlcWiY6PV0tFqHer5632RZ+fQNwa7du/+g4V/cy0az7W9o4nSO/R9/KwKK4tFy0aro6PVkmdLnq1D3xjs0qoHVrgWjefa3tFEaR3DPn5WhZXFomOj1dHRasmzJc/WMWwMdmXV2Lx181SeI7m2tzRROsfyKprZFFYWi46NVktHqyXPljxbGnydVWPz9dde2fEnmjfX9pYmSueYXkUzi8LKYtGy0WrpaLXk2ZJnR4OvZWy2XNtbmiidY3oVzSwKK4tFy8Wso6PVkmdLnh0Nvpax2XNt72iitI7pVTSzKKwsFh0Xs5aOVkueLXl2NPhaxmbLtb2jidI6tlfRTL6wsli0XMw6Olotebbk2dLg6xibrWN5KMCuaKK0ju1VNJMvrBQCHRezjo5WS54tebY0+DrGZuuYHgqwK5oonWPcd066sDrGL2RbXMxaOlotebbk2dLg6xibrWN6KMAuaKJ0jnXfOdnC6li/kG1xMWvpaLXk2ZJnR4OvZWy2jumhALugidI51n3nZAurY/1CtsXFrKOj1ZJnS54dDb6Wsdk6tocCbJsmSutY952TLKyWDe5hOOwvZBtczFo6Wi15tuTZ0eBrGZutY3sowDZporSOed85ucLqOoP7kL+QbXAx6+hoteTZkmdLg69lbHbM9ZYmSuuY952TK6wM7pbFt6Oj1ZJnS56tY+64bsOqPLk+c72nidI59n3n5Aorg7tj8W0p+lvybL3x1tvyjHgvUO+YO9g1a2dLE6Vj3zmxwsrgbll8W4r+ljw7Z+cXw3uPHi/8uzyvz3uBesfewa5ZOzuaKC37zokVVjpaHQ8AaSn6W/Js3bv/YOHf5DmO9wK1dLBb1s6OJkrPvnNChZWOVscDQHqK/pY8O6uaKPIcx3uBWjrYLWtnRxOlpeh/YhKFlY5Wy4WspehvybOzau28dfNUniN4L1DLyYmWtbOlidJS9D8xicJKIdByIeso+lvybK1aO19/7ZUdf6J5816gjpMTLWtnSxOlpej/f5MorBQCHbdiW4r+ljw77ga0bAxa5npLni1NlI6i/5P2XlgpBFpuxXZsXHvybLgb0LIx6JnrLXl2NFFaiv5P2nthpRDoWCw6Nq49TZSOC1lLni1zvSXPjiZKT9H/SXstrBQCHYtFy0arp4nScSFrybNlrrfk2XFtbyn6n7W3wkoh0LJYtGy0WpooHReyljxb5npLni3X9o6XK19tb4WVQqBlsejYaLU0UVq61y15dsz1ljxbru0dL1debC+FlYcCtCwWLRutliZKR/e6Jc/WG2+9ba6HrJ0t1/aOlysvtvPCykMBehaLjo1WSxOlo3vdkmfr7PxieO/R44V/N9fHs3Z2XNtbXq682M4LKx2YlsWiY6PV0kRpWTtb8mzdu/9g4d/M9fGcROm4tre8XHm5nRdW7+jAZCwWLRutljxbutcteXZW3Zk218dzEqXjWtTycuXldl5Y3Xrx9Mp/14EZz2LRstFqybOje92SZ2dVg+/WzVN5juQkSsu1qGNsrrbTwurs/GL48wd/febfT2+cKALWYLHo2Gi15NnSvW7Js7Oqwff6a6/s+BPNm5MoLdeijrF5PTstrO7dfzA8/ujZy9lnX3je4B7JYtGy0WrJs7XoCLUO4Xg6ri0NvpaTKC3Xoo6xeT07K6yWXcz+9P7iJwlxNYtFy8a1Y+PaOju/GJ47ubqNokM4jo5rS4Ov5SmqLdeilrF5PTsprFZdzL7gYjaaQqBj49qxcW0tewmjDuF4Oq4tDb6Op6i2XItamijXt5PCysWspRDo2Li2zPXWojw90nY9Oq4ddwNa1s6WPFuaKNe3k8LKxayjEGjZuLbM9daiPD+6vJTlSDquHXcDetbOljw7mijjbL2wcjFrKQRaNq4dc721LE/Hp8fTce24G9Cydrbk2dFEGW/rhZWLWWdZ10AhMJ6Na8tcb8mz5XepnUVZDoO7Aesw11vy7GiijLfVwsrtw44HgPQsvh1zvSXPlt+ltm69eHrlv7sbMJ653pJny5HK8bZWWLl92NI1aFl8O+Z6S54tv0ttnZ1fDH/+4K/P/PvpjRNZjmSut+TZcqRyPVsrrBQCLV2DjsW3Za635Nnyu9TWvfsPhscfPVukfvaF52U5krnekmfLqZ71bK2wcga7o2vQsvi2FP0teXb8LrW1LM8/vf94x59m/sz1ljw7TvWsbyuF1arz7L6QcXQNOt5s31L0t+TZ8bvUljxb5npLnh2nejaTF1bOs/c8zarhzfY9RX9Lnh13plvybJnrLXl2zPXN5IWV8+wtT7PqWCx6iv6Ooxctd6Zb8uyY6y15tsz1zeSFlfPsHXf/Wn7311L0dxy9aDkW1JJnx1xvybNlrm8uLay8cLXl7l/Lu1c6iv6Wu6ktx4Ja8uyY6y15tsz1zaWFlS+k42lWLe9eaSn6Ox6o0nIsqCXPlrnesXa2zPVGVlj5QjqevtTz7pWOor/jgSotx4Ja8mw5ZtWxdrbM9U5SWPlCWm5tt7x7paPob5nrLXm25NlyqqdjbLbk2UkKK19Iy63tjkKgZa63PFClZe1sybPjVE/L2GzJs5MUVr6QjqMCLYVAy1xveaBKx9rZkmfHqZ6WsdmSZ2vjwsoX0nJUoKUQ6JjrLQ9UaVk7W/LsaPC1jM2WPFsbF1a+kJajAh2FQMtcb3mgSsvLqjuOrbU0+DrGZs/a2dqosDLAW6sKAcZRCHTM9ZYHqrS8rLrj2FpLg69jbPasnb21CysDvKcQ6CgEOuZ6ywNVWl5W3XJsreW63jE2W9bO7Vi7sHrjrbcN8JBCoKMQaLmYteTZ8rLqlidVthyz6hibLWvndqxVWJ2dXwzvPVp8XMUXMo5CoGXj2vL7gJY8W15W3Vl1LEie4zhm1fIU1c6yZr61czNrFVZvvPX2wr8Z4OO5+9eyce34fUBLnq1leTpSOY5jQS15tjxFteM4+naNLqxW3a0ywMdx969l49ry+4CWPFvy7DgW1JJny1NUO071bNfowure/QcL/3br5qkBPpK7f6033nrbRivkd38dv6NsybPlWFDHMauWp6i2nOrZrtGF1bIv5PXXXtnowxwbd/9ay/K00RrP4/87f3z02O8oQ36X2nKksrNqrstzHMfWWk71bN/owurGgh9ingw2rmO5+9dalqeN1niOWXX++38+cPQi5ChLy1zvmOstc71lrm/f6MLqqh9iDsNw5RfFcsseHeru33jL8rRgjOcxwZ3HH3608G+OXoznscstc71jrrfM9Za5vn2jC6tFnX93BMZbdAvb3ar1yLO1KE9zfbzTG1cvtY5erGfZ2JTneOZ6x1xvmestc337RhdWP/rWl4ebpzc+8W9ux65nUZbuVq1Hni1zvfN3L/2NLEPGZkueHXO9ZWy25Ll9owurb3/t5eGn33l1ePnWzeFkeFLluh27Hlm25NmSZ+fWi6eyDBmbLXl2zPWWsdmS5/Y9v87/0be/9rIvISLLljxb8uzIsiXPljw7smzJsyXP7Rp9xwoAAIBPUlgBAABs6OTycsHz069w+/bt4c6dO1v8OMfjN7/5zfD1r3993x/jYMizJc/Ow4cPh2EYrJ0RY7Mlz5Y8O7JsybP18OHD4d13333m30cVVgAAADzLUUAAAIANKawAAAA2pLACAADYkMIKAABgQworAACADSmsAAAANqSwAgAA2JDCCgAAYEMKKwAAgA39L41G4QhAdgCyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1080x216 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "        axs[i].scatter(out[i,0], out[i,1], c='r')\n",
    "    \n",
    "\n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inputs, outputs = sample_batch\n",
    "    show_sample_batch(sample_batch)\n",
    "    print(inputs.shape, outputs.shape)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a53b5",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "56c6a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=2, hidden_size=512, num_layers=3, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(512, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(256, 2)\n",
    "        self.leakyrelu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        if (h is not None) and (c is not None):\n",
    "            x, (h, c) = self.lstm(x, (h, c))\n",
    "        else:\n",
    "            x, (h, c) = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        return x, h, c\n",
    "\n",
    "    def predict(self, x, h=None, c=None):\n",
    "        output, h, c = self.forward(x, h, c)\n",
    "        output = output[-1]\n",
    "        output = output.cpu().detach().numpy()\n",
    "            \n",
    "        return output, h, c\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf38ed7",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5e605e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2]) torch.Size([10, 2])\n",
      "Epoch: 0/60, Batch: 0/71958, Loss: 1511233.625\n",
      "torch.Size([10, 2]) torch.Size([10, 2])\n",
      "torch.Size([10, 2]) torch.Size([10, 2])\n",
      "torch.Size([10, 2]) torch.Size([10, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ec201\\Desktop\\CSE151BCompetition\\Load_Argo2_Public.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000013?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000013?line=22'>23</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ec201/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000013?line=23'>24</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000013?line=24'>25</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000013?line=25'>26</a>\u001b[0m \u001b[39mif\u001b[39;00m i_batch \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ec201\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\ec201\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/Desktop/CSE151BCompetition/env/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "lstm = LSTM()\n",
    "lstm.to(device)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), learning_rate)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lstm.train(True)\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        inp, label = sample_batch\n",
    "        \n",
    "        inp = inp.to(device)\n",
    "        output, h, c = lstm(inp.float())\n",
    "\n",
    "        output = output[:, -1, :]\n",
    "\n",
    "        loss = loss_fn(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}/{num_epochs}, Batch: {i_batch}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    lstm.train(False)\n",
    "    for i_batch, sample_batch in enumerate(test_loader):\n",
    "        inp, label = sample_batch\n",
    "        inp = inp.to(device)\n",
    "        output, h, c = lstm(inp)\n",
    "        loss = loss_fn(output, label)\n",
    "        print(f\"Epoch: {epoch}/{num_epochs}, Batch: {i_batch}/{len(test_loader)}, Loss: {loss.item()}\")\n",
    "        break\n",
    "    break\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dfaea13b2c70b10e61fc5114159871c988fdb8a8d57529e1e6239db333764196"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
