{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "091abbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86082, 50, 2) (86082, 50, 2)\n",
      "(6325, 50, 2) ()\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "def get_data(city, split, type):\n",
    "    f = ROOT_PATH + split + \"/\" + city + \"_\" + type\n",
    "    data = pickle.load(open(f, \"rb\"))\n",
    "    data = np.asarray(data)\n",
    "    return data\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    if city != \"all\":\n",
    "        inputs = get_data(city, split, \"inputs\")\n",
    "    else:\n",
    "        inputs = []\n",
    "        for place in cities:\n",
    "            inputs.append(get_data(place, split, \"inputs\"))\n",
    "        inputs = np.concatenate(inputs, axis=0)\n",
    "    \n",
    "    outputs = None\n",
    "    mean, std = None, None\n",
    "\n",
    "    if split==\"train\":\n",
    "        if city != \"all\":\n",
    "            outputs = get_data(city, split, \"outputs\")\n",
    "        else:\n",
    "            outputs = []\n",
    "            for place in cities:\n",
    "                outputs.append(get_data(place, split, \"outputs\"))\n",
    "            outputs = np.concatenate(outputs, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        trajectories = np.concatenate((inputs, outputs), axis=1)\n",
    "        trajectories = trajectories.reshape(-1, trajectories.shape[1], trajectories.shape[2])\n",
    "        trajectories = trajectories.astype(np.float32)\n",
    "\n",
    "        if normalized:\n",
    "            mean = np.mean(trajectories, axis=0)\n",
    "            std = np.std(trajectories, axis=0)\n",
    "            trajectories = (trajectories - mean) / std\n",
    "            mean = mean[0]\n",
    "            std = std[0]\n",
    "\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        for trajectory in trajectories:\n",
    "            for i in range(0, trajectory.shape[0] - 55, 50):\n",
    "                inputs.append(trajectory[i:i+50])\n",
    "                outputs.append(trajectory[i+1:i+51])\n",
    "\n",
    "    inputs, outputs = np.asarray(inputs), np.asarray(outputs)\n",
    "    print(inputs.shape, outputs.shape)\n",
    "    return inputs, outputs, mean, std\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.city, self.split = city, split\n",
    "\n",
    "        self.inputs, self.outputs, self.mean, self.std = get_city_trajectories(city=city, split=split, normalized=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            data = (self.inputs[idx], self.outputs[idx])\n",
    "        else:\n",
    "            data = (self.inputs[idx])\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def get_mean_std(self):\n",
    "        return self.mean, self.std\n",
    "\n",
    "# intialize each dataset\n",
    "# train_dataset = ArgoverseDataset(city=\"all\", split=\"train\")\n",
    "train_austin = ArgoverseDataset(city = \"austin\", split = \"train\")\n",
    "# train_miami = ArgoverseDataset(city = \"miami\", split = \"train\")\n",
    "# train_palo_alto = ArgoverseDataset(city = \"palo-alto\", split = \"train\")\n",
    "# train_pittsburgh = ArgoverseDataset(city = \"pittsburgh\", split = \"train\")\n",
    "# train_dearborn = ArgoverseDataset(city = \"dearborn\", split = \"train\")\n",
    "# train_washington_dc = ArgoverseDataset(city = \"washington-dc\", split = \"train\")\n",
    "\n",
    "# data_mean, data_std = train_dataset.get_mean_std()\n",
    "austin_mean, austin_std = train_austin.get_mean_std()\n",
    "# miami_mean, miami_std = train_miami.get_mean_std()\n",
    "# palo_alto_mean, palo_alto_std = train_palo_alto.get_mean_std()\n",
    "# pittsburgh_mean, pittsburgh_std = train_pittsburgh.get_mean_std()\n",
    "# dearborn_mean, dearborn_std = train_dearborn.get_mean_std()\n",
    "# washington_dc_mean, washington_dc_std = train_washington_dc.get_mean_std()\n",
    "\n",
    "# test_dataset = ArgoverseDataset(city=\"all\", split=\"test\")\n",
    "test_austin = ArgoverseDataset(city = \"austin\", split = \"test\")\n",
    "# test_miami = ArgoverseDataset(city = \"miami\", split = \"test\")\n",
    "# test_palo_alto = ArgoverseDataset(city = \"palo-alto\", split = \"test\")\n",
    "# test_pittsburgh = ArgoverseDataset(city = \"pittsburgh\", split = \"test\")\n",
    "# test_dearborn = ArgoverseDataset(city = \"dearborn\", split = \"test\")\n",
    "# test_washington_dc = ArgoverseDataset(city = \"washington-dc\", split = \"test\")\n",
    "\n",
    "# Train Validation Split\n",
    "train_size = int(0.8 * len(train_austin))\n",
    "valid_size = len(train_austin) - train_size\n",
    "# train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [train_size, valid_size])\n",
    "train_austin, val_austin = torch.utils.data.random_split(train_austin, [train_size, valid_size])\n",
    "# train_miami, val_miami = torch.utils.data.random_split(train_miami, [train_size, valid_size])\n",
    "# train_palo_alto, val_palo_alto = torch.utils.data.random_split(train_palo_alto, [train_size, valid_size])\n",
    "# train_pittsburgh, val_pittsburgh = torch.utils.data.random_split(train_pittsburgh, [train_size, valid_size])\n",
    "# train_dearborn, val_dearborn = torch.utils.data.random_split(train_dearborn, [train_size, valid_size])\n",
    "# train_washington_dc, val_washington_dc = torch.utils.data.random_split(train_washington_dc, [train_size, valid_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 10  # batch size \n",
    "train_loader = DataLoader(train_austin,batch_size=batch_sz,shuffle=True)\n",
    "val_loader = DataLoader(val_austin,batch_size=batch_sz,shuffle=True)\n",
    "test_loader = DataLoader(test_austin,batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c6507c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1YAAAC0CAYAAACXOL1/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA1eUlEQVR4nO3de3Bb130n8C/4AkA9Go8AbbVqY3TtRAG9sleUKDmhmzQhLXe9Vm1nts5C7YxssR11VXG9bDaZVmy9ne1Ss5s4VVJ63apTKlEnETbrmdiu3MzaEe2mqWqJlKh1FBNVbG+hNqoaAVrXlkgAfGH/AEECF+fidc+5z+9nJqMxJNo3Vxf33nPO73x/vnw+nwcRERERERE1rcXqAyAiIiIiInI6DqyIiIiIiIgM4sCKiIiIiIjIIA6siIiIiIiIDOLAioiIiIiIyKC2Rv5wKBRCJBJRdCjekkwmeS4l4vmUi+dTnmQyCQA8n5Lw2pSL51Munk95eC7l4vmUK5lMIp1OV3ze0MAqEong/Pnz0g7Ky3bs2MFzKRHPp1w8n/Ls2LEDAHg+JeG1KRfPp1w8n/LwXMrF8ylX8dmuxVJAIiIiIiIigziwIiIiIiIiMogDKyIiIiIiIoM4sDJJKgVMThZ+beT3iJSrcQE+de+38dO+K/j42vM484dvmHxw7sLvumQ8oVIlvnEBJx7+FhLfuGD1oTgeL02yUs3rr+QPpBJpTJ6YRipRGcRAjePAygTxOHD77cD99xd+jcfr+z0i5WpcgB2+WfzeuX+NH+GD+N7Mdtx38G48sGHCooN1Nn7XJeMJlWrwp19A1y934/E/exRdv9yNwQ8+b/UhORYvTbJSzeuv5A/Ef3IIt3d14v7HN+P2rk4c2/tdDrIM4sBKsVQKGBgAMhngvfcKvw4M6P/eE48vIXHmhrUHTd6QSiH1xOcxmbkLqffaVy/O5Smup+79NuYRBOAr+98r/6+HK1cNWlgQ3wc4m92kkptn6r32wjX8xOd5QpuU+MYFPPOjh1H6PX/m7x/hylUT+F0nK+m9c65cf5p758DSHyODTryHn0AGnfi1+MfR9/hP4YNdnfiv/a9xgNUEDqwUu/jaP6EFi2WftbcXfk0mgY6O8j+fm/Nh231rEB88Y84BkmfFf/17uD13GffjO7gdVxDHZwoX53LfpW+e/5Duzz71W1mTjtIdcrnK73rJqaZGLd884/h3uB1XCtdw7jLiv/49q4/MkZ7/g79r6HPSx+86WUn0Xll2/ZX8gSQi6MCc5t/gw02sRxad+J3xn8PtXWu4itUgDqwUig+ewcOf6cBMpvw0z88Xfo1EgDntNQ0fcghg4JltvIhJmVQijYHnfn55puoDyKATAziOVG594cIE8Jkdb+n+/A/e+6BJR+oOfn/ld31+fuVUU6MiEaSy6zCAsfJr+Lmf532zCddnOoWff++tf2bykTif6Lueyy5h7RwrUUg90Xtl2bOm5A9EkMQcNKOwMj5kEFxZxWKpYH04sFIklUhj4JluZNGJQmkFAOQRDOQxNlb4p3AYGBsD/B1LAPJlP9+GBSQnrpt5yOQhF59PogVLZZ+1Yx7Jx3+3cGEC+C9nHwSwCO21CQA+zc9SdW1the96MAisX1/4dWxs5VRTo8JhJB//3YrZ1hYs4eLzSWuOycE2bhK9XPnw8rs7+QLVoNLverBjAUAeLXNZbL8vyEoUUq74Xqn7rCn5A+H1cxhr+VUEMYu1eB+iZ31BYRWrtFSQgyx9HFgpcmzwEjIIlH22BjN44Qs/RCy2+lksBlx89V34kSv7szexDlMvsyib5IsPnsEjw3dhBmvKPp9HOyJPPlz22dd/7a8F/wYffox/jmO/9F2FR+k+sRhw5Qpw+jRw4Ts3cOccH0hGRJ58uGK2dQZr8MjwXXyBbdCjT4pXoPNoxe/v/77JR+N8sVjhO740t4DCrH9nYUWVlSikWCqRxp1z07jwnRs4fbrwzCl95wRQ9jCK/eOXcWV6Fq9+7Uf4o71/iQBmoT/AAjjIqo0DKwVSiTRGxndhdaWqYAmt2Na/oeLPR3s34Ct7z6H8YvbhyfguJF56R+mxkrcUVlK3IbMSSgEAeQQwi7FDFxGOhsr+/C/94cexET9G5Y3Wh/9wchdvog0Kh4G3//QMtt8XXElh4iCgOeFoCGOHLmpeBAqlK3yBbUz0oTvwCxtfh+h7/sWz9/FcNuHW2z9GQDNh2o55VqKQMvHBMysJf9vvC+LtPz2jXxURDgM9PUA4jHA0hJ59XTjwjU/g76Zn8Xt9ryGITI1VLEBvkOX1ZxoHVgpcfD6J1opSqTwO971e8eJa1L07jHW4WfZZDn5s27PZ8xcpySMqAVyDGbw4Mo3YaK/wZ34w3YY2zFd8Pgc/jg1yNrsRqwPb1RQmDgKaFxvtxYsj01iDmbLPW1lK3bA/+YsPowULFZ8vop3llU2I7NxYUbWSRQCRnRstOiJyM1nPlnA0hN8+/SlcmZ5ZWcWqXSoIlA6y9j+zDa8cOe/Z5xoHVpLFB8/g4eGuijKrADI4MHq37s9Fdm7EAto0nzLIguTRKwFcQgu2PRrR/blwNIRnYuLZ7CPjH+W12YDkxHV0aAapnMU2ZtujESxpHmW3sA5Tr7CUuhHhaAifv/evIHp5eu3U++YfkAtoz2S111IiI2Q/W0pXsUpLBesZZGURxKeHP+LZEkEOrCQqzhhUBFYgg+OCMqtSxbIWP7LQXrDckE1GNVoCqHXg5Cfwm/f+BSqvzUVemw2I7NyIObSXfTaPds5iGxCOhnA0NgltKfXQyR5PPcxl+I3jWwWr0z4cPfsxlqU3KDlxHZ0ob0sRRJaTKKSEymdL44MsH2aw1rMlghxYSZScuI42TSnFGszghZE3dcusSsVGe3Hx1NWKIIsZrMHDw12euShJPr1rs1oJoNZvHN+KoOZFgWEBjSlOoAQxi/V4D8E6B7ZUXfcDlaXULAdsXDgawn/uOwPtixLL0hsnetHNwY+1G/wWHRG5mVnPlmqDrDW4BdEgq1giOPDMNiReesf1K1gcWEk09XIKN7Gu7LNaZVZa0YfuwFcPXUAQGZRuyM6iEwPPdLv6YiR1pl5JG742CzfuKYYFGBQb7cWV6Vmc/tpVXJmerXtgS/pEpdQsB2zOgdHKCZRiWfp+fs/rVvqiG1y+Z7ZgCdv3bOIAlZQw+9miHWR9a+RvBPeOVXkA2/ZsXglucmuZIAdWkqQSaQzFe1CeBJjH0b2TDc8YxEZ78cLImxUbsjMI4NjgJeMHS56SSqQxdHIHKq7NWHPXpigsgPuEGlN8IAFw5YPFbCwHlKc4gSIqS88iyMCaBsRGe3Hh1LXlPYCMXSd1Uok0Jk9MAwB69nWZXgURjoaw+/AOjB2a0i0RzCKIHAIr4RpuLRPkwEoSUdraWtxE9+7mOoAWNmS3aj71YWScEdfUGNG1uQ430f2AkWuz/NbBEpfGlUbjuu3BYgWWA8pTLEvv0JSlM7Cmcbdu5Cpi19t4XZJEdnqWFFfNSksE1+M9+JFdrsQq5c4kQQ6sJNBLAlxEW9MbB8PREA73VSaxtTLIghqglwS4YPDaLJa4BFji0hTGrsvHckC5og/dgd8RPoO4Ot0I0V6rm1iHqZd5XZJxdnyWaEsET3/tKi6eulr1Z0qTBJ3+HsGBlUHVkgCNbhw8MHo3ApoRPoMsqF5GkwCrKZa45Fni0hTGrsvHckD5RPutOFhtTDgawtG9gusyzuuSjLP7s6Q4yIo+dMfKhGytJEGnr15xYGWQ0STAasLREI4fuqgTZMEXWKpORhJgNaISFzvd0O2MsetqiMoB2a6ieYXB6gQ4WDWme3fldcl7JcngpGeJqExQlCTo9NUrDqwMkpEEWI1ekAVfFqgWGUmA1Tjphm43jF1XQ1QOyJYAxnCwapzoupxHB++VZJjTniX1JQk6e/WKAysDZCYBViMKC2BJIFUjMwlQj/aGHsAsDvedk/Lv9oLSaNwLp67hzh23OerhYUfFa1LUEoBR4c3hYNW44nVZCAPJA8hjAS04/exlqw+NHC6VSOPOHbfhwqlrjmvhoU0SdMvqFQdWBshOAtSz+gLLkkCqj+wkQD3FwcHnPjUFH3x4erzbUTdAq4WjIbx9/l1s37PJFolObqDXEoBR4c3hYFWO/oNb0IJFFCa7fJiHn89vMqQ0DXD7nk14+/y7tl2pqqb4HlFr9cop3xcOrJqkIgmwGr2SQMa2kpaKJMBajry6CxkEbZNK5BR2THRyg22PRrBY8XhjVHizOFg1LjlxHX5NyABLKqlZbnt21LN65ZTvCwdWTVCZBFiNqCSQsa1USmUSoB67pxLZGc+dGuFoCMN958CocHk4WDVGtCeVJf3ULLc+O6qtXjmlBJkDqyaoTAKshrGtVIuoBFBmEqAIQyyax3OnDqPC5dIbrLZg0RGzyFZjST/J5OZnR+nqlV4Jsp0DLTiwaoLqJMBqRLGtTlkeJbX0SgBVX5sMsWie0xKdnIRR4fKJBqtOmUW2A72SfjesNJC5vPDsqFaCbOdACw6sGmRWEqAevYQmlhN4mxUlgKUYYtG84rl7buQtvDAyjf6DW6w+JNdgVLhchZc5/VlkDlhrE5X05+DH2g1+i46InMjJaYCNEH1f7B7HzoFVg8xKAtTDcgISUd0MuF4MsWjO6Wcv45HhLjw2/CEOSCViVLh8DLIwpnSlIbg8QG3BErbv2cRrkuriljTAepR+X5wSx86BVQPMTgLUw4RA0lLdDLgebt1Mq5rb0p3spFpUOM9x8xhkYUxstBcXTl1bnon3IYNOfu+pLl58Xjgtjp0DqzpZlQSohwmBVGRGM+B6iDbTzqEd716dtcXNzq44IFVLb4WllRNRTWOQhXG3buQQQK7sM06OUi1efV44KY6dA6s6WZUEqIcJgVRkVjPgWrSbaduRwxJaWd5Wg5vTnexCNBHFhEBjGGRhjOh7z8lRqsXrzwsnxLFzYFUnK5MA9TAhkKxoBlxNaRBDG5YwB79nyhWa5YV0J6sVEgIFE1FMCGxatSALftdr4+QoNYPPi9px7FbffziwqoPVSYB6mBDobVYnAeoJR0O4bXMnOjBX9rkXyhWaVRyQFtOd+g9uweSJab5gSSRKCGQ5oDEsszRGNDnK+yTpSSXSmDxRSI4tfV64MQ2wHnr3H6sXGDiwqoPVSYB6mBDobVY0A66X18sVmhGOhtCzrwunn728kvjEEkp5RBNRLAc0jmWWzRNdk4xeJ5HSJMDbuzpx+tnL6NnX5amVKhHR/cfqkkAOrGqwSxKgHr2EQKtH7KSWVc2A68Wmwc3xYuKTWVgOqAbPa/MYvU714HNBnx2TXzmwqsJuSYB69EbsLAl0p1Qijf02LAHUYtPgxnk18cksbBisBs9r8xi9TrXwuVCd3UoCObCqwm5JgHpYEugtxw5dQhbBss/sUgIowqbB9WMJpVpsGKwGz6sxjF6navhcqM1OJYEcWFVhxyRAPXolgZzVcJdUIo0jr96L8iAVYNGm1yVn2hrDxCe17Fg24gbVzut+nteaGL1O1fC5UJud7u0cWOmwaxJgNaIR+zw6OKvhIoXAikXNp3kM952z5XXJmbbGlUbWvzBSSIAieZhkp4beec0iiGOD37foqJyB0etUTSqRxp07bsOFU9c8nwRYjV1KAjmw0mHXJMBqiiP2DuRQuEHnsYAWnH72stWHRhLoBVYEkcWB0a0WHVV1opm2o7FJJCeu84WhitPPXsYjw11srqwIk+zU2PZoBIsVrxU+HBn/KL/vNbAvJYmUpgFu37MJb59/15aTqHZhh5JADqwE7J4EWE3/wS3LKxo+AD7Mw88SFxeo3rNqytY32tIeTUf3nsdQvIdR4lUwAUo9JtmpEY6GMNx3DuXnFWhl+W9N7EtJWnwWNM4OJYEcWGk4JQlQT3LiOvyaPS2c9XI+vSAVuwZWaIWjIUR2bsTQyR18SNTAfWnmYJKdGgdGtyKIbNlnXA2sjSFUpMVnQXOsLgnkwEpDr+mq3ZIA9Yj2tHDWy/mmXkk7JkhFDx8S9eG+NHMwyU6NwmrgBLga2LhiCFUnQ6gIfBYYYWVJIAdWJfRKAJ30AstZL/dJJdIYOrkDFUEqMfsGqYjwIVEf4b60vdyXJpsdSkbciquBzfvbH8xgVvMOwvukNzENsHlW3t85sFrm9BLAUoxedxdRGeA63ET3A/YNUhFhkEX9yvalxSYxdJL70lRgQqAaXA1sjhPTiEkdpgEaY9X9nQOrZU5pBlwvRq+7h6gMcMEBQSoiDLKo38q+tHgP96UpxIRA+bga2BxRubTd04hJDaYBymHF/Z0Dq2VOagZcD0avu4NbygBLMciiftyXph4TAtXgamDjIjs3IoNA2Wdz8DtyEo2axzRAeay4v3NgBfcuvzN63flEYSpOLAPU4oChPtyXZg7uCVKDq4GNy9f4Z3I/Ph/lEt3fVZ5PDqzgzGbA9WD0urPpNQR2ahlgKQ4Y6sPNy+bgniA19GaLnzy5C4mX3rHqsGwrOXEdnZqo+iCyfKH2GD4f5RLd3+cUnk/PD6yc3Ay4FkavO1cqkcZ+3YbAzn+xZpBF/Ur3pV04dQ137riN50gy7glSRzRbnIMf2/Zs5nNIQ1QKmEXA8e8i1BhOqMlVPJ/tJVtjltCqbGuMpwdWbkoCFGH0unMdO3QJWQTLPnNSQ+B6MMiifuFoCG+ffxfb92ziOVKEe4LUEM0WAz7kEOBzSIClgMQ0QPn6D25BG5ZQ3Bozp3BrjKcHVm5LAhRh9LrzpBJpHHn1XpTv+QMWHRymoodBFvXhZmZzcE+QfMUJPj+y0A4T+Bwqx1JAYhqgGoV9a3Nln6maNPP0wMptSYB6RC8LOfixdoPfoiOiagp7/hY1n+Yx3HfOlTdYbtStjefIHEwIVCM22ouLp67Cj1zZ5yr3OTgRSwG9jRNo6oi2xqiaNPPswMqtSYAipfW6weU9BC1YwvY9m1hOZDN6gRVBZHFgdKtFR6UWN+rWxnNkHrMTpLwi+tAd+OqhC6btc3AqlgJ6FyfQ1DFz0syzAyu3JgHqiY324sKpa8srVz5k0MnZEJtZna0SBVZMuW7AX6TdqBvALA73nbP6sGylcjNzBof7zlp9WK4k2hPEFX45zNzn4ESiUsAAcnyx9ghOoKll1qSZJwdWbk4CrObWjRwCmlIMzobYh96ePzcFVugpBll87lNT8MGHp8e7GdCgsXKO+qaQRx5Pj2/nOVKgdBAb4Aq/VGbuc3Ai0Yv1TazD1Mvc4+cFTANUy6zYdc8NrNyeBFiNeDakw9WDSSeZeiXtiT1/1Rx5dRcyCLK+vIoj47uQZQ2+UsUV/jxX+KUyc5+DE4WjIRzdKyhXinOPnxcwDVAts2LXPTew8kISoJ7iRdVRclEtoIU17jaQSqQxdHIHKvb8xdy3508P68tr4zkyj2iFn6srxjAcpLbu3WEEkKn4nNeduzEN0BxmlCN7bmDllSRAPf0HtywnzhUuqnnWuNuCaM/fOtxE9wPu3PMnIprN5t6WcqzBNw9XV9RgOEh1azf4K3oYZhDkfdDFmAZoHjPKkT01sEq89A6ejO+CF5IA9SQnrsOvmfFuwRIuPp+05oBINwlwweV7/rS4t6U21uCbh6srajAcpLpbN3IIalas/Mhg4oV/4HXnUqxEMI8ZE2aeGVjFB89g257NyKH85u3mJEAR0UU1gzV4eLiLL68WqJ4E6L0XZu5tqa0YYlGswe8/uAWTJ6Z5fhQQra5wIsoYTqBUJ5pMyyGIwbF7GFbjUqxEMI8ZE2aeGFgVX15zCKB8tcr9SYBaqw+1DFYvLB+yfHm1hJeTAPUwvbK2cDSEnn1dOP3s5ZW6fL50ySdaXZnBGjwyfBfPtQGcQNFXOvBch/dReE77cBPreY5cKJVIIzlxHUf3TrISwSSqy5E9MbASvbwCefiR9eTFGxvtxQsjb2INZso+58ur+ZgEWEk0ezeHdrx7dZYvFCVYl69e8SW3uLJS4EMGQZ5rg0QTKFwNLCiuSo8OvIE1uFX2e3xOu0dpYMXQyR4cjU0yDdAEogkzmSuEnhhYiQIr/Mjh4qmrnr14tz0aWW4WvIrR6+ZiEqCYdh9RO3JYQiseG/4QV2VKsC7fHLHRXrw4Ml0xEcWEQGP0ytK5GlgQjoaQnV3CDNaWfc4SMXcQTYwNxXsQ2bnR089/M4SjIQxsnUAxHRvIY2DrhLTz7vqBVSqRxlC8B9qX16/sPYfoQ3dYdViWY/S69ZgEqK84Y/vcyFtowxLm4OeqjAbr8s0jmohiQqAxXA2sTu/dxUthW27GiTHrpBJpjF3aiWI6NuDD2KWd3GNVL9HLq9cCK/Qwet06TAKsLRwN4bbNnRXRqHz4FIgSAo/unURy4jq/w5IxIVANvdVAlgSKX7w7MYOfuWuNzk+Qk3BizDqqB7WuHljFB8/g4eGuipdXrwVW6GH0ujVSiTT2MwmwLuxtVV1pQuDR2CSGTvYwyEIRJgSqIVoNZEmg+N43ywRfVzncdxZBZBhYYTK9fdzcY1VDsX41i06UvrwGkeHFu4zR69Y4duhSRQNIrycB6mE0c23haAiRnRsxFO9hkIVCTAhUgyWBYkzwda9iaMXT49uRRx6f65tiYIWJit+t9pKtMEtolbYVxrUDK70Y6xdG3uTFu4w3bvOlEmkcefVeVMb+ezsJsBpGM9fGen31OABQhwEhYnoJvlwpdS5taEUWnTgyvsvqw/Kc/oNb0IYlFLfCzEncCuPagZUoCdDrMdYijF43V2HP36Lm0zyG+85xFbUK9raqjvX65uCeIHUYECKmVyrJyhJn4iSYPRT+HtTs33blwIppOo1h9Lo59AIrgsjiwOhWi47KGThwqI5BFubhniA1GBAixsoSd+GzzB4iOzdiFoGyzzIISPl7cOXAikmAjWH0unqry/+iwIopDvhr0A4cApjF4b5zVh+WrTDIwhwsCVSHASFiepUlbR4vlXSSVCKNyRPTAFAxCcZ9/9bw1fjnZrluYMUkwOYwel0tvT1/DKyoX3Hg8LlPTcEHH54e7+agQYNBFubgniA1GBCiT7RSehPrMPWyt0slnaAYVlGc6AKwMgnG0AprJCeuI4hs2WcBZFkKqMUkwOYxel2tqVfS3PMnyZFXdyGD4MqgYT8HDWVYw28O7gmSj6uB+sLREI7uFZRKxr1dKml32rCK4kQXAPTs6+J7qUVUlmS6amAlKgFkEmB9GL2uTiqRxtDJHajY8xfjnr9GiQYNWQRxbPD7Fh2R/bD3lzm4J0gNvdVATg4A3bsrSyV5XuyNE132FI6GMLB1AsXtL0AeA1snpLyTuWZgpVcCyFWB+nCDrDqiMsB1uInuB7jnr1GRnRuR0wwaAB9Gxj/Ka3RZ6X60IHt/KcU9QWqIVgM5OSAuleR5sTeGVdhTKpHG2KWdKG5/AXwYu7STcetFLAGUgxtk1RCVAS5wz19TwtEQhvvOoXyVAMgiwFWrEsXeX0vs/aUU9wSpwcbgYpw0cabDfWcRRIZhFTaiciXRFQMrNgOWhxtk5WIZoHwHRrcioNl0CvhwhKtWZdj7Sz3uCVKHjcHFOGniHMXQiqfHtyOPPD7XN8WwCpsQrSTOcY/VKjYDlocbZOUS7ftjGaAxhVWrs9CuWrVjjoOGEixBMQcTAtXh5ICY6LywssRetKEVWXTiyPguqw+LlhUnxdpL2gwtoVVKmyHHD6zYDFg+bpCVQ68hMMsAjTswurUiKpV7DcqxabB5mBCoBicHxETnhZUl9sLQCvvrP7gFbVhCcY/VnKQ2Q44fWLEZsHzcIGtc9YbArK82qjBomOIejBrYNNgcTAhUg43BxVhZYn+cFLC/wuB3ruwzGYNfRw+s2AxYDW6QNY4NgdXjHoz6sGmwOZgQqAYbg4uJKktYDmgPqUQayYnrOLp3sqxigJOq9qJq8OvYgRWTANXiBllj2BDYHNyDUR+WpajHhEC1tI3Bvf4sEl1vLAe0XjGw4v7HN2PoZA+OxiZx+mtXGVphQ6p6WTl2YMUkQPW4QbY5TAI0D8st6iM+Tx149+qsp19OZWJCoDqcGKjEckD70QZWZNCJoXgPIjs38tlvQ6p6WTl2YMUkQPW4QbY5TAI0jyiggSvWlbTnqQM5LKAFjw1/iGVVEjEhUA3Rs4j7fhk0ZTecAHAWVX9fjhxYMQnQHJwRaxyTAM1XGtBQLLdIJdKYPDHN67RE8Tw9N/IWWrCIefhZVqUAEwLlY8NgsUI5IFfs7SCVSOPdq7OYQ0fZ5/z7sC9VvawcObASjTKZBKiGaEaMG7LFmARonXA0hJ59XQhHQ2U17lyNKReOhnDb5k74OauqDBMC1WBYTaXTz17GAlpQ3B/SgRyfNRYoPnMeG/4QFtCCDuRYQeEAqnpZOXJgtXaDH1mUlwAwCVANvQ3ZDw938YVVg0mA1hPVuHv5xUuE+63UY0KgGgyrWVW8183Dj+L+kBYsov/gFqsPzVO0z5x5+NGCRTw38hYDKxxARS8rxw2s4oNnsH3PpuU9LPmVSHDOCqixWoKRQemG7CxfWCswCdB6rHGvjfut1GNCoBoMq1klvtdxL5/ZRH8PHZjHbZs7+U7qACp6WTlqYFU6M5BZjllfQgsunLrGWQGFYqO9eGHkzYoN2XxhXZV46R08ebKYLlPEJECz8cWrPtxvpRYTAtVgw+BVDJeyB1EFFZ85zqHincFRAytRqZUfOdy6kdP5CZJFtCGbqUwF8cEz2LZnM3KamyuTAM3HlMD6cb+VWnoJgSwJNIYNgwsYLmU9VlA5n4peVo4aWIki1jkzYI7SF9YgU5lWFFdRcwigfLWKSYBWEaUEkhj3W6klmpBiSaAcbBgsDpdir0lzsILKHVT0snLMwIoR69YrpjItMZVphWgVFcjDjyxnrSxUmhJI+rjfSi2WBKrBvZQFor18LAc0h+gaZAWV86i4lzhmYCVqusqIdfOJUpm8PEMmCqzwI4eLp65y1oocgfut1GLTYPn0+s94baWV5YDWYM8q91DRy8oRA6v44Bk8PNxV0XSVEevm44bZValEGkMnd0C7ivqV2DlEH7rDqsMiATYMro77rdRi02C5tCut7chhCa2eXGllOaC52LPKXVT0srL9wKpYx5pdrmEtyCOIDC9iC3CGbJWoDJCBFfbDhsH1EU2aMKBGDjYNlq90pbUNS5jz6EorywHNw55V7iS7l5XtB1Z6TVdfGHmTF7FFRDNkXky6EpUBMrDCXtgwuH4MqFGLTYPlK660yu5D4ySc7DSPaEsKe1Y5n+xeVrYfWImSANl01Vp6zS8fHu7yzAuYXhkg+1bZCze5N4YBNeqwabAa7F0nnuzkfU4uvS0pXrvW3CiycyNmESj7LINA03+vth5YMQnQnlZntjMoTbrKeugFTDRzxTJA++FLV+NEATV8STOOCYFqsGmweNA+jw7e5yThlhT389X450bYemDFJED7io324oWRNyuSrrywaTY+eAaPDN9VMXPFMkD7YcPgxrG3lTpMCFTD602Di/e5jpIN+AtoMbQBn1aJKh+4JcU9khPXEUS27LMAsu4rBWQSoP2Jkq7cvml2dc9OEKUzVwG+sNsWGwY3hr2t1GJCoDpebhrcf3ALWrCI4gb8eYMb8GnV2g1+ZFEe4sMtKe4hO3LdlgMrLrs6gxc3zeqFqbw4Ms0Xdhtjw+DGsLeVOkwIVMPr+ymTE9cr2iV4oYJEtfjgGWzfs2m5eiq/Eu7Dd1H3kB25bsuBFZMAncNrCYGiJEDOXJEb6fW2asMCvv2lBAcBBjAhUD6v76dkj0n5SlNlM8sT/UtowYVT1/gu6jIyI9dtObBiEqBzeCkhkEmA5DV6L2uDY/ewLNAAJgTK5/X9lPoVJDs5CdKEVCKNb38pUTHJ70cOt27kdH6KnEpm5LrtBlZMAnQWLyUEMgmQvKb0ZXUd3kfhO+7DTaxnWaABTAhUo3Q/5YVT13Dnjts8dS5FFSQZBHBs8JJFR+RMxabyg2P3VEzye2kV1EtkrnjbbmDFJEDn8UJCIJMAyauKL6ujA2+wfE0ivYRAnlNjwtEQ3j7/Lrbv2YT7H9/sqZXVwkpou+ZTH0bGd3lqgGlEafnfTaxHYZI/j3V433OroF5SOtm1BrcMBZLZamDFJEDncnNCIJMA3SmVSGPyxDRfOOoQjobw4Gejnin7NYvovsmSQGNKX4y9FrgSjoZwuO91lJcDAq0crNdNb3J/dOANpsp6gG/5Hc9noJOVbQZWTAJ0NjcnBDIJ0H2KpR5em9E2wktlv2ZhSaB8Xk8HPDB6NwLIlH3GCZD6VJvcf/CzUb6HuljpBPoM1hq6B9tmYCWaJWASoLO4NSGQSYDu4uUZbaP0yn699OIqG5sGy6XXk8YrDa7D0RCO60yAPPHMdiReesfKw7MtTu57m8wJGVsMrPRmCfjy6ixuTAhkEqD7eH1G2yhR+do8Ojzz4qoCmwbLo00HbEcOS2j1VINrvQmQHPzYtmezJ85Bozi5720ymwRbPrDiLIF7uLFUiEmA7uP1fjdGaV9cO5DDAlo89eIqG5sGy1Xa4LoNS5jzYINr0WAd8CGHgGfOQb04uU8ymwRbPrBiM2B3cVNCIJMA3cnr/W5kKH1xbcEi5j344iobmwbLVWxwLas3jdMU73N+ZKENs+B1tYqT+1Qkq0mw5QMrNgN2HzckBDIJ0N1K+90w6ak5xRdXv6aski9tzWHTYPlEq9M5+LF2g9+iIzJXbLQXF09dhR/lDW2dXqIvk6g0nJP73iSrSbClAys2A3YnNyQEMgnQ/cLREHr2dfFeY4DoxZUvbc1hQqB8pavTxfPagiVs37PJM9dn9KE78NVDF1xVoi/T2g1+ZFE+0ObkvjfJ2iZg6cCKzYDdy+kJgUwCJKrNjfsqrcSmwfLFRntx4dQ15NGCwkC103Mlq3ol+l6/ruKDZ7B9z6bl99A8gphlabiHhaMhDGydQHGPFZDHwNaJhq8FywZWbAbsbk5OCGQSIFH9+NImF5sGy3frRg4BTTmcV/ZaFeldV054JqtQ2nYjs7y/agktuHDqGqtSPCqVSGPs0k4U91gBPoxd2umMPVbcLOh+1Weyu209UyiquWYSIJE+vrTJw5JA+ZgEWvuZ/MqR8566tkTPeT9yuHUjp/MT5HayWrFYMrBiEqA36M1kZxDAscFLFh1VbaKaayYBEumr9tLGpqSNY9NguURJoEdjk0hOXPfUYKLaM/nTwx/xVLuEwmC7o+wzrw22qZysXlaWDKyYBOgdhZnsVs2nPoyM77LlA01bcx1gzTVRXdiUVC42DZarNAn06N7zGIr34P7HN3tqMAHo97eawdrlvWfeWL06/exlLKAFxb00HcjxOe9xsnpZmT6wYhKgt4SjIRzuex3aPhqtNtx/Iaq5zrPm2pNSiTQmT0y7/uVCNjYllYdNg+ULR0OI7NyIoZM7kEGnJ3uvla7ercEtaJ/NXli9Kj7r5+FHcS9NCxbRf3CL1YdGFpPRy8r0gRWTAL3nwOjdCCBT9pkd91+ISlRZc+098cEzuL2r05Oz2UaxKalcbBosn6x9FE5WXL371sjfVDyby1ev3DngFF0DHR67BkhMRi8rUwdWTAL0pnA0hOMOiGQWRayz5tpbSlctvTibLQObksrDpsHyMciiIBwNYffhHcvPZvHqVRsW8O0vJVx3/+M1QHpkXBumDawKLyzdTAL0KL39F2022YzNiHUCOJstC5uSysGEQPm0QRYBzOJw3zmrD8sy1VavbmIdBsfucd3KfTgawsC/NN6viNxHRi8r0wZWxwYvIYNA2WdMAvQW0f6Lm1iHqZet34wtKlFlxLr3cCZTHva3koNNg+UrDiY+96kp+ODD0+Pdrhs8NEK7erUO76PwUunDTax3XahFKpHG2A+M9ysi95HRy8qUgVUqkcbI+C6UrwYAS2hlEqCHhKMhHN0r2Iwdt3YzdnzwDB4ZvquiRJUR694jimXminrz2N9KDjYNVuPIq7uQQZBlv8uKA87RgTcq9va5KdSClQmkR8a1YcrA6uLzSbRqVgOAPA73vc4XFo/p3m2vzdire2qCKC1RDfCF2rNKY5mvTM9yRd0AJzcKtxOWBMrHl2uxcDSEBz8brdjb55ZQi1QijXevzrKHFQnJ6GWlfGClF1gRQAYHRu9W/Z8nm9HbjG3VDLaoBHANZvDiyDRfqD0sHA2hZ18XB9YSOLVRuN2wabBcLPvVVyuS3amhFsXE18eGP4QFtKADOVYmUBkZvayUDqyKqwGiwIrjvIg9yU4z2HolgGxWTSSXXqPw3xv/KBIvvWPJMTkRmwbLIyr7PRqbRHLiuuMGDCq4LdRCm/g6Dz9asIjnRt5iZQKVMdrLSunAStQXiIEVZIcZbJYAEplHr1H4HPzYtmezY17OrMamwXKVlv0e3XseQ/Ee9q8rUV+ohTPKAvV6V922uZPPeypjtJeV0oHV1Mupir5AXA0gQH8Ge2R8lyk3ab1BP0sAidQQNQoHfMghwP1WDWDTYLnC0RAiOzdi6OQO9q/TUS3UwillgWs3+JGFv+wzln6SiNEyYWUDq1QijaF4Dyr6Au1lXyDSn8FuNekFQdQMmIN+InWKjcL9yEL7ved+q/qxabB8DLKoTS/UwgllgfHBM9i+Z9Pyfuo8gpjlvirSZbSXlbKBlSgUYC1uons3+wJRgWgG24wgCzYDpmakEmlMnpi2/cysncVGe3Hx1FX4kdP8jg8j4/fy3NaBCYHyiWaoc/Bj7Qa/zk94U+m+NFFZ4BPPbLfdnsnSvVWZ5f3+S2jBhVPXWJ1CQkZ7WSkZWOklAS6yLxCVKM5gi4Ms1L0giGYn2QyYqimmSXH/hXHRh+7Ab/f9NbSrVlmuWtWNCYFylQ4YigPWFixh+55N/K5rVCsLzNlwz6Toee9HDrduaCd3iAqMrmBLH1hVSwLksitp6QVZqNwzIKq1ZjNg0qNNk+L+C+MKq9VZzadctWoEEwLlio324sKpa8ijBYUVwE5+13VU63VV2DNpn3MW2bkRGQTKPssiwOc96TLay0r6wIpJgNQo0QuCqpJAba11gLXWVAP3X8gXjoYwLNhjyVWr+jEhUL5bN3IIaMpU+V0XK67yifZM2i1MJV/jn4lKGe1lJX1gxSRAalT13lbyZr5EtdZ51lpTDWwkqgZXrYxjQqBc/K43Rm/PpBl7peuVnLiOTs19JogsB8tUlZFeVlIHVkwCpGbplQS2SdwzIFpNZa011SJqJMoVTuOqrVr9/n6uWtWDCYFysWlw46IP3YGvHrqgMzHajVeOnLf03BUGyx1ln3GwTLUY6WUldWDFJEAyQlQSeBPrMPWynD0Dooh13mCpHqWNRK9Mz3KFUxK9Vav/dvbncGzvdy05JidhQqB8bBrcOL2J0QwC+PTwRyw9d6efvYwFtKBY0tWBHCfGqCYjq9fSBlZMAiSjwtEQju4V7BmIG98zwIh1MiocDaFnXxevF4n0Vq0AH56Mm9Ms3On0EgJZEtg8Ng1unGhiFPBhBmstO3fF8v95+FEs6WrBIvoPbjH1OMh5jPSykjKwYhIgydK9W82eAdFqKiPWiax3YPRudFT0tQKAPPdB1EkvAIglgc1jaE1jSsso1+AWtJMlbVjAt7+UMHVwJSr/7+DfIdXBSC8rKQMrJgGSLHp7BoxshI0PnsEjw3dVrKYyYp2MYtNg48LREEb+TWVfqxwC+O5z/2jNQTlMtZLA/VxlaQqbBjeuWEb5rZG/QQCZst+7iXUYHLvH1LJAUZgay/+pHkYmVqQMrJgESLJUTwjsbvgFYTUJMIjS1dQAAwjIIDYNlucTv/iT6NC8iAE+/Oaf/ywHBXXSKwnMIohjg9+36Kici02DmxOOhrD78A4cXz536/A+Cs9yH25ivWllgQxTIyOM9LIyPLDixUuyVdsI22iPG1EJ4BrM4MWRaa6mUtPYNFiuyM6N8AkeR4toY0JgA7Y9GsGiYJ/LkfGP8tpsApsGN6+4ejU68IYlLQEYpkZGGOllZXhgxYuXVCjsGWjVfOrDyHj9G9r1SgC5mkpGcf+FXOFoCEP3VpYDAj584ex9fImtUyEM5BwqG7YuMsiiSWwa3LxwNIQHPxuVXt5fC8PUSIZme1kZGljx4iVVwtEQDgvSwtrr7GvFEkBSiY1E5fuN43ejRTNYBYAlrlo15MDoVgQ1EfYMsmge91oZU6u8X3afq8Kzv5thamRYs72smh5YMQmQVCv0uKncAFtPXyvRigJLAEkWNg2WLxwN4fP3/hVEq1ZfPNvLVas6Fa7NKfa2koR7rYwzs8/VscFLyCBQ9hnD1KgZzU6gNj2wYhIgqRaOhvBlYV+r2pGXazf4kUX5jCJLAEkmNg2WT2/VahHtLGVrgF6QRWudK/5UjnutjDOjz1UqkcbI+L0o3/MPLKGVz35qWLO9rJoeWDEJkMwg6mtVK8QiPngG2/dsWt77Vyj/44oCqcCmwXKFoyEculu0agX8+TdvVv4A6RK9yN7COky9UnvFnypxr5UxtfpcNRNokUqk8b/+4xkc2f0q/mTfdzG4+zKymtUqII/Dfa/zHk0Na7aXVVMDKyYBklkKfa3aNZ/6MDJ+r/DiLk1ryyyXqebRggunrnFFgcgBej+pfTECAB9Gv8/o9UaEoyEcjQlW/E/28Dw2gfsqjavW56reQIvES+9g+GPj+Ojai9jYdRs+85WPYfg7n8Sv/unH8c0ffQza1aoAsjgwerfs/yvkAc2GVDU1sBL9x5gESCrohVhkEcCxAxcq/ryoRNWPHG7dyFX8WSIVUilgcrLwKzXuAxs7hJ/n0YrX/viHJh+Ns3U/ULni34IlXDx9w6Ijci7RvsqjA5eQvBXid70B5X2uRIEWOiWBqRQGPvgyuvb8Cxx5/VM4O/OvALSidDVBO6gC8hjmahU1qdleVk0NrCKpyYr/GJMASZVCiEVW86kPI9/7WaSePlH26VTqp9hpnSwTjwO33w7cf3/h13jc6iNynm2PRuDDovD33jinbSJM1RRW/Cujrh/5/Id5bTahdF/l0S8uYOjkLn7Xm6QXaNGOeSS/8mL5H47H8fTG/47jf78b+oOoSh3IcbWKmibsZdXmx+n/I3uPVSqF8FP/HmPYX56Itf+vOStAShR6s1SuWnVgHsnDf7yyNJBKAUNPrUdFierAJV6bpFwqBQwMAJkM8N57hV8HBoCFhdo/S6vC0RB+IXwWon1WmK8MtiB9xReDioTArA8DA1xVbUY4GkLkwS4MPbW+4rvO89kY0T7AebQj8rXfXT2ZqRRST3wev4UR1DOYWpXHH+w9x2c/GdL/VC/aAh1Y6WW10Frzu974wCqZBDo6EMM3cQW34zT6cWXNXYj92k80e9xENYmi1+fRjkjHPxSuSaxcmmXWrc2j+8Aucw6SPE10/bW3AzlWoTZsb9+PhZ/fc+eM8HPSFxvtxYvfnMOa4FLZ5+3tK7dOapDed53nszHhaAhjv/i/yyfpsR9h//urJzOZRNL3M2jVWcUuyJf9rw1z+KPYX+LANz6h/P8DuVsyCXT4ywf0tb7rbfq/pSMSAeYKDbPCSCOMNLAULHxOpEg4GsLx/acxcPxjaMc85tFeuAEv/Xjl2iu5NFcsLLbw0iRTiK6/+XnAzz6iDfvkoa1o+Z8LWCopOW/BAj55aCu+8KSFB+ZQ2z75ASxpPpuf52O7WXrfdZ7PxsX+x8+i/8+2IJn7SUSQLLxTzpe8U0YiiOT/Fj7halVhFXZX5/fxK//2n7D+tnZ8YGMHtj0aQTjKQRUZ18x3vfEVq3AYGBsDgkFg/frCr2Njhc+JFIqN9ePKF5/D6fYHC6ukwT8ru/Z4aZKV9K6/tsanrzwv3PthfH331+Ffbsrqxyy+vvvrCPd+2OpDcyTeG+Xi+ZQoHEb4q19AT/BNhNfPVZ7M5d8/3vIraMM8SlemugM/wPSp/4uzM/fgV058Ao99+WPYfXgHy/9Imma+68098mMxoL+/sBYWifBuQqYJ/6d9CO97UPfa46VJVhJdf1/6ktVH5Uyxl59A/5kfIvnKDxHZ/WGEe5+w+pAcjfdGuXg+Jap1MmMxxPr70f/aX+G1Vxfx43Qb+h//aUQf2mrF0ZLHNPpdb34uNRzmnYSsUePa46VJVuL1J0+498NcpZKI16ZcPJ8S1TqZ4TDCj30Sjz1m3iERFTXyXW8qbp2IiIiIiIhWcWBFRERERERkkC+fzwuahYiFQiFEGHsjxdTUFLq7u60+DNfg+ZSL51Oe5HIuK++dcvDalIvnUy6eT3l4LuXi+ZQrmUwinU5XfN7QwIqIiIiIiIgqsRSQiIiIiIjIIA6siIiIiIiIDOLAioiIiIiIyCAOrIiIiIiIiAziwIqIiIiIiMggDqyIiIiIiIgM4sCKiIiIiIjIIA6siIiIiIiIDOLAioiIiIiIyKD/D+P6iBww3CpfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1080x216 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1], c='r', s=20)\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1], c='b', s=20)\n",
    "    \n",
    "\n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    if i_batch != 4: continue\n",
    "    inputs, outputs = sample_batch\n",
    "    show_sample_batch(sample_batch)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a53b5",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "56c6a4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=2, hidden_size=512, num_layers=3, dropout=0.5, batch_first=True)\n",
    "        self.linear = nn.Linear(512, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.linear2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x, h=None, c=None):\n",
    "        if (h is not None) and (c is not None):\n",
    "            x, (h, c) = self.lstm(x, (h, c))\n",
    "        else:\n",
    "            x, (h, c) = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.linear2(x)\n",
    "        return x, h, c\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf38ed7",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e605e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/5, Batch: 10/48916, Loss: 0.35264140367507935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\Load_Argo2_Public.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000012?line=13'>14</a>\u001b[0m inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000012?line=14'>15</a>\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000012?line=15'>16</a>\u001b[0m output, h, c \u001b[39m=\u001b[39m lstm(inp\u001b[39m.\u001b[39;49mfloat())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000012?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000012?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\Load_Argo2_Public.ipynb Cell 10'\u001b[0m in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x, h, c)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=18'>19</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=19'>20</a>\u001b[0m     x, (h, c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=20'>21</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=21'>22</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=22'>23</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtanh(x)\n",
      "File \u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/dropout.py?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/dropout.py?line=57'>58</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\nn\\functional.py:1279\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/functional.py?line=1276'>1277</a>\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/functional.py?line=1277'>1278</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[1;32m-> <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/functional.py?line=1278'>1279</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "learning_rate = 0.001\n",
    "\n",
    "lstm = LSTM()\n",
    "lstm.to(device)\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.25, verbose=True)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lstm.train(True)\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        inp, label = sample_batch\n",
    "        \n",
    "        inp = inp.to(device)\n",
    "        label = label.to(device)\n",
    "        output, h, c = lstm(inp.float())\n",
    "\n",
    "        loss = loss_fn(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch % 10 == 0 and i_batch != 0:\n",
    "            print(f\"Epoch: {epoch}/{num_epochs}, Batch: {i_batch}/{len(train_loader)}, Loss: {loss.item()}\")\n",
    "\n",
    "    lstm.train(False)\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i_batch, sample_batch in enumerate(val_loader):\n",
    "            inp, label = sample_batch\n",
    "            inp, label = inp.to(device), label.to(device)\n",
    "            output, h, c = lstm(inp.float())\n",
    "            loss = loss_fn(output, label)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch: {epoch}/{num_epochs}, Val Loss: {val_loss}\")\n",
    "            \n",
    "torch.save(lstm.state_dict(), \"models/palo_alto_model.pt\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae6203",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "de312802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(2, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (linear): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (tanh): Tanh()\n",
       "  (linear2): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = LSTM()\n",
    "lstm.load_state_dict(torch.load(\"models/austin_model.pt\", map_location=device))\n",
    "lstm.to(device)\n",
    "lstm.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac0370",
   "metadata": {},
   "source": [
    "## Sample Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a571d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(x, model, steps=60):\n",
    "    h = None\n",
    "    c = None\n",
    "    arr = []\n",
    "    for i in range(steps):\n",
    "        output, h, c = model(x, h, c)\n",
    "        # if i == 0:\n",
    "        #     print(x[0][40:-1], output[0][40:-1])\n",
    "        elem = output[:, -1, :]\n",
    "        x = elem.unsqueeze(1)\n",
    "        elem = elem.squeeze().detach().cpu().numpy()\n",
    "        arr.append(elem)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d9c395",
   "metadata": {},
   "source": [
    "## Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "15b2920a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5767,  0.2304],\n",
      "        [-0.5774,  0.2307],\n",
      "        [-0.5780,  0.2309],\n",
      "        [-0.5786,  0.2311],\n",
      "        [-0.5792,  0.2314],\n",
      "        [-0.5798,  0.2316],\n",
      "        [-0.5804,  0.2319],\n",
      "        [-0.5810,  0.2321],\n",
      "        [-0.5816,  0.2324]]) tensor([[-0.5731,  0.2248],\n",
      "        [-0.5738,  0.2251],\n",
      "        [-0.5744,  0.2253],\n",
      "        [-0.5750,  0.2255],\n",
      "        [-0.5756,  0.2258],\n",
      "        [-0.5762,  0.2260],\n",
      "        [-0.5769,  0.2263],\n",
      "        [-0.5775,  0.2265],\n",
      "        [-0.5781,  0.2268]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.8301,  0.7677],\n",
      "        [-0.8300,  0.7672],\n",
      "        [-0.8298,  0.7668],\n",
      "        [-0.8296,  0.7663],\n",
      "        [-0.8294,  0.7659],\n",
      "        [-0.8292,  0.7655],\n",
      "        [-0.8289,  0.7651],\n",
      "        [-0.8287,  0.7647],\n",
      "        [-0.8284,  0.7643]]) tensor([[-0.8437,  0.7678],\n",
      "        [-0.8435,  0.7673],\n",
      "        [-0.8433,  0.7668],\n",
      "        [-0.8432,  0.7663],\n",
      "        [-0.8430,  0.7658],\n",
      "        [-0.8427,  0.7654],\n",
      "        [-0.8425,  0.7650],\n",
      "        [-0.8422,  0.7646],\n",
      "        [-0.8420,  0.7642]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257],\n",
      "        [-0.5360,  0.5257]]) tensor([[-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181],\n",
      "        [-0.5356,  0.5181]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.6576,  2.3997],\n",
      "        [-0.6575,  2.3998],\n",
      "        [-0.6575,  2.3999],\n",
      "        [-0.6575,  2.4000],\n",
      "        [-0.6575,  2.4001],\n",
      "        [-0.6575,  2.4003],\n",
      "        [-0.6575,  2.4004],\n",
      "        [-0.6575,  2.4005],\n",
      "        [-0.6575,  2.4007]]) tensor([[-0.7435,  2.6390],\n",
      "        [-0.7435,  2.6392],\n",
      "        [-0.7435,  2.6393],\n",
      "        [-0.7434,  2.6395],\n",
      "        [-0.7434,  2.6397],\n",
      "        [-0.7434,  2.6399],\n",
      "        [-0.7434,  2.6400],\n",
      "        [-0.7434,  2.6402],\n",
      "        [-0.7434,  2.6404]], grad_fn=<SliceBackward0>)\n",
      "tensor([[0.3002, 0.1719],\n",
      "        [0.3010, 0.1716],\n",
      "        [0.3017, 0.1712],\n",
      "        [0.3024, 0.1708],\n",
      "        [0.3032, 0.1704],\n",
      "        [0.3039, 0.1700],\n",
      "        [0.3047, 0.1697],\n",
      "        [0.3055, 0.1692],\n",
      "        [0.3063, 0.1688]]) tensor([[0.2964, 0.1674],\n",
      "        [0.2971, 0.1670],\n",
      "        [0.2979, 0.1667],\n",
      "        [0.2986, 0.1663],\n",
      "        [0.2993, 0.1659],\n",
      "        [0.3000, 0.1655],\n",
      "        [0.3008, 0.1651],\n",
      "        [0.3016, 0.1647],\n",
      "        [0.3024, 0.1643]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.9210, -1.1868],\n",
      "        [ 0.9216, -1.1871],\n",
      "        [ 0.9223, -1.1874],\n",
      "        [ 0.9229, -1.1876],\n",
      "        [ 0.9236, -1.1881],\n",
      "        [ 0.9243, -1.1884],\n",
      "        [ 0.9249, -1.1887],\n",
      "        [ 0.9256, -1.1891],\n",
      "        [ 0.9262, -1.1894]]) tensor([[ 0.9512, -1.2350],\n",
      "        [ 0.9520, -1.2353],\n",
      "        [ 0.9527, -1.2356],\n",
      "        [ 0.9533, -1.2359],\n",
      "        [ 0.9541, -1.2363],\n",
      "        [ 0.9549, -1.2367],\n",
      "        [ 0.9556, -1.2371],\n",
      "        [ 0.9563, -1.2375],\n",
      "        [ 0.9570, -1.2378]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 2.1759, -1.1351],\n",
      "        [ 2.1755, -1.1361],\n",
      "        [ 2.1751, -1.1371],\n",
      "        [ 2.1747, -1.1380],\n",
      "        [ 2.1743, -1.1389],\n",
      "        [ 2.1740, -1.1396],\n",
      "        [ 2.1736, -1.1405],\n",
      "        [ 2.1732, -1.1415],\n",
      "        [ 2.1727, -1.1425]]) tensor([[ 2.4154, -1.2999],\n",
      "        [ 2.4147, -1.3008],\n",
      "        [ 2.4142, -1.3017],\n",
      "        [ 2.4136, -1.3025],\n",
      "        [ 2.4131, -1.3033],\n",
      "        [ 2.4126, -1.3040],\n",
      "        [ 2.4122, -1.3048],\n",
      "        [ 2.4118, -1.3056],\n",
      "        [ 2.4114, -1.3065]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 0.5309, -0.9934],\n",
      "        [ 0.5302, -0.9931],\n",
      "        [ 0.5295, -0.9927],\n",
      "        [ 0.5288, -0.9924],\n",
      "        [ 0.5281, -0.9920],\n",
      "        [ 0.5274, -0.9917],\n",
      "        [ 0.5267, -0.9913],\n",
      "        [ 0.5259, -0.9910],\n",
      "        [ 0.5252, -0.9906]]) tensor([[ 0.5369, -1.0204],\n",
      "        [ 0.5361, -1.0201],\n",
      "        [ 0.5354, -1.0197],\n",
      "        [ 0.5347, -1.0193],\n",
      "        [ 0.5339, -1.0189],\n",
      "        [ 0.5332, -1.0185],\n",
      "        [ 0.5325, -1.0182],\n",
      "        [ 0.5317, -1.0178],\n",
      "        [ 0.5310, -1.0174]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.7803, -0.0776],\n",
      "        [-0.7803, -0.0778],\n",
      "        [-0.7804, -0.0780],\n",
      "        [-0.7805, -0.0783],\n",
      "        [-0.7805, -0.0785],\n",
      "        [-0.7806, -0.0788],\n",
      "        [-0.7807, -0.0791],\n",
      "        [-0.7807, -0.0793],\n",
      "        [-0.7808, -0.0796]]) tensor([[-0.7809, -0.0798],\n",
      "        [-0.7809, -0.0800],\n",
      "        [-0.7810, -0.0802],\n",
      "        [-0.7810, -0.0804],\n",
      "        [-0.7811, -0.0807],\n",
      "        [-0.7812, -0.0809],\n",
      "        [-0.7812, -0.0812],\n",
      "        [-0.7813, -0.0815],\n",
      "        [-0.7814, -0.0818]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.9303,  1.0487],\n",
      "        [-0.9304,  1.0488],\n",
      "        [-0.9305,  1.0488],\n",
      "        [-0.9306,  1.0489],\n",
      "        [-0.9307,  1.0490],\n",
      "        [-0.9308,  1.0491],\n",
      "        [-0.9309,  1.0491],\n",
      "        [-0.9310,  1.0492],\n",
      "        [-0.9311,  1.0493]]) tensor([[-0.9571,  1.0620],\n",
      "        [-0.9572,  1.0621],\n",
      "        [-0.9573,  1.0622],\n",
      "        [-0.9574,  1.0622],\n",
      "        [-0.9575,  1.0623],\n",
      "        [-0.9576,  1.0624],\n",
      "        [-0.9577,  1.0625],\n",
      "        [-0.9578,  1.0626],\n",
      "        [-0.9580,  1.0627]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.4405,  0.1986],\n",
      "        [-0.4403,  0.1995],\n",
      "        [-0.4401,  0.2004],\n",
      "        [-0.4399,  0.2013],\n",
      "        [-0.4396,  0.2022],\n",
      "        [-0.4394,  0.2030],\n",
      "        [-0.4392,  0.2039],\n",
      "        [-0.4390,  0.2047],\n",
      "        [-0.4388,  0.2056]]) tensor([[-0.4355,  0.1923],\n",
      "        [-0.4353,  0.1932],\n",
      "        [-0.4350,  0.1941],\n",
      "        [-0.4348,  0.1950],\n",
      "        [-0.4346,  0.1959],\n",
      "        [-0.4344,  0.1967],\n",
      "        [-0.4342,  0.1975],\n",
      "        [-0.4339,  0.1984],\n",
      "        [-0.4337,  0.1992]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 1.3955, -1.4146],\n",
      "        [ 1.3956, -1.4147],\n",
      "        [ 1.3957, -1.4148],\n",
      "        [ 1.3959, -1.4148],\n",
      "        [ 1.3960, -1.4149],\n",
      "        [ 1.3961, -1.4149],\n",
      "        [ 1.3961, -1.4150],\n",
      "        [ 1.3962, -1.4150],\n",
      "        [ 1.3963, -1.4150]]) tensor([[ 1.4981, -1.5026],\n",
      "        [ 1.4983, -1.5027],\n",
      "        [ 1.4984, -1.5027],\n",
      "        [ 1.4986, -1.5028],\n",
      "        [ 1.4987, -1.5029],\n",
      "        [ 1.4988, -1.5029],\n",
      "        [ 1.4989, -1.5030],\n",
      "        [ 1.4990, -1.5031],\n",
      "        [ 1.4991, -1.5031]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.7437,  2.7918],\n",
      "        [-0.7433,  2.7905],\n",
      "        [-0.7429,  2.7892],\n",
      "        [-0.7426,  2.7879],\n",
      "        [-0.7422,  2.7866],\n",
      "        [-0.7417,  2.7853],\n",
      "        [-0.7412,  2.7840],\n",
      "        [-0.7407,  2.7828],\n",
      "        [-0.7401,  2.7816]]) tensor([[-0.8537,  3.1593],\n",
      "        [-0.8533,  3.1577],\n",
      "        [-0.8529,  3.1561],\n",
      "        [-0.8524,  3.1544],\n",
      "        [-0.8520,  3.1527],\n",
      "        [-0.8515,  3.1510],\n",
      "        [-0.8511,  3.1494],\n",
      "        [-0.8505,  3.1478],\n",
      "        [-0.8500,  3.1462]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.7191,  0.0761],\n",
      "        [-0.7191,  0.0761],\n",
      "        [-0.7191,  0.0761],\n",
      "        [-0.7191,  0.0761],\n",
      "        [-0.7191,  0.0761],\n",
      "        [-0.7192,  0.0761],\n",
      "        [-0.7192,  0.0761],\n",
      "        [-0.7192,  0.0761],\n",
      "        [-0.7192,  0.0761]]) tensor([[-0.7182,  0.0728],\n",
      "        [-0.7182,  0.0728],\n",
      "        [-0.7182,  0.0728],\n",
      "        [-0.7182,  0.0728],\n",
      "        [-0.7182,  0.0728],\n",
      "        [-0.7182,  0.0728],\n",
      "        [-0.7183,  0.0728],\n",
      "        [-0.7183,  0.0728],\n",
      "        [-0.7183,  0.0728]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 1.8780, -0.9495],\n",
      "        [ 1.8777, -0.9505],\n",
      "        [ 1.8774, -0.9515],\n",
      "        [ 1.8771, -0.9525],\n",
      "        [ 1.8767, -0.9535],\n",
      "        [ 1.8764, -0.9545],\n",
      "        [ 1.8761, -0.9555],\n",
      "        [ 1.8758, -0.9566],\n",
      "        [ 1.8755, -0.9576]]) tensor([[ 2.0249, -1.0523],\n",
      "        [ 2.0246, -1.0533],\n",
      "        [ 2.0243, -1.0543],\n",
      "        [ 2.0239, -1.0553],\n",
      "        [ 2.0236, -1.0562],\n",
      "        [ 2.0233, -1.0572],\n",
      "        [ 2.0230, -1.0582],\n",
      "        [ 2.0227, -1.0592],\n",
      "        [ 2.0225, -1.0603]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 1.3886, -1.4113],\n",
      "        [ 1.3890, -1.4115],\n",
      "        [ 1.3894, -1.4117],\n",
      "        [ 1.3898, -1.4118],\n",
      "        [ 1.3902, -1.4120],\n",
      "        [ 1.3905, -1.4122],\n",
      "        [ 1.3909, -1.4123],\n",
      "        [ 1.3913, -1.4125],\n",
      "        [ 1.3916, -1.4127]]) tensor([[ 1.4895, -1.4984],\n",
      "        [ 1.4900, -1.4986],\n",
      "        [ 1.4904, -1.4988],\n",
      "        [ 1.4909, -1.4990],\n",
      "        [ 1.4913, -1.4992],\n",
      "        [ 1.4918, -1.4994],\n",
      "        [ 1.4922, -1.4996],\n",
      "        [ 1.4927, -1.4998],\n",
      "        [ 1.4931, -1.5000]], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.2122, -0.6324],\n",
      "        [-0.2122, -0.6324],\n",
      "        [-0.2122, -0.6324],\n",
      "        [-0.2122, -0.6324],\n",
      "        [-0.2122, -0.6325],\n",
      "        [-0.2122, -0.6325],\n",
      "        [-0.2122, -0.6325],\n",
      "        [-0.2122, -0.6324],\n",
      "        [-0.2122, -0.6324]]) tensor([[-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397],\n",
      "        [-0.2081, -0.6397]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 1.6322, -0.7112],\n",
      "        [ 1.6322, -0.7113],\n",
      "        [ 1.6322, -0.7113],\n",
      "        [ 1.6322, -0.7114],\n",
      "        [ 1.6322, -0.7114],\n",
      "        [ 1.6321, -0.7114],\n",
      "        [ 1.6321, -0.7115],\n",
      "        [ 1.6321, -0.7115],\n",
      "        [ 1.6321, -0.7115]]) tensor([[ 1.7187, -0.7804],\n",
      "        [ 1.7187, -0.7804],\n",
      "        [ 1.7187, -0.7804],\n",
      "        [ 1.7187, -0.7805],\n",
      "        [ 1.7186, -0.7805],\n",
      "        [ 1.7186, -0.7805],\n",
      "        [ 1.7186, -0.7806],\n",
      "        [ 1.7186, -0.7806],\n",
      "        [ 1.7186, -0.7806]], grad_fn=<SliceBackward0>)\n",
      "tensor([[ 1.2708, -0.5980],\n",
      "        [ 1.2699, -0.5978],\n",
      "        [ 1.2690, -0.5976],\n",
      "        [ 1.2682, -0.5974],\n",
      "        [ 1.2673, -0.5972],\n",
      "        [ 1.2664, -0.5970],\n",
      "        [ 1.2655, -0.5968],\n",
      "        [ 1.2646, -0.5966],\n",
      "        [ 1.2637, -0.5964]]) tensor([[ 1.3098, -0.6384],\n",
      "        [ 1.3088, -0.6382],\n",
      "        [ 1.3079, -0.6379],\n",
      "        [ 1.3069, -0.6377],\n",
      "        [ 1.3059, -0.6374],\n",
      "        [ 1.3050, -0.6372],\n",
      "        [ 1.3040, -0.6369],\n",
      "        [ 1.3030, -0.6367],\n",
      "        [ 1.3020, -0.6364]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\Load_Argo2_Public.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000018?line=3'>4</a>\u001b[0m inp \u001b[39m=\u001b[39m (inp \u001b[39m-\u001b[39m austin_mean) \u001b[39m/\u001b[39m austin_std\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000018?line=4'>5</a>\u001b[0m inp \u001b[39m=\u001b[39m inp\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000018?line=5'>6</a>\u001b[0m output \u001b[39m=\u001b[39m sample(inp, lstm)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000018?line=7'>8</a>\u001b[0m output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(output)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000018?line=8'>9</a>\u001b[0m output \u001b[39m=\u001b[39m output \u001b[39m*\u001b[39m austin_std \u001b[39m+\u001b[39m austin_mean     \n",
      "\u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\Load_Argo2_Public.ipynb Cell 16'\u001b[0m in \u001b[0;36msample\u001b[1;34m(x, model, steps)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000016?line=3'>4</a>\u001b[0m arr \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000016?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000016?line=5'>6</a>\u001b[0m     output, h, c \u001b[39m=\u001b[39m model(x, h, c)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000016?line=6'>7</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000016?line=7'>8</a>\u001b[0m         \u001b[39mprint\u001b[39m(x[\u001b[39m0\u001b[39m][\u001b[39m40\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], output[\u001b[39m0\u001b[39m][\u001b[39m40\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\Load_Argo2_Public.ipynb Cell 10'\u001b[0m in \u001b[0;36mLSTM.forward\u001b[1;34m(self, x, h, c)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, h\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, c\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m (h \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (c \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=17'>18</a>\u001b[0m         x, (h, c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(x, (h, c))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=18'>19</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/Load_Argo2_Public.ipynb#ch0000010?line=19'>20</a>\u001b[0m         x, (h, c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(x)\n",
      "File \u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\ec201\\OneDrive\\Desktop\\CSE151BCompetition\\env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=758'>759</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=759'>760</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=760'>761</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=761'>762</a>\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=762'>763</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=763'>764</a>\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    <a href='file:///c%3A/Users/ec201/OneDrive/Desktop/CSE151BCompetition/env/lib/site-packages/torch/nn/modules/rnn.py?line=764'>765</a>\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, inp in enumerate(test_loader):\n",
    "    original_inp = inp\n",
    "    inp = inp.to(device)\n",
    "    inp = (inp - austin_mean) / austin_std\n",
    "    inp = inp.float()\n",
    "    output = sample(inp, lstm)\n",
    "\n",
    "    output = np.array(output)\n",
    "    output = output * austin_std + austin_mean     \n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    plt.scatter(original_inp[0][:,0], original_inp[0][:,1], c='b', s=20)\n",
    "    plt.scatter(output[:,0], output[:,1], c='r', s=20)\n",
    "    plt.savefig(f\"outputs/austin_prediction_{i}.png\")\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e29fbf03db329727dad78c9f26053fabfa9b2ead348090a9805a8744123bd7f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
